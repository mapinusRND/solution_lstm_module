# -*- coding: utf-8 -*-
"""
Title   : ì™¸ë¶€ë°ì´í„° ê¸°ë°˜ LSTM ì˜ˆì¸¡ ë° ë©€í‹° ì‹¤í—˜ ìë™í™” ëª¨ë“ˆ (ì˜ˆì¸¡ê°’ JSON ê¸°ë¡ ê¸°ëŠ¥ ì¶”ê°€)
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
Description:
    - LSTM ëª¨ë¸ì„ ì‚¬ìš©í•œ ì „ë ¥ ìƒì‚°ëŸ‰ ì˜ˆì¸¡ í•™ìŠµ ëª¨ë“ˆ
    - íŠ¹ì • ì‹œê°„ëŒ€(07:00~16:45) ë°ì´í„°ë§Œ ì‚¬ìš©
    - ë©€í‹° ì‹¤í—˜ ìë™í™” ì§€ì› (JSON ì„¤ì • ê¸°ë°˜)
    - ì˜ˆì¸¡ ê²°ê³¼ ë° ì„±ëŠ¥ ì§€í‘œ ìë™ ì €ì¥
    - PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
    - 80/20 ë°ì´í„° ë¶„í•  ì ìš©
"""

# ============================================================================
# í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================================================

import os
# TensorFlow ìµœì í™” ì˜µì…˜ ë¹„í™œì„±í™” (ê²½ê³  ë©”ì‹œì§€ ì–µì œ)
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
# TensorFlow ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ERRORë§Œ ì¶œë ¥)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import psycopg2
from sklearn.preprocessing import StandardScaler
import json
import joblib
from sqlalchemy import create_engine
from datetime import datetime

# ============================================================================
# í™˜ê²½ë³„ ê²½ë¡œ ì„¤ì •
# ============================================================================

# Flask í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ ë¡œì»¬/ì„œë²„ í™˜ê²½ êµ¬ë¶„
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"
else:
    root = "/app/webfiles/lstm"

# ê·¸ë˜í”„ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ ìƒì„±
graph_path = os.path.abspath(root + "/graphImage")
os.makedirs(graph_path, exist_ok=True)

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ ìƒì„±
model_path = os.path.abspath(root + "/saved_models")
os.makedirs(model_path, exist_ok=True)

# ì˜ˆì¸¡ ê²°ê³¼ JSON ì €ì¥ ê²½ë¡œ ìƒì„±
prediction_path = os.path.abspath(root + "/predictions")
os.makedirs(prediction_path, exist_ok=True)

# ============================================================================
# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
# ============================================================================

def get_db_engine():
    """
    SQLAlchemy ì—”ì§„ ìƒì„± í•¨ìˆ˜
    
    Returns:
        SQLAlchemy Engine ê°ì²´ - PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    
    ìš©ë„:
        - ë°ì´í„° ë¡œë“œ (í•™ìŠµìš© ì‹œê³„ì—´ ë°ì´í„°)
        - ì‹¤í—˜ ê²°ê³¼ ì €ì¥ (ëª¨ë¸ ì •ë³´, ì„±ëŠ¥ ì§€í‘œ)
    """
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    # connection_string = "postgresql://postgres:7926@localhost:5432/postgres"
    return create_engine(connection_string)

# ============================================================================
# ì‹¤í—˜ ì„¤ì • íŒŒì¼ ë¡œë“œ
# ============================================================================

def load_experiments_config(config_file="experiments.json"):
    """
    ì‹¤í—˜ ì„¤ì • JSON íŒŒì¼ ë¡œë“œ
    
    Args:
        config_file (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: experiments.json)
    
    Returns:
        list: ì‹¤í—˜ ì„¤ì • ë¦¬ìŠ¤íŠ¸
            ê° ì‹¤í—˜ ì„¤ì •ì€ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨:
            - name: ì‹¤í—˜ëª…
            - modelName: ëª¨ë¸ëª…
            - tablename: ë°ì´í„° í…Œì´ë¸”ëª…
            - dateColumn: ë‚ ì§œ ì»¬ëŸ¼ëª…
            - studyColumns: í•™ìŠµ ì»¬ëŸ¼ë“¤ (ì‰¼í‘œ êµ¬ë¶„)
            - targetColumn: ì˜ˆì¸¡ ëŒ€ìƒ ì»¬ëŸ¼
            - r_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
            - r_batchSize: ë°°ì¹˜ í¬ê¸°
            - r_validationSplit: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            - r_seqLen: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
            - r_predDays: ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜
        []: ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    """
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config['experiments']
    except FileNotFoundError:
        print(f"âŒ ì„¤ì • íŒŒì¼ '{config_file}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {config_file}")
        return []

# ============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œ (íŠ¹ì • ì‹œê°„ëŒ€ ë° ë‚ ì§œ í•„í„°ë§)
# ============================================================================

def load_data_from_db(tablename, dateColumn, studyColumns):
    """
    ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•™ìŠµìš© ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ (íŠ¹ì • ë‚ ì§œ ì œì™¸)
    
    Args:
        tablename (str): í…Œì´ë¸”ëª… (ì˜ˆ: lstm_input_15m_new)
        dateColumn (str): ë‚ ì§œ ì»¬ëŸ¼ëª… (ì˜ˆ: time_point)
        studyColumns (str): í•™ìŠµí•  ì»¬ëŸ¼ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)
    
    Returns:
        pandas.DataFrame: ì‹œê³„ì—´ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„°
            - ì‹œê°„ëŒ€ë³„ ë¶„í¬ ì •ë³´ ì¶œë ¥
            - NULL ê°’ì´ ìˆëŠ” í–‰ì€ ì œì™¸
        None: ë¡œë“œ ì‹¤íŒ¨ ì‹œ
    
    ë°ì´í„° í•„í„°ë§:
        - dateColumn IS NOT NULL: ë‚ ì§œê°€ ìˆëŠ” ë°ì´í„°ë§Œ
        - íŠ¹ì • ë‚ ì§œ ì œì™¸ (ì´ìƒì¹˜ ë‚ ì§œë“¤):
          '06-02', '06-13~17', '06-20~21', '06-24~26', '06-28',
          '07-01', '07-08', '07-13~19', '07-21~22'
        - ì‹œê°„ëŒ€ ë¶„í¬ ë¶„ì„ ë° ì¶œë ¥
    
    ì´ìœ :
        - ì „ë ¥ ìƒì‚°ëŸ‰ì€ ì£¼ë¡œ 07:00~16:45ì— ë°œìƒ
        - íŠ¹ì • ë‚ ì§œëŠ” ì´ìƒì¹˜ ë˜ëŠ” íŠ¹ìˆ˜í•œ ìƒí™© (ê³µíœ´ì¼, ì„¤ë¹„ ì ê²€ ë“±)
    """
    try:
        engine = get_db_engine()
        
        # SQL ì¿¼ë¦¬ ì‘ì„±
        # - carbontwin ìŠ¤í‚¤ë§ˆì˜ ì§€ì •ëœ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ
        # - íŠ¹ì • ë‚ ì§œ ì œì™¸ ì²˜ë¦¬ (ì´ìƒì¹˜ ë‚ ì§œë“¤)
        # - ë‚ ì§œ ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
        query = f"""
        SELECT {studyColumns},{dateColumn}
        FROM carbontwin.{tablename}
        WHERE {dateColumn} IS NOT NULL
        AND TO_CHAR({dateColumn}, 'MM-DD') NOT IN (
            '06-02', '06-13', '06-14', '06-15', '06-16', '06-17',
            '06-20', '06-21', '06-24', '06-25', '06-26', '06-28',
            '07-01', '07-08', '07-13', '07-14', '07-15', '07-16',
            '07-17', '07-18', '07-19', '07-21', '07-22'
        )
        ORDER BY {dateColumn} ASC
        """
        
        # ë°ì´í„° ë¡œë“œ
        data = pd.read_sql_query(query, engine)

        # âœ… ì‹œê°„ëŒ€ ë¶„í¬ í™•ì¸ ë° ì¶œë ¥
        # ë°ì´í„°ì˜ ì‹œê°„ ë²”ìœ„ì™€ ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜ ë¶„ì„
        if dateColumn in data.columns and len(data) > 0:
            data[dateColumn] = pd.to_datetime(data[dateColumn])
            hours = data[dateColumn].dt.hour
            print(f"   ğŸ“Š ì‹œê°„ ë²”ìœ„: {hours.min()}ì‹œ ~ {hours.max()}ì‹œ")
            
            # ì‹œê°„ëŒ€ë³„ ë°ì´í„° ê°œìˆ˜ ì§‘ê³„
            hour_counts = hours.value_counts().sort_index()
            print(f"   ğŸ“Š ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜:")
            for hour, count in hour_counts.items():
                print(f"      {hour:2d}ì‹œ: {count:5d}ê°œ")
        
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        return None

# ============================================================================
# JSON ì§ë ¬í™” ì§€ì› í•¨ìˆ˜
# ============================================================================

def convert_numpy_to_json_serializable(obj):
    """
    NumPy ë°°ì—´ê³¼ íŠ¹ìˆ˜ íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    
    Args:
        obj: ë³€í™˜í•  ê°ì²´
            - np.ndarray: NumPy ë°°ì—´
            - np.integer: NumPy ì •ìˆ˜í˜•
            - np.floating: NumPy ì‹¤ìˆ˜í˜•
            - pd.Timestamp: Pandas íƒ€ì„ìŠ¤íƒ¬í”„
            - datetime: Python datetime ê°ì²´
    
    Returns:
        JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python ê¸°ë³¸ íƒ€ì…
            - list: NumPy ë°°ì—´ â†’ Python ë¦¬ìŠ¤íŠ¸
            - int: NumPy ì •ìˆ˜ â†’ Python int
            - float: NumPy ì‹¤ìˆ˜ â†’ Python float
            - str: Timestamp/datetime â†’ ISO í˜•ì‹ ë¬¸ìì—´
    
    ìš©ë„:
        - ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•  ë•Œ
        - ì‹¤í—˜ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”í•  ë•Œ
    """
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    else:
        return obj

# ============================================================================
# ì˜ˆì¸¡ ê²°ê³¼ JSON ì €ì¥ í•¨ìˆ˜
# ============================================================================

def save_predictions_to_json(modelName, dates, actual_values, predicted_values, target_column):
    """
    ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        modelName (str): ëª¨ë¸ëª… (íŒŒì¼ëª…ì— ì‚¬ìš©)
        dates: ì˜ˆì¸¡ ì‹œì ë“¤ (pandas Series ë˜ëŠ” array)
        actual_values (array): ì‹¤ì œê°’ ë°°ì—´
        predicted_values (array): ì˜ˆì¸¡ê°’ ë°°ì—´
        target_column (str): íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
    
    Returns:
        dict: ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ì •ë³´
            - model_name: ëª¨ë¸ëª…
            - target_column: íƒ€ê²Ÿ ì»¬ëŸ¼
            - prediction_count: ì˜ˆì¸¡ ê°œìˆ˜
            - timestamp: ì €ì¥ ì‹œê°
            - statistics: í†µê³„ ì •ë³´ (ìµœì†Œ, ìµœëŒ€, í‰ê· , MAE, RMSE)
            - predictions: ê° ì‹œì ë³„ ì˜ˆì¸¡ ìƒì„¸ ì •ë³´
        None: ì €ì¥ ì‹¤íŒ¨ ì‹œ
    
    ì €ì¥ ë‚´ìš©:
        ê° ì˜ˆì¸¡ ì‹œì ë§ˆë‹¤:
        - index: ìˆœì„œ
        - date: ë‚ ì§œ/ì‹œê°„
        - actual_value: ì‹¤ì œê°’
        - predicted_value: ì˜ˆì¸¡ê°’
        - difference: ì°¨ì´ (ì˜ˆì¸¡ - ì‹¤ì œ)
        - percentage_error: ë°±ë¶„ìœ¨ ì˜¤ì°¨
    
    ì €ì¥ ìœ„ì¹˜:
        {prediction_path}/{modelName}_predictions.json
    """
    try:
        # ì˜ˆì¸¡ ë°ì´í„° êµ¬ì„± - ê° ì‹œì ë³„ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¹„êµ
        predictions_data = []
        
        for i in range(len(actual_values)):
            prediction_record = {
                "index": i,
                "date": convert_numpy_to_json_serializable(dates.iloc[i] if hasattr(dates, 'iloc') else dates[i]),
                "actual_value": convert_numpy_to_json_serializable(actual_values[i]),
                "predicted_value": convert_numpy_to_json_serializable(predicted_values[i]),
                "difference": convert_numpy_to_json_serializable(predicted_values[i] - actual_values[i]),
                "percentage_error": convert_numpy_to_json_serializable(
                    abs((predicted_values[i] - actual_values[i]) / actual_values[i] * 100) if actual_values[i] != 0 else 0
                )
            }
            predictions_data.append(prediction_record)
        
        # ì €ì¥ íŒŒì¼ ê²½ë¡œ ìƒì„±
        prediction_file_path = os.path.join(prediction_path, f"{modelName}_predictions.json")
        
        # ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ì •ë³´ êµ¬ì„±
        prediction_summary = {
            "model_name": modelName,
            "target_column": target_column,
            "prediction_count": len(predictions_data),
            "timestamp": datetime.now().isoformat(),
            "statistics": {
                "actual_min": convert_numpy_to_json_serializable(np.min(actual_values)),
                "actual_max": convert_numpy_to_json_serializable(np.max(actual_values)),
                "actual_mean": convert_numpy_to_json_serializable(np.mean(actual_values)),
                "predicted_min": convert_numpy_to_json_serializable(np.min(predicted_values)),
                "predicted_max": convert_numpy_to_json_serializable(np.max(predicted_values)),
                "predicted_mean": convert_numpy_to_json_serializable(np.mean(predicted_values)),
                "mean_absolute_error": convert_numpy_to_json_serializable(np.mean(np.abs(predicted_values - actual_values))),
                "rmse": convert_numpy_to_json_serializable(np.sqrt(np.mean((predicted_values - actual_values) ** 2)))
            },
            "predictions": predictions_data
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(prediction_file_path, 'w', encoding='utf-8') as f:
            json.dump(prediction_summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {prediction_file_path}")
        return prediction_summary
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ============================================================================
# ì‹¤í—˜ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í•¨ìˆ˜
# ============================================================================

def save_experiment_to_db(result, config, is_new_model):
    """
    ì‹¤í—˜ ê²°ê³¼ë¥¼ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    
    Args:
        result (dict): ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            - modelName: ëª¨ë¸ëª…
            - accuracy, mape, rmse, r2_score: ì„±ëŠ¥ ì§€í‘œ
            - training_loss_img, total_graph_img, diff_graph_img: ê·¸ë˜í”„ ê²½ë¡œ
            - prediction_file: ì˜ˆì¸¡ íŒŒì¼ ê²½ë¡œ
            - execution_time: ì‹¤í–‰ ì‹œê°„
        config (dict): ì‹¤í—˜ ì„¤ì • ì •ë³´
        is_new_model (bool): ì‹ ê·œ ëª¨ë¸ ì—¬ë¶€
    
    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    
    ì €ì¥ í…Œì´ë¸”:
        1. lstm_model í…Œì´ë¸”:
           - ì‹ ê·œ ëª¨ë¸ì¸ ê²½ìš° ëª¨ë¸ ê¸°ë³¸ ì •ë³´ ë“±ë¡
           - model_name, target_column, sequence_length ë“±
        
        2. lstm_experiment í…Œì´ë¸”:
           - ì‹¤í—˜ ê²°ê³¼ ì €ì¥
           - model_id (ì™¸ë˜í‚¤), ì„±ëŠ¥ ì§€í‘œ, ê·¸ë˜í”„ ê²½ë¡œ ë“±
    
    í”„ë¡œì„¸ìŠ¤:
        1. ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        2. ì‹ ê·œ ëª¨ë¸ì¸ ê²½ìš° lstm_modelì— ë“±ë¡
        3. model_id ì¡°íšŒ
        4. ì‹¤í—˜ ê²°ê³¼ë¥¼ lstm_experimentì— ì €ì¥
    """
    try:
        engine = get_db_engine()
        model_name = result.get('modelName')
        
        # ì‹ ê·œ ëª¨ë¸ì¸ ê²½ìš° lstm_model í…Œì´ë¸”ì— ë“±ë¡
        if is_new_model:
            # ê¸°ì¡´ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            check_query = f"SELECT model_id FROM carbontwin.lstm_model WHERE model_name = '{model_name}'"
            existing = pd.read_sql_query(check_query, engine)
            
            if existing.empty:
                # ëª¨ë¸ ê¸°ë³¸ ì •ë³´ êµ¬ì„±
                model_data = {
                    'model_name': model_name,
                    'target_column': config.get('targetColumn'),
                    'date_column': config.get('dateColumn'),
                    'study_columns': config.get('studyColumns'),
                    'epochs': config.get('r_epochs'),
                    'batch_size': config.get('r_batchSize'),
                    'validation_split': config.get('r_validationSplit'),
                    'sequence_length': config.get('r_seqLen'),
                    'prediction_days': config.get('r_predDays'),
                    'created_at': datetime.now()
                }
                
                # lstm_model í…Œì´ë¸”ì— ì‚½ì…
                df_model = pd.DataFrame([model_data])
                df_model.to_sql('lstm_model', engine, schema='carbontwin',
                              if_exists='append', index=False)
                print(f"âœ… ì‹ ê·œ ëª¨ë¸ ë“±ë¡: {model_name}")
            else:
                print(f"â„¹ï¸  ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©: {model_name}")
        
        # model_id ì¡°íšŒ
        query = f"SELECT model_id FROM carbontwin.lstm_model WHERE model_name = '{model_name}'"
        model_id = pd.read_sql_query(query, engine).iloc[0]['model_id']
        
        # ì‹¤í—˜ ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        experiment_data = {
            'model_id': model_id,
            'experiment_name': result.get('experiment_name', config.get('name')),
            'accuracy': result.get('accuracy'),
            'mape': result.get('mape'),
            'rmse': result.get('rmse'),
            'r2_score': result.get('r2_score'),
            'model_file_path': os.path.abspath(os.path.join(model_path, f"{model_name}.h5")),
            'training_loss_img_path': os.path.abspath(os.path.join(root, result.get('training_loss_img'))),
            'total_graph_img_path': os.path.abspath(os.path.join(root, result.get('total_graph_img'))),
            'diff_graph_img_path': os.path.abspath(os.path.join(root, result.get('diff_graph_img'))),
            'prediction_file_path': os.path.abspath(os.path.join(root, result.get('prediction_file'))),
            'execution_time_seconds': result.get('execution_time'),
            'status': result.get('status'),
            'config_json': json.dumps(config, ensure_ascii=False),
            'created_at': datetime.now()
        }
        
        # lstm_experiment í…Œì´ë¸”ì— ì‚½ì…
        df_experiment = pd.DataFrame([experiment_data])
        df_experiment.to_sql('lstm_experiment', engine, schema='carbontwin',
                           if_exists='append', index=False)
        
        print(f"ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ (Model ID: {model_id})")
        return True
        
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def run_single_experiment(experiment_config, experiment_index):
    """
    ë‹¨ì¼ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    
    Args:
        experiment_config (dict): ì‹¤í—˜ ì„¤ì • ì •ë³´
        experiment_index (int): ì‹¤í—˜ ìˆœì„œ (ì¶œë ¥ìš©)
    
    Returns:
        dict: ì‹¤í—˜ ê²°ê³¼
            - status: ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ
            - execution_time: ì‹¤í–‰ ì‹œê°„
            - ì„±ëŠ¥ ì§€í‘œë“¤ (accuracy, mape, rmse ë“±)
    
    ì‹¤í–‰ ìˆœì„œ:
        1. ì‹¤í—˜ ì‹œì‘ ì•Œë¦¼
        2. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ
        3. LSTM ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (lstmFinance í•¨ìˆ˜ í˜¸ì¶œ)
        4. ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
        5. ì„±ê³µ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ì— ê²°ê³¼ ì €ì¥
        6. ê²°ê³¼ ë°˜í™˜
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ ì‹¤í—˜ {experiment_index + 1} ì‹œì‘: {experiment_config['name']}")
    print(f"{'='*60}")
    
    # 1. ë°ì´í„° ë¡œë“œ
    data = load_data_from_db(
        experiment_config['tablename'],
        experiment_config['dateColumn'], 
        experiment_config['studyColumns']
    )
    
    if data is None:
        return {"status": "error", "message": "ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"}
    
    # 2. í•™ìŠµ ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •
    start_time = time.time()
    result = lstmFinance(data, experiment_config)
    end_time = time.time()
    
    # 3. ì‹¤í–‰ ì‹œê°„ ë° ì‹¤í—˜ëª… ì¶”ê°€
    result['execution_time'] = round(end_time - start_time, 2)
    result['experiment_name'] = experiment_config['name']
    
    print(f"â±ï¸  ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {result['execution_time']}ì´ˆ")
    
    # 4. ì„±ê³µ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    if result['status'] == 'success':
        print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ê²°ê³¼ ì €ì¥ ì¤‘...")
        save_success = save_experiment_to_db(
            result, 
            experiment_config,
            is_new_model=result.get('is_new_model', False)
        )
        
        if save_success:
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
    
    return result

# ============================================================================
# LSTM ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜
# ============================================================================

def lstmFinance(lstmData, config):
    """
    LSTM ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (ì „ë ¥ ìƒì‚°ëŸ‰ ì˜ˆì¸¡ìš©)
    
    íŠ¹ì§•:
    - 80/20 ë°ì´í„° ë¶„í•  ì ìš©
    - íŠ¹ì • ì‹œê°„ëŒ€ ë°ì´í„°ë§Œ ì‚¬ìš© (07:00~16:45)
    - StandardScalerë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ì •ê·œí™”
    - ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    - ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ì½˜ì†” ì¶œë ¥
    
    Args:
        lstmData (DataFrame): í•™ìŠµ ë°ì´í„°
        config (dict): ëª¨ë¸ í•™ìŠµ ì„¤ì •
            - modelName: ëª¨ë¸ëª…
            - dateColumn: ë‚ ì§œ ì»¬ëŸ¼ëª…
            - studyColumns: í•™ìŠµ ì»¬ëŸ¼ë“¤
            - targetColumn: ì˜ˆì¸¡ íƒ€ê²Ÿ
            - r_epochs: ì—í¬í¬ ìˆ˜
            - r_batchSize: ë°°ì¹˜ í¬ê¸°
            - r_validationSplit: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            - r_seqLen: ì‹œí€€ìŠ¤ ê¸¸ì´
            - r_predDays: ì˜ˆì¸¡ ìŠ¤í… ìˆ˜
    
    Returns:
        dict: í•™ìŠµ ê²°ê³¼
            - status: ì„±ê³µ/ì‹¤íŒ¨
            - modelName: ëª¨ë¸ëª…
            - ì„±ëŠ¥ ì§€í‘œ (accuracy, mape, rmse, r2_score ë“±)
            - ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œë“¤
            - ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
    
    í•™ìŠµ í”„ë¡œì„¸ìŠ¤:
        1. ì„¤ì • íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        2. ë°ì´í„° ê²€ì¦
        3. ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬ ë° ì‹œê°„ëŒ€ í™•ì¸
        4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        5. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  (80/20)
        6. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        7. ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
        8. í•™ìŠµ (ìƒˆ ëª¨ë¸ì¸ ê²½ìš°)
        9. ì˜ˆì¸¡ ìˆ˜í–‰
        10. ì—­ì •ê·œí™”
        11. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        12. ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        13. ì‹œê°í™” ê·¸ë˜í”„ ìƒì„±
        14. ê²°ê³¼ ì €ì¥ ë° ë°˜í™˜
    """
    
    # TensorFlow eager execution í™œì„±í™” (ë””ë²„ê¹… ìš©ì´)
    if not tf.executing_eagerly():
        tf.config.run_functions_eagerly(True)

    # ====================================================================
    # 1ë‹¨ê³„: ì„¤ì • íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    # ====================================================================
    modelName = config['modelName']
    dateColumn = config['dateColumn']
    studyColumns = config['studyColumns']
    targetColumn = config['targetColumn']
    r_epochs = config['r_epochs']
    r_batchSize = config['r_batchSize']
    r_validationSplit = config['r_validationSplit']
    r_seqLen = config['r_seqLen']
    r_predDays = config['r_predDays']

    # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    training_loss_path = os.path.join(graph_path, f"{modelName}_trainingLoss.png")
    total_graph_path = os.path.join(graph_path, f"{modelName}_totalgraph.png")
    diff_graph_path = os.path.join(graph_path, f"{modelName}_diffgraph.png")
    model_file_path = os.path.join(model_path, f"{modelName}.h5")

    stock_data = lstmData
    
    # ====================================================================
    # 2ë‹¨ê³„: ë°ì´í„° ê²€ì¦
    # ====================================================================
    if stock_data.empty:
        return {"status": "error", "message": "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
    
    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ì •ë³´:")
    print(f"   - ì´ ë°ì´í„° ìˆ˜: {len(stock_data)}ê°œ")
    
    # í•™ìŠµ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° íƒ€ê²Ÿ ì»¬ëŸ¼ ê²€ì¦
    study_columns_list = [col.strip() for col in studyColumns.split(',')]
    if targetColumn not in study_columns_list:
        return {"status": "error", "message": f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{targetColumn}'ì´ í•™ìŠµ ì»¬ëŸ¼ì— ì—†ìŠµë‹ˆë‹¤."}

    # ====================================================================
    # 3ë‹¨ê³„: ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬ ë° ì‹œê°„ëŒ€ í™•ì¸
    # ====================================================================
    # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê°€ìƒ ë‚ ì§œ ìƒì„±
    if dateColumn in stock_data.columns:
        dates = pd.to_datetime(stock_data[dateColumn], errors='coerce')
        
        # âœ… ì‹œê°„ ë²”ìœ„ í™•ì¸ ë° ì¶œë ¥
        # ë°ì´í„°ê°€ 07:00~16:45 ë²”ìœ„ì— ìˆëŠ”ì§€ í™•ì¸
        hours = dates.dt.hour
        print(f"   - ì‹œê°„ ë²”ìœ„: {hours.min()}ì‹œ ~ {hours.max()}ì‹œ")
        print(f"   - ê³ ìœ  ì‹œê°„ëŒ€: {sorted(hours.unique())}")
    else:
        # 15ë¶„ ê°„ê²© ê°€ìƒ ë‚ ì§œ ìƒì„±
        dates = pd.date_range(start='2023-01-01', periods=len(stock_data), freq='15T')
        print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ '{dateColumn}'ì´ ì—†ì–´ì„œ ê°€ìƒ ë‚ ì§œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    
    # ====================================================================
    # 4ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§
    # ====================================================================
    # ì›ë³¸ íƒ€ê²Ÿ ê°’ ì €ì¥ (ë‚˜ì¤‘ì— ê·¸ë˜í”„ì— ì‚¬ìš©)
    original_open = stock_data[targetColumn].values
    # í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ (ëª¨ë“  study columns)
    stock_data_for_training = stock_data[study_columns_list].astype(float)

    # ë°ì´í„° í‘œì¤€í™” (í‰ê·  0, ë¶„ì‚° 1ë¡œ ë³€í™˜)
    # ì´ìœ : LSTMì´ ì •ê·œí™”ëœ ë°ì´í„°ì—ì„œ ë” ì˜ í•™ìŠµí•¨
    scaler = StandardScaler()
    stock_data_scaled = scaler.fit_transform(stock_data_for_training)

    # ====================================================================
    # 5ë‹¨ê³„: 80/20 split (í•™ìŠµ 80%, í…ŒìŠ¤íŠ¸ 20%)
    # ====================================================================
    # ì´ì „ 90/10ì—ì„œ ë³€ê²½í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ì¤‘ ì¦ê°€
    split_index = int(len(stock_data_scaled) * 0.8)
    train_data_scaled = stock_data_scaled[:split_index]
    test_data_scaled = stock_data_scaled[split_index:]
    test_dates = dates[split_index:]

    # í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
    pred_days = int(r_predDays)  # ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜
    seq_len = int(r_seqLen)  # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
    input_dim = stock_data_for_training.shape[1]  # ì…ë ¥ í”¼ì²˜ ê°œìˆ˜
    target_idx = study_columns_list.index(targetColumn)  # íƒ€ê²Ÿ ì»¬ëŸ¼ì˜ ì¸ë±ìŠ¤

    # ====================================================================
    # 6ë‹¨ê³„: ë°ì´í„° ì¶©ë¶„ì„± ê²€ì¦
    # ====================================================================
    # ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    print(f"\nğŸ” ì‹œí€€ìŠ¤ ìƒì„± ê²€ì¦:")
    print(f"   - ì „ì²´ ë°ì´í„°: {len(stock_data_scaled)}ê°œ")
    print(f"   - í•™ìŠµ ë°ì´í„°: {len(train_data_scaled)}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data_scaled)}ê°œ")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´(seq_len): {seq_len}")
    print(f"   - ì˜ˆì¸¡ ì¼ìˆ˜(pred_days): {pred_days}")
    
    min_required = seq_len + pred_days
    print(f"   - í•„ìš”í•œ ìµœì†Œ ë°ì´í„°: {min_required}ê°œ")
    
    # í•™ìŠµ ë°ì´í„° ì¶©ë¶„ì„± ê²€ì‚¬
    if len(train_data_scaled) < min_required:
        error_msg = f"í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(train_data_scaled)}ê°œ (ìµœì†Œ {min_required}ê°œ í•„ìš”)"
        print(f"âŒ {error_msg}")
        return {"status": "error", "message": error_msg}
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶©ë¶„ì„± ê²€ì‚¬
    if len(test_data_scaled) < min_required:
        error_msg = f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ì¡±: {len(test_data_scaled)}ê°œ (ìµœì†Œ {min_required}ê°œ í•„ìš”)"
        print(f"âŒ {error_msg}")
        return {"status": "error", "message": error_msg}

    # ====================================================================
    # 7ë‹¨ê³„: ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    # ====================================================================
    # LSTM ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜: (ìƒ˜í”Œ ìˆ˜, ì‹œí€€ìŠ¤ ê¸¸ì´, í”¼ì²˜ ìˆ˜)
    trainX, trainY, testX, testY = [], [], [], []
    
    # ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ë²”ìœ„ ê³„ì‚°
    train_range = range(seq_len, len(train_data_scaled) - pred_days + 1)
    test_range = range(seq_len, len(test_data_scaled) - pred_days + 1)
    
    print(f"\nğŸ“Š ì‹œí€€ìŠ¤ ìƒì„± ë²”ìœ„:")
    print(f"   - í•™ìŠµ ì‹œí€€ìŠ¤: {len(train_range)}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤: {len(test_range)}ê°œ")
    
    # ì‹œí€€ìŠ¤ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if len(train_range) == 0:
        return {"status": "error", "message": "í•™ìŠµ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    if len(test_range) == 0:
        return {"status": "error", "message": "í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    # í•™ìŠµ ì‹œí€€ìŠ¤ ìƒì„±
    for i in train_range:
        # X: ê³¼ê±° seq_len ìŠ¤í…ì˜ ëª¨ë“  í”¼ì²˜
        trainX.append(train_data_scaled[i - seq_len:i, 0:input_dim])
        # Y: pred_days í›„ì˜ íƒ€ê²Ÿ ê°’
        trainY.append(train_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    # í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤ ìƒì„±
    for i in test_range:
        testX.append(test_data_scaled[i - seq_len:i, 0:input_dim])
        testY.append(test_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    trainX, trainY = np.array(trainX), np.array(trainY)
    testX, testY = np.array(testX), np.array(testY)

    print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
    print(f"   - trainX: {trainX.shape}, trainY: {trainY.shape}")
    print(f"   - testX: {testX.shape}, testY: {testY.shape}")

    # ====================================================================
    # 8ë‹¨ê³„: ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    # ====================================================================
    print(f"\nğŸ”„ {modelName} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    is_new_model = False

    try:
        # ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
        model = load_model(model_file_path, compile=False)
        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
        print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œë¨")
        is_new_model = False
    except (OSError, IOError):
        # ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        print("ğŸ”„ ìƒˆ ëª¨ë¸ ìƒì„± ì¤‘...")
        is_new_model = True

        # LSTM ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì„±
        model = Sequential([
            Input(shape=(trainX.shape[1], trainX.shape[2])),  # (ì‹œí€€ìŠ¤ ê¸¸ì´, í”¼ì²˜ ìˆ˜)
            LSTM(64, return_sequences=True),  # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´ (64 ìœ ë‹›)
            LSTM(32, return_sequences=False),  # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´ (32 ìœ ë‹›)
            Dense(trainY.shape[1])  # ì¶œë ¥ ë ˆì´ì–´
        ])

        # ëª¨ë¸ ì»´íŒŒì¼
        # optimizer: Adam (í•™ìŠµë¥  0.01)
        # loss: MSE (í‰ê· ì œê³±ì˜¤ì°¨)
        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')

        # ================================================================
        # í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§ ì½œë°± í´ë˜ìŠ¤
        # ================================================================
        class TrainingCallback(Callback):
            """
            í•™ìŠµ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±
            
            ê¸°ëŠ¥:
            - ì—í¬í¬ë³„ ì†ì‹¤ ì¶œë ¥
            - ì§„í–‰ë¥  í‘œì‹œ (í”„ë¡œê·¸ë ˆìŠ¤ ë°”)
            - ê²€ì¦ ì†ì‹¤ ê°œì„ /ì•…í™” ì•Œë¦¼
            """
            def __init__(self, total_epochs, batch_size):
                super().__init__()
                self.total_epochs = total_epochs
                self.batch_size = batch_size
                self.prev_val_loss = None
                
            def on_train_begin(self, logs=None):
                """í•™ìŠµ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
                print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - ì´ {self.total_epochs} ì—í¬í¬")
                print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
                
            def on_epoch_begin(self, epoch, logs=None):
                """ê° ì—í¬í¬ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
                print(f"\nâ³ Epoch {epoch + 1}/{self.total_epochs} ì‹œì‘...")
                
            def on_epoch_end(self, epoch, logs=None):
                """ê° ì—í¬í¬ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
                logs = logs or {}
                loss = logs.get('loss', 0)
                val_loss = logs.get('val_loss', 0)
                
                # ì§„í–‰ë¥  ê³„ì‚° ë° í”„ë¡œê·¸ë ˆìŠ¤ ë°” í‘œì‹œ
                progress = (epoch + 1) / self.total_epochs * 100
                bar_length = 30
                filled_length = int(bar_length * (epoch + 1) // self.total_epochs)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                
                print(f"âœ… Epoch {epoch + 1}/{self.total_epochs} [{bar}] {progress:.1f}%")
                print(f"   ğŸ“‰ Loss: {loss:.6f} | Val_Loss: {val_loss:.6f}")
                
                # ê²€ì¦ ì†ì‹¤ ë³€í™” ë¶„ì„
                if epoch > 0 and self.prev_val_loss is not None:
                    if val_loss < self.prev_val_loss:
                        print(f"   ğŸ“ˆ ê²€ì¦ ì†ì‹¤ ê°œì„ : {self.prev_val_loss:.6f} â†’ {val_loss:.6f}")
                    elif val_loss > self.prev_val_loss * 1.1:  # 10% ì´ìƒ ì¦ê°€
                        print(f"   âš ï¸  ê²€ì¦ ì†ì‹¤ ì¦ê°€: {self.prev_val_loss:.6f} â†’ {val_loss:.6f}")
                
                self.prev_val_loss = val_loss
                
            def on_train_end(self, logs=None):
                """í•™ìŠµ ì™„ë£Œ ì‹œ í˜¸ì¶œ"""
                print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")

        # ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
        history = model.fit(
            trainX, trainY,
            epochs=int(r_epochs),
            batch_size=int(r_batchSize),
            validation_split=float(r_validationSplit),
            verbose=1,
            callbacks=[TrainingCallback(int(r_epochs), int(r_batchSize))]
        )

        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        model.save(model_file_path)
        print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

        # í•™ìŠµ ì†ì‹¤ ê·¸ë˜í”„ ìƒì„± ë° ì €ì¥
        plt.figure(figsize=(12, 4))
        plt.plot(history.history['loss'], label='Training loss')
        plt.plot(history.history['val_loss'], label='Validation loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(f'{modelName} - Training Loss')
        plt.legend()
        plt.savefig(training_loss_path)
        plt.close()

    # ====================================================================
    # 9ë‹¨ê³„: ì˜ˆì¸¡ ìˆ˜í–‰
    # ====================================================================
    print(f"\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    print(f"ğŸ“Š ì˜ˆì¸¡í•  ìƒ˜í”Œ ìˆ˜: {len(testX)}")
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    batch_size_pred = 32
    predictions = []
    total_batches = (len(testX) + batch_size_pred - 1) // batch_size_pred
    
    for i in range(0, len(testX), batch_size_pred):
        batch_end = min(i + batch_size_pred, len(testX))
        batch_data = testX[i:batch_end]
        
        # ë°°ì¹˜ ì˜ˆì¸¡
        batch_pred = model.predict(batch_data, verbose=0)
        predictions.append(batch_pred)
        
        # ì§„í–‰ë¥  í‘œì‹œ
        current_batch = (i // batch_size_pred) + 1
        progress = current_batch / total_batches * 100
        bar_length = 25
        filled_length = int(bar_length * current_batch // total_batches)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        print(f"\râ³ ì˜ˆì¸¡ ì§„í–‰: [{bar}] {progress:.1f}% ({current_batch}/{total_batches} ë°°ì¹˜)", end='', flush=True)
    
    # ëª¨ë“  ë°°ì¹˜ ì˜ˆì¸¡ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
    prediction = np.vstack(predictions)
    print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ! ì´ {len(prediction)}ê°œ ìƒ˜í”Œ ì˜ˆì¸¡ë¨")

    # ====================================================================
    # 10ë‹¨ê³„: ì˜ˆì¸¡ ê²°ê³¼ ì—­ë³€í™˜ (ì—­ì •ê·œí™”)
    # ====================================================================
    # ì •ê·œí™”ëœ ê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë˜ëŒë¦¼
    
    # ì˜ˆì¸¡ê°’ ì—­ë³€í™˜
    mean_values_pred = np.repeat(scaler.mean_[np.newaxis, :], prediction.shape[0], axis=0)
    mean_values_pred[:, target_idx] = np.squeeze(prediction)
    y_pred = scaler.inverse_transform(mean_values_pred)[:, target_idx]

    # ì‹¤ì œê°’ ì—­ë³€í™˜
    mean_values_testY = np.repeat(scaler.mean_[np.newaxis, :], testY.shape[0], axis=0)
    mean_values_testY[:, target_idx] = np.squeeze(testY)
    testY_original = scaler.inverse_transform(mean_values_testY)[:, target_idx]
    
    # ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ë‚ ì§œ ì¶”ì¶œ
    valid_test_dates = test_dates[seq_len : seq_len + len(testY_original)]

    # ====================================================================
    # 11ë‹¨ê³„: ì˜ˆì¸¡ ê²°ê³¼ JSON ì €ì¥
    # ====================================================================
    print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
    prediction_summary = save_predictions_to_json(
        modelName, 
        valid_test_dates, 
        testY_original, 
        y_pred, 
        targetColumn
    )

    # ====================================================================
    # 12ë‹¨ê³„: ì‹œê°í™” ê·¸ë˜í”„ ìƒì„±
    # ====================================================================
    
    # ì „ì²´ ê¸°ê°„ ê·¸ë˜í”„ (ì›ë³¸ + ì‹¤ì œ + ì˜ˆì¸¡)
    plt.figure(figsize=(15, 5))
    plt.plot(dates, original_open, color='green', label=f'Original {targetColumn}', alpha=0.7)
    plt.plot(valid_test_dates, testY_original, color='blue', label=f'Actual {targetColumn}')
    plt.plot(valid_test_dates, y_pred, color='red', linestyle='--', label=f'Predicted {targetColumn}')
    plt.xlabel(dateColumn)
    plt.ylabel(f'{targetColumn} Value')
    plt.title(f'{modelName} - Prediction Results')
    plt.legend()
    plt.savefig(total_graph_path)
    plt.close()

    # ìµœê·¼ 50ê°œ í¬ì¸íŠ¸ í™•ëŒ€ ê·¸ë˜í”„
    zoom_start = max(0, len(valid_test_dates) - 50)
    plt.figure(figsize=(15, 5))
    plt.plot(valid_test_dates[zoom_start:], testY_original[zoom_start:], color='blue', label=f'Actual {targetColumn}')
    plt.plot(valid_test_dates[zoom_start:], y_pred[zoom_start:], color='red', linestyle='--', label=f'Predicted {targetColumn}')
    plt.xlabel(dateColumn)
    plt.ylabel(f'{targetColumn} Value')
    plt.title(f'{modelName} - Recent Predictions (Last 50 points)')
    plt.legend()
    plt.savefig(diff_graph_path)
    plt.close()

    # ====================================================================
    # 13ë‹¨ê³„: ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    # ====================================================================
    print(f"\nğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    
    # ====================================================================
    # MAPE ê³„ì‚° í•¨ìˆ˜ (ìƒì„¸ ì¶œë ¥ í¬í•¨)
    # ====================================================================
    def mean_absolute_percentage_error(y_true, y_pred, valid_test_dates):
        """
        MAPE (Mean Absolute Percentage Error) ê³„ì‚° í•¨ìˆ˜
        
        Args:
            y_true: ì‹¤ì œê°’
            y_pred: ì˜ˆì¸¡ê°’
            valid_test_dates: ë‚ ì§œ ì •ë³´
        
        Returns:
            float: MAPE ê°’ (%)
                   ì„ê³„ê°’ ì´ˆê³¼ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 999.0 ë°˜í™˜
        
        íŠ¹ì§•:
            - ì„ê³„ê°’(eps=9) ì´ìƒì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ MAPE ê³„ì‚°
            - ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‘œ í˜•íƒœë¡œ ì¶œë ¥
            - ì˜¤ì°¨ ë¶„ì„ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ€/ìµœì†Œ ì˜¤ì°¨)
        """
        eps = 9  # ì„ê³„ê°’ (ì´ ê°’ë³´ë‹¤ ì‘ì€ ì‹¤ì œê°’ì€ MAPE ê³„ì‚°ì—ì„œ ì œì™¸)
        mask = y_true > eps
        
        print(f"\nğŸ“Š MAPE ê³„ì‚° ì •ë³´:")
        print(f"   - ì„ê³„ê°’(eps): {eps}")
        print(f"   - ì „ì²´ ë°ì´í„°: {len(y_true)}ê°œ")
        print(f"   - ì„ê³„ê°’ ì´ˆê³¼ ë°ì´í„°: {np.sum(mask)}ê°œ")
        
        if np.sum(mask) == 0:
            print("   âš ï¸ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 999.0
        
        # âœ… ì„ê³„ê°’ ì´ˆê³¼í•˜ëŠ” ëª¨ë“  ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ ì¶œë ¥
        filtered_dates = valid_test_dates[mask]
        filtered_true = y_true[mask]
        filtered_pred = y_pred[mask]
        
        print(f"\nğŸ“‹ ì„ê³„ê°’ ì´ˆê³¼ ë°ì´í„° ì „ì²´ ({len(filtered_true)}ê°œ):")
        print(f"{'='*90}")
        print(f"{'ë‚ ì§œ/ì‹œê°„':<25} {'ì‹¤ì œê°’':>12} {'ì˜ˆì¸¡ê°’':>12} {'ì˜¤ì°¨':>12} {'ì˜¤ì°¨ìœ¨(%)':>12}")
        print(f"{'-'*90}")
        
        # ê° ë°ì´í„° í¬ì¸íŠ¸ë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥
        for i in range(len(filtered_true)):
            date_str = filtered_dates.iloc[i].strftime('%Y-%m-%d %H:%M:%S') if hasattr(filtered_dates.iloc[i], 'strftime') else str(filtered_dates.iloc[i])
            true_val = filtered_true[i]
            pred_val = filtered_pred[i]
            error = pred_val - true_val
            error_pct = abs(error / true_val * 100)
            
            print(f"{date_str:<25} {true_val:>12.4f} {pred_val:>12.4f} {error:>12.4f} {error_pct:>12.2f}")
        
        print(f"{'='*90}")
        
        # MAPE ê³„ì‚°
        mape_value = np.mean(np.abs((filtered_pred - filtered_true) / filtered_true)) * 100
        print(f"\n   âœ… ê³„ì‚°ëœ MAPE: {mape_value:.2f}%")
        
        # ì¶”ê°€ ì˜¤ì°¨ ë¶„ì„
        errors = filtered_pred - filtered_true
        print(f"\nğŸ“Š ì˜¤ì°¨ ë¶„ì„:")
        print(f"   - í‰ê·  ì˜¤ì°¨: {np.mean(errors):.4f}")
        print(f"   - ì˜¤ì°¨ í‘œì¤€í¸ì°¨: {np.std(errors):.4f}")
        print(f"   - ìµœëŒ€ ê³¼ëŒ€ì˜ˆì¸¡: {np.max(errors):.4f}")
        print(f"   - ìµœëŒ€ ê³¼ì†Œì˜ˆì¸¡: {np.min(errors):.4f}")
        print(f"   - ê³¼ëŒ€ì˜ˆì¸¡ ë¹„ìœ¨: {np.sum(errors > 0) / len(errors) * 100:.1f}%")
        print(f"   - ê³¼ì†Œì˜ˆì¸¡ ë¹„ìœ¨: {np.sum(errors < 0) / len(errors) * 100:.1f}%")
        
        return mape_value

    # sklearn ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    try:
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        sklearn_available = True
    except ImportError:
        print("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì§€í‘œë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.")
        sklearn_available = False
    
    # ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    mape = mean_absolute_percentage_error(testY_original, y_pred, valid_test_dates)
    accuracy = 100 - mape if not np.isnan(mape) else 0
    
    # ì¶”ê°€ ì§€í‘œë“¤ ê³„ì‚°
    if sklearn_available:
        # sklearn ì‚¬ìš©
        mse = mean_squared_error(testY_original, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(testY_original, y_pred)
        r2 = r2_score(testY_original, y_pred)
    else:
        # ìˆ˜ë™ ê³„ì‚°
        mse = np.mean((testY_original - y_pred) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(testY_original - y_pred))
        
        # RÂ² ìˆ˜ë™ ê³„ì‚°
        ss_res = np.sum((testY_original - y_pred) ** 2)  # ì”ì°¨ ì œê³±í•©
        ss_tot = np.sum((testY_original - np.mean(testY_original)) ** 2)  # ì´ ì œê³±í•©
        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
    
    # ë°©í–¥ì„± ì •í™•ë„ (ìƒìŠ¹/í•˜ë½ ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„)
    if len(testY_original) > 1:
        actual_direction = np.diff(testY_original) > 0  # ì‹¤ì œ ì¦ê°€/ê°ì†Œ
        pred_direction = np.diff(y_pred) > 0  # ì˜ˆì¸¡ ì¦ê°€/ê°ì†Œ
        direction_accuracy = np.mean(actual_direction == pred_direction) * 100
    else:
        direction_accuracy = 0
    
    # ====================================================================
    # 14ë‹¨ê³„: ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
    # ====================================================================
    # âœ… ì „ì²´ ë°ì´í„°ì˜ ì˜ˆì¸¡ ê²°ê³¼ ëª¨ë‘ ì¶œë ¥
    print(f"\nğŸ“‹ ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ({len(testY_original)}ê°œ):")
    print(f"{'='*90}")
    print(f"{'ë‚ ì§œ/ì‹œê°„':<25} {'ì‹¤ì œê°’':>12} {'ì˜ˆì¸¡ê°’':>12} {'ì˜¤ì°¨':>12} {'ì˜¤ì°¨ìœ¨(%)':>12}")
    print(f"{'-'*90}")
    
    for i in range(len(testY_original)):
        date_str = valid_test_dates.iloc[i].strftime('%Y-%m-%d %H:%M:%S') if hasattr(valid_test_dates.iloc[i], 'strftime') else str(valid_test_dates.iloc[i])
        true_val = testY_original[i]
        pred_val = y_pred[i]
        error = pred_val - true_val
        error_pct = abs(error / true_val * 100) if true_val != 0 else 0
        
        print(f"{date_str:<25} {true_val:>12.4f} {pred_val:>12.4f} {error:>12.4f} {error_pct:>12.2f}")
    
    print(f"{'='*90}")
    
    # ====================================================================
    # 15ë‹¨ê³„: ì„±ëŠ¥ ì§€í‘œ ìš”ì•½ ì¶œë ¥
    # ====================================================================
    print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼:")
    print(f"   ğŸ¯ MAPE: {mape:.2f}%")
    print(f"   ğŸ“ˆ ì •í™•ë„: {accuracy:.2f}%")
    print(f"   ğŸ“ MAE: {mae:.4f}")
    print(f"   ğŸ“ RMSE: {rmse:.4f}")
    print(f"   ğŸ” RÂ² Score: {r2:.4f}")
    print(f"   ğŸ§­ ë°©í–¥ì„± ì •í™•ë„: {direction_accuracy:.2f}%")
    
    # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
    if accuracy >= 90:
        grade = "ğŸ† ìš°ìˆ˜"
    elif accuracy >= 80:
        grade = "ğŸ¥‡ ì–‘í˜¸"
    elif accuracy >= 70:
        grade = "ğŸ¥ˆ ë³´í†µ"
    elif accuracy >= 60:
        grade = "ğŸ¥‰ ê°œì„ í•„ìš”"
    else:
        grade = "âŒ ë¶ˆëŸ‰"
    
    print(f"   ğŸ“Š ì„±ëŠ¥ ë“±ê¸‰: {grade}")
    
    # ì˜ˆì¸¡ ë²”ìœ„ ë¶„ì„
    pred_min, pred_max = np.min(y_pred), np.max(y_pred)
    actual_min, actual_max = np.min(testY_original), np.max(testY_original)
    print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ ë²”ìœ„ ë¶„ì„:")
    print(f"   ì‹¤ì œê°’ ë²”ìœ„: {actual_min:.3f} ~ {actual_max:.3f}")
    print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {pred_min:.3f} ~ {pred_max:.3f}")
    
    # ê³¼ì˜ˆì¸¡/ì†Œì˜ˆì¸¡ ë¹„ìœ¨
    over_predict = np.sum(y_pred > testY_original) / len(y_pred) * 100
    under_predict = 100 - over_predict
    print(f"   ê³¼ì˜ˆì¸¡ ë¹„ìœ¨: {over_predict:.1f}%")
    print(f"   ì†Œì˜ˆì¸¡ ë¹„ìœ¨: {under_predict:.1f}%")

    # (ì¤‘ë³µëœ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì½”ë“œ - ì œê±°í•˜ì§€ ì•Šê³  ìœ ì§€)
    try:
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        sklearn_available = True
    except ImportError:
        print("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì§€í‘œë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.")
        sklearn_available = False
    
    mape = mean_absolute_percentage_error(testY_original, y_pred, valid_test_dates)
    accuracy = 100 - mape if not np.isnan(mape) else 0
    
    if sklearn_available:
        mse = mean_squared_error(testY_original, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(testY_original, y_pred)
        r2 = r2_score(testY_original, y_pred)
    else:
        mse = np.mean((testY_original - y_pred) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(testY_original - y_pred))
        
        ss_res = np.sum((testY_original - y_pred) ** 2)
        ss_tot = np.sum((testY_original - np.mean(testY_original)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
    
    if len(testY_original) > 1:
        actual_direction = np.diff(testY_original) > 0
        pred_direction = np.diff(y_pred) > 0
        direction_accuracy = np.mean(actual_direction == pred_direction) * 100
    else:
        direction_accuracy = 0
    
    # (ì¤‘ë³µ ì¶œë ¥ - ì œê±°í•˜ì§€ ì•Šê³  ìœ ì§€)
    print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼:")
    print(f"   ğŸ¯ MAPE: {mape:.2f}%")
    print(f"   ğŸ“ˆ ì •í™•ë„: {accuracy:.2f}%")
    print(f"   ğŸ“ MAE: {mae:.4f}")
    print(f"   ğŸ“ RMSE: {rmse:.4f}")
    print(f"   ğŸ” RÂ² Score: {r2:.4f}")
    print(f"   ğŸ§­ ë°©í–¥ì„± ì •í™•ë„: {direction_accuracy:.2f}%")
    
    if accuracy >= 90:
        grade = "ğŸ† ìš°ìˆ˜"
    elif accuracy >= 80:
        grade = "ğŸ¥‡ ì–‘í˜¸"
    elif accuracy >= 70:
        grade = "ğŸ¥ˆ ë³´í†µ"
    elif accuracy >= 60:
        grade = "ğŸ¥‰ ê°œì„ í•„ìš”"
    else:
        grade = "âŒ ë¶ˆëŸ‰"
    
    print(f"   ğŸ“Š ì„±ëŠ¥ ë“±ê¸‰: {grade}")
    
    pred_min, pred_max = np.min(y_pred), np.max(y_pred)
    actual_min, actual_max = np.min(testY_original), np.max(testY_original)
    print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ ë²”ìœ„ ë¶„ì„:")
    print(f"   ì‹¤ì œê°’ ë²”ìœ„: {actual_min:.3f} ~ {actual_max:.3f}")
    print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {pred_min:.3f} ~ {pred_max:.3f}")
    
    over_predict = np.sum(y_pred > testY_original) / len(y_pred) * 100
    under_predict = 100 - over_predict
    print(f"   ê³¼ì˜ˆì¸¡ ë¹„ìœ¨: {over_predict:.1f}%")
    print(f"   ì†Œì˜ˆì¸¡ ë¹„ìœ¨: {under_predict:.1f}%")

    # ====================================================================
    # 16ë‹¨ê³„: ì„¤ì • ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    # ====================================================================
    # ëª¨ë¸ ì„¤ì •ì„ JSONìœ¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì˜ˆì¸¡ ì‹œ ì‚¬ìš©)
    with open(os.path.join(model_path, f"{modelName}_config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4, ensure_ascii=False)

    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ì˜ˆì¸¡ ì‹œ ë™ì¼í•œ ì •ê·œí™” ì ìš© í•„ìš”)
    joblib.dump(scaler, os.path.join(model_path, f"{modelName}_scaler.pkl"))

    # ====================================================================
    # 17ë‹¨ê³„: ê²°ê³¼ ë°˜í™˜
    # ====================================================================
    # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°˜í™˜ê°’ êµ¬ì„±
    result = {
        "status": "success",
        "modelName": modelName,
        "training_loss_img": f"graphImage/{modelName}_trainingLoss.png",
        "total_graph_img": f"graphImage/{modelName}_totalgraph.png",
        "diff_graph_img": f"graphImage/{modelName}_diffgraph.png",
        "mape": round(mape, 2),
        "accuracy": round(accuracy, 2),
        "mae": round(mae, 4),
        "rmse": round(rmse, 4),
        "r2_score": round(r2, 4),
        "direction_accuracy": round(direction_accuracy, 2),
        "prediction_file": f"predictions/{modelName}_predictions.json",
        "prediction_summary": {
            "total_predictions": len(y_pred),
            "prediction_period": {
                "start_date": convert_numpy_to_json_serializable(valid_test_dates.iloc[0]) if len(valid_test_dates) > 0 else None,
                "end_date": convert_numpy_to_json_serializable(valid_test_dates.iloc[-1]) if len(valid_test_dates) > 0 else None
            },
            "value_statistics": {
                "actual_min": convert_numpy_to_json_serializable(np.min(testY_original)),
                "actual_max": convert_numpy_to_json_serializable(np.max(testY_original)),
                "actual_mean": convert_numpy_to_json_serializable(np.mean(testY_original)),
                "predicted_min": convert_numpy_to_json_serializable(np.min(y_pred)),
                "predicted_max": convert_numpy_to_json_serializable(np.max(y_pred)),
                "predicted_mean": convert_numpy_to_json_serializable(np.mean(y_pred))
            }
        }
    }
    
    # ìµœê·¼ Nê°œ ì˜ˆì¸¡ê°’ì„ ì§ì ‘ ê²°ê³¼ì— í¬í•¨ (ìµœê·¼ 10ê°œ)
    recent_predictions_count = min(10, len(y_pred))
    if recent_predictions_count > 0:
        result["recent_predictions"] = []
        for i in range(-recent_predictions_count, 0):
            result["recent_predictions"].append({
                "date": convert_numpy_to_json_serializable(valid_test_dates.iloc[i]),
                "actual": convert_numpy_to_json_serializable(testY_original[i]),
                "predicted": convert_numpy_to_json_serializable(y_pred[i]),
                "error": convert_numpy_to_json_serializable(abs(y_pred[i] - testY_original[i]))
            })

    result['is_new_model'] = is_new_model
    return result

# ============================================================================
# ë©€í‹° ì‹¤í—˜ ìë™í™” í•¨ìˆ˜
# ============================================================================

def run_multiple_experiments(config_file="experiments.json"):
    """
    ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ìë™í™” í•¨ìˆ˜
    
    Args:
        config_file (str): ì‹¤í—˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    
    Returns:
        list: ê° ì‹¤í—˜ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    
    ê¸°ëŠ¥:
        1. JSON ì„¤ì • íŒŒì¼ì—ì„œ ì‹¤í—˜ ëª©ë¡ ë¡œë“œ
        2. ê° ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        3. ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë° ìˆœìœ„ ìƒì„±
        4. ì¢…í•© ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    
    ìƒì„± íŒŒì¼:
        - comprehensive_experiment_results.json: ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ì¢…í•©
        - predictions/{model}_predictions.json: ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ ê²°ê³¼
        - graphImage/{model}_*.png: ê° ëª¨ë¸ë³„ ê·¸ë˜í”„
        - saved_models/{model}.*: ê° ëª¨ë¸ë³„ ì €ì¥ íŒŒì¼
    """
    # ì‹¤í—˜ ì„¤ì • ë¡œë“œ
    experiments = load_experiments_config(config_file)
    
    if not experiments:
        print("âŒ ì‹¤í–‰í•  ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ”¬ ì´ {len(experiments)}ê°œì˜ ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = []
    total_start_time = time.time()
    
    # ê° ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰
    for i, experiment in enumerate(experiments):
        try:
            result = run_single_experiment(experiment, i)
            results.append(result)
            
            if result['status'] == 'success':
                print(f"âœ… {experiment['name']} ì™„ë£Œ - ì •í™•ë„: {result['accuracy']:.2f}%")
                print(f"   ğŸ“Š ì˜ˆì¸¡ ë°ì´í„° ìˆ˜: {result['prediction_summary']['total_predictions']}ê°œ")
            else:
                print(f"âŒ {experiment['name']} ì‹¤íŒ¨: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
        except Exception as e:
            print(f"âŒ {experiment['name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            results.append({"status": "error", "message": str(e), "experiment_name": experiment['name']})
    
    # ì´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    total_end_time = time.time()
    total_time = round(total_end_time - total_start_time, 2)
    
    # ========================================================================
    # ê²°ê³¼ ìš”ì•½
    # ========================================================================
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_time}ì´ˆ")
    print(f"âœ… ì„±ê³µ: {len([r for r in results if r['status'] == 'success'])}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {len([r for r in results if r['status'] == 'error'])}ê°œ")
    
    # ì„±ê³µí•œ ì‹¤í—˜ë“¤ì˜ ì •í™•ë„ ìˆœìœ„
    successful_results = [r for r in results if r['status'] == 'success']
    if successful_results:
        successful_results.sort(key=lambda x: x['accuracy'], reverse=True)
        print(f"\nğŸ† ì •í™•ë„ ìˆœìœ„:")
        for i, result in enumerate(successful_results, 1):
            print(f"{i}. {result['experiment_name']}: {result['accuracy']:.2f}% (MAPE: {result['mape']:.2f}%)")
            print(f"   ğŸ“ˆ RÂ² Score: {result.get('r2_score', 'N/A')}, ë°©í–¥ì„± ì •í™•ë„: {result.get('direction_accuracy', 'N/A'):.1f}%")
    
    # ========================================================================
    # ì¢…í•© ê²°ê³¼ JSON ìƒì„±
    # ========================================================================
    comprehensive_results = {
        "experiment_summary": {
            "total_experiments": len(experiments),
            "successful_experiments": len(successful_results),
            "failed_experiments": len(results) - len(successful_results),
            "total_execution_time_seconds": total_time,
            "start_timestamp": datetime.now().isoformat(),
            "completion_timestamp": datetime.now().isoformat()
        },
        "performance_ranking": [
            {
                "rank": i + 1,
                "experiment_name": result['experiment_name'],
                "model_name": result['modelName'],
                "accuracy": result['accuracy'],
                "mape": result['mape'],
                "mae": result.get('mae', None),
                "rmse": result.get('rmse', None),
                "r2_score": result.get('r2_score', None),
                "direction_accuracy": result.get('direction_accuracy', None),
                "total_predictions": result['prediction_summary']['total_predictions'] if 'prediction_summary' in result else 0
            }
            for i, result in enumerate(successful_results)
        ],
        "detailed_results": results,
        "prediction_files": [
            {
                "experiment_name": result['experiment_name'],
                "model_name": result['modelName'],
                "prediction_file_path": result.get('prediction_file', 'N/A'),
                "prediction_count": result['prediction_summary']['total_predictions'] if 'prediction_summary' in result else 0,
                "recent_predictions": result.get('recent_predictions', [])
            }
            for result in successful_results
        ]
    }
    
    # ì¢…í•© ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    comprehensive_results_file = "comprehensive_experiment_results.json"
    with open(comprehensive_results_file, "w", encoding="utf-8") as f:
        json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=convert_numpy_to_json_serializable)
    
    print(f"\nğŸ’¾ ì¢…í•© ê²°ê³¼ê°€ '{comprehensive_results_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“ ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” 'predictions/' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì˜ˆì¸¡ íŒŒì¼ ëª©ë¡ ì¶œë ¥
    if successful_results:
        print(f"\nğŸ“„ ìƒì„±ëœ ì˜ˆì¸¡ íŒŒì¼ ëª©ë¡:")
        for result in successful_results:
            if 'prediction_file' in result:
                print(f"   - {result['prediction_file']}")
    
    return results

# ============================================================================
# ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„ í•¨ìˆ˜
# ============================================================================

def analyze_prediction_file(prediction_file_path):
    """
    ì €ì¥ëœ ì˜ˆì¸¡ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½ ì •ë³´ ì¶œë ¥
    
    Args:
        prediction_file_path (str): ë¶„ì„í•  ì˜ˆì¸¡ íŒŒì¼ ê²½ë¡œ
    
    ê¸°ëŠ¥:
        - ëª¨ë¸ ì •ë³´ ì¶œë ¥ (ëª¨ë¸ëª…, íƒ€ê²Ÿ ì»¬ëŸ¼, ì˜ˆì¸¡ ê°œìˆ˜)
        - í†µê³„ ì •ë³´ ì¶œë ¥ (ì‹¤ì œê°’/ì˜ˆì¸¡ê°’ ë²”ìœ„, MAE, RMSE)
        - ìµœê·¼ 5ê°œ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
    """
    try:
        with open(prediction_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"\nğŸ“Š ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„: {prediction_file_path}")
        print(f"{'='*50}")
        print(f"ëª¨ë¸ëª…: {data['model_name']}")
        print(f"íƒ€ê²Ÿ ì»¬ëŸ¼: {data['target_column']}")
        print(f"ì˜ˆì¸¡ ê°œìˆ˜: {data['prediction_count']}")
        print(f"ìƒì„± ì‹œê°„: {data['timestamp']}")
        
        stats = data['statistics']
        print(f"\nğŸ“ˆ í†µê³„ ì •ë³´:")
        print(f"   ì‹¤ì œê°’ ë²”ìœ„: {stats['actual_min']:.3f} ~ {stats['actual_max']:.3f}")
        print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {stats['predicted_min']:.3f} ~ {stats['predicted_max']:.3f}")
        print(f"   í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {stats['mean_absolute_error']:.4f}")
        print(f"   ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨ (RMSE): {stats['rmse']:.4f}")
        
        # ìµœê·¼ 5ê°œ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        predictions = data['predictions']
        print(f"\nğŸ” ìµœê·¼ 5ê°œ ì˜ˆì¸¡ ê²°ê³¼:")
        for pred in predictions[-5:]:
            error_pct = pred['percentage_error']
            print(f"   {pred['date'][:19]}: ì‹¤ì œ={pred['actual_value']:.3f}, ì˜ˆì¸¡={pred['predicted_value']:.3f}, ì˜¤ì°¨={error_pct:.2f}%")
            
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ í•¨ìˆ˜ë“¤
# ============================================================================

def get_model_history(model_name=None, limit=10):
    """
    ëª¨ë¸ë³„ ì‹¤í—˜ ì´ë ¥ ì¡°íšŒ
    
    Args:
        model_name (str): ì¡°íšŒí•  ëª¨ë¸ëª… (Noneì´ë©´ ì „ì²´ ì¡°íšŒ)
        limit (int): ì¡°íšŒí•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
        DataFrame: ì‹¤í—˜ ì´ë ¥ ë°ì´í„°
            - model_name: ëª¨ë¸ëª…
            - experiment_name: ì‹¤í—˜ëª…
            - accuracy, mape, r2_score: ì„±ëŠ¥ ì§€í‘œ
            - created_at: ìƒì„± ì¼ì‹œ
    """
    try:
        engine = get_db_engine()
        
        if model_name:
            # íŠ¹ì • ëª¨ë¸ì˜ ì´ë ¥ ì¡°íšŒ
            query = f"""
            SELECT 
                m.model_name,
                m.epochs as model_epochs,
                m.sequence_length,
                e.experiment_id,
                e.experiment_name,
                e.accuracy,
                e.mape,
                e.r2_score,
                e.created_at
            FROM carbontwin.lstm_experiment e
            JOIN carbontwin.lstm_model m ON e.model_id = m.model_id
            WHERE m.model_name = '{model_name}'
            ORDER BY e.created_at DESC
            LIMIT {limit}
            """
        else:
            # ì „ì²´ ëª¨ë¸ ì´ë ¥ ì¡°íšŒ
            query = f"""
            SELECT 
                m.model_name,
                m.epochs as model_epochs,
                e.experiment_id,
                e.experiment_name,
                e.accuracy,
                e.mape,
                e.r2_score,
                e.created_at
            FROM carbontwin.lstm_experiment e
            JOIN carbontwin.lstm_model m ON e.model_id = m.model_id
            ORDER BY e.created_at DESC
            LIMIT {limit}
            """
        
        return pd.read_sql_query(query, engine)
        
    except Exception as e:
        print(f"âŒ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return None

def get_best_models(metric='accuracy', top_n=5):
    """
    ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì¡°íšŒ
    
    Args:
        metric (str): ì •ë ¬ ê¸°ì¤€ (accuracy/mape/rmse/r2_score)
        top_n (int): ì¡°íšŒí•  ìƒìœ„ ê°œìˆ˜
    
    Returns:
        DataFrame: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë°ì´í„°
            - model_name: ëª¨ë¸ëª…
            - experiment_name: ì‹¤í—˜ëª…
            - ì„±ëŠ¥ ì§€í‘œë“¤
            - íŒŒì¼ ê²½ë¡œë“¤
    
    ì •ë ¬ ë°©ì‹:
        - accuracy, r2_score: ë‚´ë¦¼ì°¨ìˆœ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        - mape, rmse: ì˜¤ë¦„ì°¨ìˆœ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    """
    try:
        engine = get_db_engine()
        # ì§€í‘œì— ë”°ë¼ ì •ë ¬ ìˆœì„œ ê²°ì •
        order = 'ASC' if metric in ['mape', 'rmse'] else 'DESC'
        
        query = f"""
        SELECT 
            m.model_name,
            m.epochs,
            m.sequence_length,
            m.prediction_days,
            e.experiment_name,
            e.accuracy,
            e.mape,
            e.rmse,
            e.r2_score,
            e.model_file_path,
            e.prediction_file_path,
            e.created_at
        FROM carbontwin.lstm_experiment e
        JOIN carbontwin.lstm_model m ON e.model_id = m.model_id
        WHERE e.status = 'success'
        ORDER BY e.{metric} {order}
        LIMIT {top_n}
        """
        
        return pd.read_sql_query(query, engine)
        
    except Exception as e:
        print(f"âŒ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return None

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# ============================================================================

if __name__ == "__main__":
    """
    í”„ë¡œê·¸ë¨ ì‹œì‘ì 
    
    ì‹¤í–‰ ëª¨ë“œ:
        1. ë©€í‹° ì‹¤í—˜ (JSON íŒŒì¼ ê¸°ë°˜)
        2. ë‹¨ì¼ ì‹¤í—˜ (ìˆ˜ë™ ì…ë ¥)
        3. ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„
        4. DBì—ì„œ ëª¨ë¸ ì´ë ¥ ì¡°íšŒ
        5. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ
    
    íŠ¹ì§•:
        - 07:00~16:45 ì‹œê°„ëŒ€ ë°ì´í„°ë§Œ ì‚¬ìš©
        - 80/20 ë°ì´í„° ë¶„í• 
        - ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
        - PostgreSQL DB ì—°ë™
    """
    print("ğŸ§ª LSTM ë©€í‹° ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œ (ì˜ˆì¸¡ê°’ ê¸°ë¡ + DB ì €ì¥)")
    print("=" * 60)
    print("ğŸ“‹ ì´ ì‹œìŠ¤í…œì˜ ê¸°ëŠ¥:")
    print("   1. ì—¬ëŸ¬ LSTM ëª¨ë¸ì„ ìë™ìœ¼ë¡œ í•™ìŠµ ë° í‰ê°€")
    print("   2. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒì„¸í•œ JSON íŒŒì¼ë¡œ ì €ì¥")
    print("   3. ì„±ëŠ¥ ì§€í‘œë³„ ëª¨ë¸ ìˆœìœ„ ìë™ ìƒì„±")
    print("   4. ì‹œê°í™” ê·¸ë˜í”„ ë° ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±")
    print("   5. PostgreSQL DBì— ì‹¤í—˜ ê²°ê³¼ ìë™ ì €ì¥")
    print("   6. 07:00~16:45 ì‹œê°„ëŒ€ ë°ì´í„°ë§Œ ì‚¬ìš© (80/20 split)")
    print("=" * 60)
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    choice = input("ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\n"
                  "1. ë©€í‹° ì‹¤í—˜ (JSON íŒŒì¼ ê¸°ë°˜)\n"
                  "2. ë‹¨ì¼ ì‹¤í—˜ (ìˆ˜ë™ ì…ë ¥)\n"
                  "3. ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„\n"
                  "4. DBì—ì„œ ëª¨ë¸ ì´ë ¥ ì¡°íšŒ\n"
                  "5. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ\n"
                  "ì„ íƒ (1-5): ").strip()
    
    if choice == "1":
        # ë©€í‹° ì‹¤í—˜ ëª¨ë“œ
        print("\nğŸ“– ë©€í‹° ì‹¤í—˜ ëª¨ë“œ ì„¤ëª…:")
        print("   - experiments.json íŒŒì¼ì˜ ì„¤ì •ì— ë”°ë¼ ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ ì‹¤í–‰")
        print("   - ê° ì‹¤í—˜ë³„ë¡œ ëª¨ë¸ í•™ìŠµ, ì˜ˆì¸¡, ì„±ëŠ¥ í‰ê°€ë¥¼ ìë™í™”")
        print("   - ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì„±ëŠ¥ ìˆœìœ„í‘œ ìë™ ìƒì„±")
        
        config_file = input("ì„¤ì • íŒŒì¼ëª… (ê¸°ë³¸ê°’: experiments.json): ").strip() or "experiments.json"
        results = run_multiple_experiments(config_file)
        
        if results and any(r['status'] == 'success' for r in results):
            print(f"\nğŸ‰ ëª¨ë“  ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
            print(f"   - comprehensive_experiment_results.json (ì¢…í•© ê²°ê³¼)")
            print(f"   - predictions/ í´ë” (ê°œë³„ ì˜ˆì¸¡ íŒŒì¼ë“¤)")
            print(f"   - graphImage/ í´ë” (ì‹œê°í™” ê·¸ë˜í”„ë“¤)")
            print(f"   - saved_models/ í´ë” (í•™ìŠµëœ ëª¨ë¸ë“¤)")
            
    elif choice == "2":
        # ë‹¨ì¼ ì‹¤í—˜ ëª¨ë“œ
        print("\nğŸ“– ë‹¨ì¼ ì‹¤í—˜ ëª¨ë“œ ì•ˆë‚´:")
        print("   í˜„ì¬ ë‹¨ì¼ ì‹¤í—˜ì€ JSON ì„¤ì • íŒŒì¼ì„ í†µí•´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        print("   experiments.json íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
        
    elif choice == "3":
        # ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„ ëª¨ë“œ
        print("\nğŸ“– ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„ ëª¨ë“œ")
        prediction_file = input("ë¶„ì„í•  ì˜ˆì¸¡ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if prediction_file and os.path.exists(prediction_file):
            analyze_prediction_file(prediction_file)
        else:
            print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ íŒŒì¼ ëª©ë¡ ì¶œë ¥
            if os.path.exists(prediction_path):
                pred_files = [f for f in os.listdir(prediction_path) if f.endswith('_predictions.json')]
                if pred_files:
                    print(f"\nğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ íŒŒì¼ë“¤:")
                    for i, file in enumerate(pred_files, 1):
                        print(f"   {i}. {file}")
                        
    elif choice == "4":
        # ëª¨ë¸ ì´ë ¥ ì¡°íšŒ ëª¨ë“œ
        print("\nğŸ“Š ëª¨ë¸ í•™ìŠµ ì´ë ¥ ì¡°íšŒ")
        model_name = input("ëª¨ë¸ëª… ì…ë ¥ (ì „ì²´: Enter): ").strip() or None
        limit = input("ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ): ").strip() or "10"
        
        history = get_model_history(model_name, int(limit))
        if history is not None and not history.empty:
            print(f"\nğŸ“‹ ì¡°íšŒ ê²°ê³¼ ({len(history)}ê°œ):")
            print(history.to_string(index=False))
        else:
            print("âŒ ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    elif choice == "5":
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ ëª¨ë“œ
        print("\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ")
        metric = input("ì •ë ¬ ê¸°ì¤€ (accuracy/mape/r2_score/rmse): ").strip() or "accuracy"
        top_n = input("ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ 5ê°œ): ").strip() or "5"

        best_models = get_best_models(metric, int(top_n))
        if best_models is not None and not best_models.empty:
            print(f"\nğŸ¯ {metric} ê¸°ì¤€ ìƒìœ„ {top_n}ê°œ:")
            print(best_models.to_string(index=False))
        else:
            print("âŒ ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")