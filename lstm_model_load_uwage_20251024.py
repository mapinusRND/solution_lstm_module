# -*- coding: utf-8 -*-
"""
Title   : ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ LSTM (ê²€ì¦ ê¸°ëŠ¥ ì¶”ê°€)
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
Description: 
    - ì›ë³¸ ì½”ë“œì˜ ì—­ìŠ¤ì¼€ì¼ë§ ë°©ì‹ ìœ ì§€
    - í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë‚ ì§œ í•„í„°ë§Œ ì¶”ê°€
    - âœ¨ ì‹¤ì œ ë°ì´í„° vs ì˜ˆì¸¡ ë°ì´í„° ë¹„êµ ê¸°ëŠ¥ ì¶”ê°€
Version : 2.8
Date    : 2025-10-23
"""

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
import json
import joblib
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta

# í™˜ê²½ ì„¤ì •
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"
else:
    root = "/app/webfiles/lstm"

model_path = os.path.abspath(root + "/saved_models")

PREDICTION_EPS_THRESHOLD = 0

def get_db_engine():
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    return create_engine(connection_string)

def convert_to_serializable(obj):
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    return obj

def load_new_data(tablename, dateColumn, studyColumns, start_date=None, end_date=None, days_limit=7):
    try:
        engine = get_db_engine()
        
        if start_date is None and end_date is None:
            query = f"""
            SELECT {studyColumns},{dateColumn}
            FROM carbontwin.{tablename}
            WHERE {dateColumn} IS NOT NULL
            AND TO_CHAR({dateColumn}, 'MM-DD') NOT IN (
                '06-02', '06-13', '06-14', '06-15', '06-16', '06-17',
                '06-20', '06-21', '06-24', '06-25', '06-26', '06-28',
                '07-01', '07-08', '07-13', '07-14', '07-15', '07-16',
                '07-17', '07-18', '07-19', '07-21', '07-22'
            )
            ORDER BY {dateColumn} ASC
            """
        else:
            where_conditions = [f"{dateColumn} IS NOT NULL"]
            where_conditions.append(f"""TO_CHAR({dateColumn}, 'MM-DD') NOT IN (
                '06-02', '06-13', '06-14', '06-15', '06-16', '06-17',
                '06-20', '06-21', '06-24', '06-25', '06-26', '06-28',
                '07-01', '07-08', '07-13', '07-14', '07-15', '07-16',
                '07-17', '07-18', '07-19', '07-21', '07-22'
            )""")
            if start_date:
                where_conditions.append(f"{dateColumn} >= '{start_date}'")
            if end_date:
                where_conditions.append(f"{dateColumn} <= '{end_date}'")
            
            query = f"""
            SELECT {studyColumns},{dateColumn}
            FROM carbontwin.{tablename}
            WHERE {' AND '.join(where_conditions)}
            ORDER BY {dateColumn} ASC
            """
        
        data = pd.read_sql_query(query, engine)
        print(f"âœ… ë°ì´í„° ë¡œë“œ: {len(data)}í–‰")
        
        if len(data) > 0 and dateColumn in data.columns:
            min_date = pd.to_datetime(data[dateColumn]).min()
            max_date = pd.to_datetime(data[dateColumn]).max()
            print(f"   ğŸ“… ê¸°ê°„: {min_date} ~ {max_date}")
        
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None

def load_trained_model(model_name):
    try:
        model_file = os.path.join(model_path, f"{model_name}.h5")
        scaler_file = os.path.join(model_path, f"{model_name}_scaler.pkl")
        config_file = os.path.join(model_path, f"{model_name}_config.json")
        
        if not all(os.path.exists(f) for f in [model_file, scaler_file, config_file]):
            print(f"âŒ í•„ìš”í•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None
        
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ: {model_name}")
        
        model = load_model(model_file, compile=False)
        model.compile(optimizer='adam', loss='mse')
        scaler = joblib.load(scaler_file)
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"   - íƒ€ê²Ÿ: {config['targetColumn']}")
        print(f"   - ì‹œí€€ìŠ¤: {config['r_seqLen']}")
        
        return model, scaler, config
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None, None, None

# âœ¨ ì‹ ê·œ: ì‹¤ì œ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
def validate_with_actual_data(model, scaler, config, data, validation_days=7):
    """
    ìµœê·¼ Nì¼ì¹˜ ë°ì´í„°ë¡œ ëª¨ë¸ ì˜ˆì¸¡ vs ì‹¤ì œê°’ ë¹„êµ
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        scaler: ìŠ¤ì¼€ì¼ëŸ¬
        config: ëª¨ë¸ ì„¤ì •
        data: ì „ì²´ ë°ì´í„°
        validation_days: ê²€ì¦í•  ì¼ìˆ˜ (ê¸°ë³¸ 7ì¼)
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼ (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’, ì˜¤ì°¨ ë“±)
    """
    try:
        print(f"\n{'='*80}")
        print(f"ğŸ” ëª¨ë¸ ê²€ì¦ ì‹œì‘ (ìµœê·¼ {validation_days}ì¼ ë°ì´í„°)")
        print(f"{'='*80}")
        
        dateColumn = config['dateColumn']
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        seq_len = int(config['r_seqLen'])
        
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        target_idx = study_columns_list.index(targetColumn)
        
        # ë°ì´í„° ì¤€ë¹„
        data_for_prediction = data[study_columns_list].astype(float)
        dates = pd.to_datetime(data[dateColumn])
        actual_values = data[targetColumn].values
        
        # ê²€ì¦í•  ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ (15ë¶„ ê°„ê²© * 96 * Nì¼)
        validation_points = 96 * validation_days
        
        if len(data) < seq_len + validation_points:
            print(f"âš ï¸  ë°ì´í„° ë¶€ì¡±: ìµœì†Œ {seq_len + validation_points}ê°œ í•„ìš”, í˜„ì¬ {len(data)}ê°œ")
            validation_points = len(data) - seq_len
            if validation_points <= 0:
                print("âŒ ê²€ì¦í•  ë°ì´í„° ì—†ìŒ")
                return None
        
        # ì •ê·œí™”
        data_scaled = scaler.transform(data_for_prediction)
        
        # ê²€ì¦ ì‹œì‘ ì¸ë±ìŠ¤ (ìµœê·¼ Nì¼ì¹˜)
        validation_start_idx = len(data) - validation_points
        
        print(f"\nğŸ“Š ê²€ì¦ ì„¤ì •:")
        print(f"   - ê²€ì¦ ê¸°ê°„: {dates.iloc[validation_start_idx]} ~ {dates.iloc[-1]}")
        print(f"   - ê²€ì¦ í¬ì¸íŠ¸: {validation_points}ê°œ")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = []
        
        print(f"\nğŸ”„ ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
        
        for i in range(validation_start_idx, len(data)):
            # ì‹œí€€ìŠ¤ ì¶”ì¶œ (i ì´ì „ seq_lenê°œ)
            if i < seq_len:
                continue
                
            current_sequence = data_scaled[i - seq_len:i]
            
            # ëª¨ë¸ ì˜ˆì¸¡
            input_data = current_sequence.reshape(1, seq_len, len(study_columns_list))
            pred_scaled = model.predict(input_data, verbose=0)[0, 0]
            
            # ì—­ìŠ¤ì¼€ì¼ë§ (ì›ë³¸ê³¼ ë™ì¼í•œ ë°©ì‹)
            pred_original = pred_scaled * scaler.scale_[target_idx] + scaler.mean_[target_idx]
            pred_original = max(0, pred_original)
            
            predictions.append(pred_original)
            
            if (len(predictions)) % 100 == 0:
                print(f"   â³ {len(predictions)}/{validation_points} ì™„ë£Œ")
        
        # ì‹¤ì œê°’ê³¼ ë¹„êµ
        actual_values_subset = actual_values[validation_start_idx:validation_start_idx + len(predictions)]
        dates_subset = dates.iloc[validation_start_idx:validation_start_idx + len(predictions)]
        
        # ì˜¤ì°¨ ê³„ì‚°
        predictions_array = np.array(predictions)
        errors = predictions_array - actual_values_subset
        abs_errors = np.abs(errors)
        percentage_errors = np.abs(errors / (actual_values_subset + 1e-10)) * 100  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        # í†µê³„
        mae = np.mean(abs_errors)
        rmse = np.sqrt(np.mean(errors ** 2))
        mape = np.mean(percentage_errors)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ ê²€ì¦ ê²°ê³¼")
        print(f"{'='*80}")
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"   MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨):        {mae:.4f}")
        print(f"   RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {rmse:.4f}")
        print(f"   MAPE (í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨): {mape:.2f}%")
        
        print(f"\nğŸ“Š ì‹¤ì œê°’ ë²”ìœ„:")
        print(f"   ìµœì†Œ: {np.min(actual_values_subset):.4f}")
        print(f"   ìµœëŒ€: {np.max(actual_values_subset):.4f}")
        print(f"   í‰ê· : {np.mean(actual_values_subset):.4f}")
        
        print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ ë²”ìœ„:")
        print(f"   ìµœì†Œ: {np.min(predictions_array):.4f}")
        print(f"   ìµœëŒ€: {np.max(predictions_array):.4f}")
        print(f"   í‰ê· : {np.mean(predictions_array):.4f}")
        
        # ìƒì„¸ ë¹„êµí‘œ ì¶œë ¥ (ìƒ˜í”Œë§)
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ ìƒì„¸ ë¹„êµ (ë§¤ ì‹œê°„ë§ˆë‹¤ ìƒ˜í”Œë§)")
        print(f"{'='*80}")
        print(f"{'ë‚ ì§œ/ì‹œê°„':<22} {'ì‹¤ì œê°’':>12} {'ì˜ˆì¸¡ê°’':>12} {'ì˜¤ì°¨':>12} {'ì˜¤ì°¨ìœ¨(%)':>12}")
        print(f"{'-'*80}")
        
        # 1ì‹œê°„ë§ˆë‹¤ ìƒ˜í”Œë§ (4ê°œë§ˆë‹¤ = 15ë¶„ * 4 = 1ì‹œê°„)
        sample_indices = range(0, len(predictions), 4)
        
        for idx in sample_indices:
            if idx >= len(predictions):
                break
            
            date_str = dates_subset.iloc[idx].strftime('%Y-%m-%d %H:%M')
            actual = actual_values_subset[idx]
            pred = predictions_array[idx]
            error = errors[idx]
            error_pct = percentage_errors[idx]
            
            print(f"{date_str:<22} {actual:>12.4f} {pred:>12.4f} {error:>12.4f} {error_pct:>12.2f}")
        
        print(f"{'='*80}")
        
        # ì¼ë³„ í†µê³„
        print(f"\nğŸ“… ì¼ë³„ í†µê³„:")
        print(f"{'-'*80}")
        print(f"{'ë‚ ì§œ':<12} {'ì‹¤ì œ í‰ê· ':>12} {'ì˜ˆì¸¡ í‰ê· ':>12} {'MAE':>12} {'MAPE(%)':>12}")
        print(f"{'-'*80}")
        
        # ì¼ë³„ë¡œ ê·¸ë£¹í™”
        dates_only = dates_subset.dt.date
        unique_dates = dates_only.unique()
        
        for date in unique_dates:
            mask = dates_only == date
            daily_actual = actual_values_subset[mask]
            daily_pred = predictions_array[mask]
            daily_errors = np.abs(daily_pred - daily_actual)
            daily_pct_errors = np.abs((daily_pred - daily_actual) / (daily_actual + 1e-10)) * 100
            
            print(f"{str(date):<12} {np.mean(daily_actual):>12.4f} {np.mean(daily_pred):>12.4f} "
                  f"{np.mean(daily_errors):>12.4f} {np.mean(daily_pct_errors):>12.2f}")
        
        print(f"{'='*80}")
        
        # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        validation_result = {
            "validation_period": {
                "start_date": convert_to_serializable(dates_subset.iloc[0]),
                "end_date": convert_to_serializable(dates_subset.iloc[-1]),
                "days": validation_days,
                "points": len(predictions)
            },
            "statistics": {
                "mae": float(mae),
                "rmse": float(rmse),
                "mape": float(mape),
                "actual_min": float(np.min(actual_values_subset)),
                "actual_max": float(np.max(actual_values_subset)),
                "actual_mean": float(np.mean(actual_values_subset)),
                "predicted_min": float(np.min(predictions_array)),
                "predicted_max": float(np.max(predictions_array)),
                "predicted_mean": float(np.mean(predictions_array))
            },
            "comparison_data": [
                {
                    "date": convert_to_serializable(dates_subset.iloc[i]),
                    "actual": float(actual_values_subset[i]),
                    "predicted": float(predictions_array[i]),
                    "error": float(errors[i]),
                    "error_percentage": float(percentage_errors[i])
                }
                for i in range(len(predictions))
            ]
        }
        
        return validation_result
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def predict_future_simple(model, scaler, config, data, future_steps=672):
    try:
        print(f"\nğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ì‹œì‘")
        print(f"   - ì˜ˆì¸¡ ìŠ¤í…: {future_steps}ê°œ")
        
        dateColumn = config['dateColumn']
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        seq_len = int(config['r_seqLen'])
        
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        target_idx = study_columns_list.index(targetColumn)
        
        # ë§ˆì§€ë§‰ ë‚ ì§œ
        if dateColumn in data.columns:
            last_date = pd.to_datetime(data[dateColumn].iloc[-1])
        else:
            last_date = datetime.now()
        
        # ë°ì´í„° ì¤€ë¹„
        data_for_prediction = data[study_columns_list].astype(float)
        
        if len(data_for_prediction) < seq_len:
            raise ValueError(f"ë°ì´í„° ë¶€ì¡±: {len(data_for_prediction)}ê°œ (ìµœì†Œ {seq_len}ê°œ í•„ìš”)")
        
        # ì •ê·œí™”
        data_scaled = scaler.transform(data_for_prediction)
        
        # ì‹œê°„ ê°„ê²©
        if dateColumn in data.columns and len(data) > 1:
            dates = pd.to_datetime(data[dateColumn])
            time_delta = (dates.iloc[-1] - dates.iloc[-2])
        else:
            time_delta = pd.Timedelta(minutes=15)
        
        # í˜„ì¬ ì‹œí€€ìŠ¤
        current_sequence = data_scaled[-seq_len:].copy()
        
        # ê²°ê³¼ ì €ì¥
        future_predictions = []
        future_dates = []
        
        print(f"   ğŸ”„ ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
        
        # ì˜ˆì¸¡ ë£¨í”„
        for step in range(future_steps):
            next_date = last_date + time_delta * (step + 1)
            
            # ëª¨ë¸ ì…ë ¥
            input_data = current_sequence.reshape(1, seq_len, len(study_columns_list))
            pred_scaled = model.predict(input_data, verbose=0)[0, 0]
            
            # ì—­ìŠ¤ì¼€ì¼ë§
            pred_original = pred_scaled * scaler.scale_[target_idx] + scaler.mean_[target_idx]
            
            # ë””ë²„ê¹… (ì²˜ìŒ 5ê°œ)
            if step < 5:
                print(f"   [Step {step+1}] pred_scaled={pred_scaled:.6f}, pred_original={pred_original:.4f}")
            
            # ìŒìˆ˜ ë°©ì§€
            pred_original = max(0, pred_original)
            
            future_predictions.append(pred_original)
            future_dates.append(next_date)
            
            # ë‹¤ìŒ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
            new_point = current_sequence[-1].copy()
            new_point_scaled = (pred_original - scaler.mean_[target_idx]) / scaler.scale_[target_idx]
            new_point[target_idx] = new_point_scaled
            
            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
            current_sequence = np.vstack([current_sequence[1:], new_point])
            
            if (step + 1) % 100 == 0:
                print(f"   â³ {step + 1}/{future_steps} ì™„ë£Œ")
        
        print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        
        # ê²°ê³¼ í¬ë§·
        predictions_list = []
        for i, (pred_val, pred_date) in enumerate(zip(future_predictions, future_dates)):
            predictions_list.append({
                "date": convert_to_serializable(pred_date),
                "predicted_value": convert_to_serializable(pred_val),
                "is_reliable": True
            })
        
        # í†µê³„
        print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"   - ìµœì†Œ: {np.min(future_predictions):.2f}")
        print(f"   - ìµœëŒ€: {np.max(future_predictions):.2f}")
        print(f"   - í‰ê· : {np.mean(future_predictions):.2f}")
        print(f"   - í‘œì¤€í¸ì°¨: {np.std(future_predictions):.2f}")
        
        future_result = {
            "metadata": {
                "model_name": config.get('modelName', 'unknown'),
                "target_column": targetColumn,
                "sequence_length": seq_len,
                "prediction_steps": future_steps,
                "last_known_date": convert_to_serializable(last_date),
            },
            "predictions": predictions_list,
            "statistics": {
                "min_predicted": convert_to_serializable(np.min(future_predictions)),
                "max_predicted": convert_to_serializable(np.max(future_predictions)),
                "mean_predicted": convert_to_serializable(np.mean(future_predictions)),
                "std_predicted": convert_to_serializable(np.std(future_predictions))
            }
        }
        
        return future_result
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def save_predictions_to_db(prediction_result, target_table="usage_generation_forecast"):
    if prediction_result is None:
        print("âŒ ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
        return 0, 0
    
    try:
        engine = get_db_engine()
        predictions = prediction_result.get('predictions', [])
        
        if not predictions:
            print("âŒ ì˜ˆì¸¡ ë°ì´í„° ì—†ìŒ")
            return 0, 0
        
        print(f"\nğŸ’¾ DB ì €ì¥ ì‹œì‘...")
        print(f"   - í…Œì´ë¸”: carbontwin.{target_table}")
        print(f"   - ë°ì´í„°: {len(predictions)}ê±´")
        
        success_count = 0
        
        with engine.connect() as conn:
            trans = conn.begin()
            
            try:
                for pred in predictions:
                    time_point = pred['date']
                    forecast_value = pred['predicted_value']
                    
                    delete_query = text(f"""
                    DELETE FROM carbontwin.{target_table}
                    WHERE time_point = :time_point
                    """)
                    conn.execute(delete_query, {"time_point": time_point})
                    
                    insert_query = text(f"""
                    INSERT INTO carbontwin.{target_table} 
                        (time_point, forecast_usage_kwh, reg_dt)
                    VALUES 
                        (:time_point, :forecast_value, CURRENT_TIMESTAMP)
                    """)
                    
                    conn.execute(insert_query, {
                        "time_point": time_point,
                        "forecast_value": forecast_value
                    })
                    
                    success_count += 1
                    
                    if success_count % 100 == 0:
                        print(f"   â³ {success_count}/{len(predictions)} ê±´")
                
                trans.commit()
                print(f"âœ… DB ì €ì¥ ì™„ë£Œ: {success_count}ê±´")
                
            except Exception as e:
                trans.rollback()
                print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                return success_count, len(predictions) - success_count
        
        return success_count, 0
        
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        return 0, len(predictions) if predictions else 0

def main(model_name=None, tablename=None, future_steps=672, save_to_db_flag=True, validation_days=3):
    print("=" * 70)
    print("âš¡ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ê²€ì¦ ê¸°ëŠ¥ ì¶”ê°€)")
    print("=" * 70)
    
    model, scaler, config = load_trained_model(model_name)
    
    if model is None:
        return None
    
    print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
    new_data = load_new_data(tablename, config['dateColumn'], config['studyColumns'])
    
    if new_data is None or new_data.empty:
        print("âŒ ë°ì´í„° ì—†ìŒ")
        return None
    
    # âœ¨ ê²€ì¦ ìˆ˜í–‰ (ìµœê·¼ 3ì¼ ë°ì´í„°ë¡œ)
    validation_result = validate_with_actual_data(
        model, scaler, config, new_data, validation_days=validation_days
    )
    
    # ë¯¸ë˜ ì˜ˆì¸¡
    print(f"\nâš¡ ë¯¸ë˜ ì˜ˆì¸¡")
    print(f"   - ì˜ˆì¸¡: {future_steps}ê°œ ìŠ¤í… ({future_steps//96}ì¼)")
    
    future_result = predict_future_simple(
        model, scaler, config, new_data, future_steps
    )
    
    if future_result and save_to_db_flag:
        success, fail = save_predictions_to_db(future_result)
        
        if success > 0:
            print(f"\nâœ… {success}ê±´ ì €ì¥")
        if fail > 0:
            print(f"âš ï¸ {fail}ê±´ ì‹¤íŒ¨")
    
    print(f"\n{'='*70}")
    print("ğŸ‰ ì™„ë£Œ!")
    print("="*70)
    
    return {
        "validation": validation_result,
        "future_prediction": future_result
    }

if __name__ == "__main__":
    try:
        model_name = "solar-hybrid-seq-2-test-20251017-test-no-add-usage_kwh"
        tablename = "lstm_input_15m_new"
        
        print("\n" + "=" * 80)
        print("âš¡ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ë° ê²€ì¦")
        print("=" * 80)
        
        # ì˜ˆì¸¡ ë²”ìœ„ ì„¤ì •
        future_steps = 192  # 2ì¼
        validation_days = 7  # ìµœê·¼ 7ì¼ë¡œ ê²€ì¦
        
        print(f"\nâš™ï¸  ì„¤ì •:")
        print(f"   - ëª¨ë¸: {model_name}")
        print(f"   - ê²€ì¦: ìµœê·¼ {validation_days}ì¼ ë°ì´í„°ë¡œ ì •í™•ë„ í™•ì¸")
        print(f"   - ì˜ˆì¸¡: {future_steps}ê°œ ìŠ¤í… ({future_steps//96}ì¼)")

        result = main(
            model_name=model_name,
            tablename=tablename,
            future_steps=future_steps,
            save_to_db_flag=True,
            validation_days=validation_days
        )
        
        if result and result.get('validation'):
            val_stats = result['validation']['statistics']
            print(f"\n" + "="*80)
            print(f"ğŸ“Š ìµœì¢… ê²€ì¦ ìš”ì•½")
            print(f"="*80)
            print(f"   MAPE (í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨): {val_stats['mape']:.2f}%")
            print(f"   MAE  (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨):        {val_stats['mae']:.4f}")
            print(f"   RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {val_stats['rmse']:.4f}")
            
            if val_stats['mape'] < 10:
                print(f"   âœ… ëª¨ë¸ ì„±ëŠ¥: ìš°ìˆ˜ (MAPE < 10%)")
            elif val_stats['mape'] < 20:
                print(f"   âš ï¸  ëª¨ë¸ ì„±ëŠ¥: ë³´í†µ (10% â‰¤ MAPE < 20%)")
            else:
                print(f"   âŒ ëª¨ë¸ ì„±ëŠ¥: ê°œì„  í•„ìš” (MAPE â‰¥ 20%)")
            print(f"="*80)
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()