# -*- coding: utf-8 -*-
"""
Title   : ì™¸ë¶€ë°ì´í„° ê¸°ë°˜ LSTM ì˜ˆì¸¡ ë° ë©€í‹° ì‹¤í—˜ ìë™í™” ëª¨ë“ˆ
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
"""

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import psycopg2
from sklearn.preprocessing import StandardScaler
import json
import joblib
from sqlalchemy import create_engine
from datetime import datetime

# í™˜ê²½ ì„¤ì •
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"
else:
    root = "/app/webfiles/lstm"

# ê²½ë¡œ ì„¤ì •
graph_path = os.path.abspath(root + "/graphImage")
os.makedirs(graph_path, exist_ok=True)
model_path = os.path.abspath(root + "/saved_models")
os.makedirs(model_path, exist_ok=True)

# âœ… PostgreSQL ì—°ê²° í•¨ìˆ˜ (SQLAlchemy ì‚¬ìš©)
def get_db_engine():
    """SQLAlchemy ì—”ì§„ ìƒì„±"""
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    return create_engine(connection_string)

# âœ… JSON ì„¤ì • íŒŒì¼ ë¡œë“œ
def load_experiments_config(config_file="experiments.json"):
    """ì‹¤í—˜ ì„¤ì • JSON íŒŒì¼ ë¡œë“œ"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config['experiments']
    except FileNotFoundError:
        print(f"âŒ ì„¤ì • íŒŒì¼ '{config_file}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {config_file}")
        return []

# âœ… ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_data_from_db(tablename, dateColumn, studyColumns):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        engine = get_db_engine()
        
        query = f"""
        SELECT {studyColumns},{dateColumn}
        FROM carbontwin.{tablename}
        WHERE {dateColumn} IS NOT NULL
        ORDER BY {dateColumn} ASC
        """
        
        data = pd.read_sql_query(query, engine)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(data)}í–‰")
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        return None

# âœ… ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
def run_single_experiment(experiment_config, experiment_index):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ ì‹¤í—˜ {experiment_index + 1}/{len(experiment_config)} ì‹œì‘: {experiment_config['name']}")
    print(f"{'='*60}")
    
    # ì„¤ì • ì¶œë ¥
    print(f"ğŸ“‹ ì‹¤í—˜ ì„¤ì •:")
    print(f"   - í…Œì´ë¸”: {experiment_config['tablename']}")
    print(f"   - ëª¨ë¸ëª…: {experiment_config['modelName']}")
    print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼: {experiment_config['targetColumn']}")
    print(f"   - ì—í¬í¬: {experiment_config['r_epochs']}")
    print(f"   - ë°°ì¹˜í¬ê¸°: {experiment_config['r_batchSize']}")
    print(f"   - ì‹œí€€ìŠ¤ê¸¸ì´: {experiment_config['r_seqLen']}")
    
    # ë°ì´í„° ë¡œë“œ
    data = load_data_from_db(
        experiment_config['tablename'],
        experiment_config['dateColumn'], 
        experiment_config['studyColumns']
    )
    
    if data is None:
        return {"status": "error", "message": "ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"}
    
    # í•™ìŠµ ì‹¤í–‰
    start_time = time.time()
    result = lstmFinance(data, experiment_config)
    end_time = time.time()
    
    # ì‹¤í–‰ ì‹œê°„ ì¶”ê°€
    result['execution_time'] = round(end_time - start_time, 2)
    result['experiment_name'] = experiment_config['name']
    
    print(f"â±ï¸  ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {result['execution_time']}ì´ˆ")
    return result

# âœ… LSTM í•™ìŠµ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
def lstmFinance(lstmData, config):
    """LSTM ëª¨ë¸ í•™ìŠµ (ì„¤ì • ê°ì²´ ê¸°ë°˜)"""
    
    if not tf.executing_eagerly():
        tf.config.run_functions_eagerly(True)

    # ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    modelName = config['modelName']
    dateColumn = config['dateColumn']
    studyColumns = config['studyColumns']
    targetColumn = config['targetColumn']
    r_epochs = config['r_epochs']
    r_batchSize = config['r_batchSize']
    r_validationSplit = config['r_validationSplit']
    r_seqLen = config['r_seqLen']
    r_predDays = config['r_predDays']

    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    training_loss_path = os.path.join(graph_path, f"{modelName}_trainingLoss.png")
    total_graph_path = os.path.join(graph_path, f"{modelName}_totalgraph.png")
    diff_graph_path = os.path.join(graph_path, f"{modelName}_diffgraph.png")
    model_file_path = os.path.join(model_path, f"{modelName}.h5")

    stock_data = lstmData
    
    # ë°ì´í„° ê²€ì¦
    if stock_data.empty:
        return {"status": "error", "message": "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
    
    study_columns_list = [col.strip() for col in studyColumns.split(',')]
    if targetColumn not in study_columns_list:
        return {"status": "error", "message": f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{targetColumn}'ì´ í•™ìŠµ ì»¬ëŸ¼ì— ì—†ìŠµë‹ˆë‹¤."}

    # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
    if dateColumn in stock_data.columns:
        dates = pd.to_datetime(stock_data[dateColumn], errors='coerce')
    else:
        dates = pd.date_range(start='2023-01-01', periods=len(stock_data), freq='5T')
        print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ '{dateColumn}'ì´ ì—†ì–´ì„œ ê°€ìƒ ë‚ ì§œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    
    original_open = stock_data[targetColumn].values
    stock_data_for_training = stock_data[study_columns_list].astype(float)

    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    stock_data_scaled = scaler.fit_transform(stock_data_for_training)

    n_train = int(0.9 * stock_data_scaled.shape[0])
    train_data_scaled = stock_data_scaled[:n_train]
    test_data_scaled = stock_data_scaled[n_train:]
    test_dates = dates[n_train:]

    pred_days = int(r_predDays)
    seq_len = int(r_seqLen)
    input_dim = stock_data_for_training.shape[1]
    target_idx = study_columns_list.index(targetColumn)

    # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    trainX, trainY, testX, testY = [], [], [], []
    for i in range(seq_len, n_train - pred_days + 1):
        trainX.append(train_data_scaled[i - seq_len:i, 0:input_dim])
        trainY.append(train_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    for i in range(seq_len, len(test_data_scaled) - pred_days + 1):
        testX.append(test_data_scaled[i - seq_len:i, 0:input_dim])
        testY.append(test_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    trainX, trainY = np.array(trainX), np.array(trainY)
    testX, testY = np.array(testX), np.array(testY)

    print(f"ğŸ”„ {modelName} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {trainX.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {testX.shape}")

    # ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    try:
        model = load_model(model_file_path, compile=False)
        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
        print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œë¨")
    except (OSError, IOError):
        print("ğŸ”„ ìƒˆ ëª¨ë¸ ìƒì„± ì¤‘...")

        model = Sequential([
            Input(shape=(trainX.shape[1], trainX.shape[2])),
            LSTM(64, return_sequences=True),
            LSTM(32, return_sequences=False),
            Dense(trainY.shape[1])
        ])

        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')

        class TrainingCallback(Callback):
            def __init__(self, total_epochs, batch_size):
                super().__init__()
                self.total_epochs = total_epochs
                self.batch_size = batch_size
                self.prev_val_loss = None
                
            def on_train_begin(self, logs=None):
                print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - ì´ {self.total_epochs} ì—í¬í¬")
                print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
                
            def on_epoch_begin(self, epoch, logs=None):
                print(f"\nâ³ Epoch {epoch + 1}/{self.total_epochs} ì‹œì‘...")
                
            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                loss = logs.get('loss', 0)
                val_loss = logs.get('val_loss', 0)
                
                # ì§„í–‰ë¥  ê³„ì‚°
                progress = (epoch + 1) / self.total_epochs * 100
                
                # ì§„í–‰ë°” ìƒì„±
                bar_length = 30
                filled_length = int(bar_length * (epoch + 1) // self.total_epochs)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                
                print(f"âœ… Epoch {epoch + 1}/{self.total_epochs} [{bar}] {progress:.1f}%")
                print(f"   ğŸ“‰ Loss: {loss:.6f} | Val_Loss: {val_loss:.6f}")
                
                # ê°œì„  ì—¬ë¶€ í™•ì¸
                if epoch > 0 and self.prev_val_loss is not None:
                    if val_loss < self.prev_val_loss:
                        print(f"   ğŸ“ˆ ê²€ì¦ ì†ì‹¤ ê°œì„ : {self.prev_val_loss:.6f} â†’ {val_loss:.6f}")
                    elif val_loss > self.prev_val_loss * 1.1:  # 10% ì´ìƒ ì¦ê°€ì‹œ ê²½ê³ 
                        print(f"   âš ï¸  ê²€ì¦ ì†ì‹¤ ì¦ê°€: {self.prev_val_loss:.6f} â†’ {val_loss:.6f}")
                
                self.prev_val_loss = val_loss
                
            def on_train_end(self, logs=None):
                print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")

        history = model.fit(
            trainX, trainY,
            epochs=int(r_epochs),
            batch_size=int(r_batchSize),
            validation_split=float(r_validationSplit),
            verbose=1,  # ê¸°ë³¸ ì§„í–‰ìƒí™© í‘œì‹œ
            callbacks=[TrainingCallback(int(r_epochs), int(r_batchSize))]
        )

        model.save(model_file_path)
        print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

        # í•™ìŠµ ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥
        plt.figure(figsize=(12, 4))
        plt.plot(history.history['loss'], label='Training loss')
        plt.plot(history.history['val_loss'], label='Validation loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(f'{modelName} - Training Loss')
        plt.legend()
        plt.savefig(training_loss_path)
        plt.close()

    # ì˜ˆì¸¡ ìˆ˜í–‰
    print(f"\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    print(f"ğŸ“Š ì˜ˆì¸¡í•  ìƒ˜í”Œ ìˆ˜: {len(testX)}")
    
    # ë°°ì¹˜ë³„ë¡œ ì˜ˆì¸¡í•˜ì—¬ ì§„í–‰ìƒí™© í‘œì‹œ
    batch_size_pred = 32  # ì˜ˆì¸¡ìš© ë°°ì¹˜ í¬ê¸°
    predictions = []
    
    total_batches = (len(testX) + batch_size_pred - 1) // batch_size_pred
    
    for i in range(0, len(testX), batch_size_pred):
        batch_end = min(i + batch_size_pred, len(testX))
        batch_data = testX[i:batch_end]
        
        batch_pred = model.predict(batch_data, verbose=0)
        predictions.append(batch_pred)
        
        # ì§„í–‰ìƒí™© í‘œì‹œ
        current_batch = (i // batch_size_pred) + 1
        progress = current_batch / total_batches * 100
        
        # ì§„í–‰ë°” ìƒì„±
        bar_length = 25
        filled_length = int(bar_length * current_batch // total_batches)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        print(f"\râ³ ì˜ˆì¸¡ ì§„í–‰: [{bar}] {progress:.1f}% ({current_batch}/{total_batches} ë°°ì¹˜)", end='', flush=True)
    
    prediction = np.vstack(predictions)
    print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ! ì´ {len(prediction)}ê°œ ìƒ˜í”Œ ì˜ˆì¸¡ë¨")

    # ì˜ˆì¸¡ ê²°ê³¼ ì—­ë³€í™˜
    mean_values_pred = np.repeat(scaler.mean_[np.newaxis, :], prediction.shape[0], axis=0)
    mean_values_pred[:, target_idx] = np.squeeze(prediction)
    y_pred = scaler.inverse_transform(mean_values_pred)[:, target_idx]

    mean_values_testY = np.repeat(scaler.mean_[np.newaxis, :], testY.shape[0], axis=0)
    mean_values_testY[:, target_idx] = np.squeeze(testY)
    testY_original = scaler.inverse_transform(mean_values_testY)[:, target_idx]
    valid_test_dates = test_dates[seq_len : seq_len + len(testY_original)]

    # ì „ì²´ ê·¸ë˜í”„ ì €ì¥
    plt.figure(figsize=(15, 5))
    plt.plot(dates, original_open, color='green', label=f'Original {targetColumn}', alpha=0.7)
    plt.plot(valid_test_dates, testY_original, color='blue', label=f'Actual {targetColumn}')
    plt.plot(valid_test_dates, y_pred, color='red', linestyle='--', label=f'Predicted {targetColumn}')
    plt.xlabel(dateColumn)
    plt.ylabel(f'{targetColumn} Value')
    plt.title(f'{modelName} - Prediction Results')
    plt.legend()
    plt.savefig(total_graph_path)
    plt.close()

    # í™•ëŒ€ ê·¸ë˜í”„ ì €ì¥
    zoom_start = max(0, len(valid_test_dates) - 50)
    plt.figure(figsize=(15, 5))
    plt.plot(valid_test_dates[zoom_start:], testY_original[zoom_start:], color='blue', label=f'Actual {targetColumn}')
    plt.plot(valid_test_dates[zoom_start:], y_pred[zoom_start:], color='red', linestyle='--', label=f'Predicted {targetColumn}')
    plt.xlabel(dateColumn)
    plt.ylabel(f'{targetColumn} Value')
    plt.title(f'{modelName} - Recent Predictions (Last 50 points)')
    plt.legend()
    plt.savefig(diff_graph_path)
    plt.close()

    # ì •í™•ë„ ê³„ì‚°
    print(f"\nğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    
    def mean_absolute_percentage_error(y_true, y_pred):
        mask = y_true != 0
        if np.sum(mask) == 0:
            return 999.0
        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

    # ì¶”ê°€ í‰ê°€ ì§€í‘œë“¤ ê³„ì‚°
    try:
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        sklearn_available = True
    except ImportError:
        print("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì§€í‘œë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.")
        sklearn_available = False
    
    mape = mean_absolute_percentage_error(testY_original, y_pred)
    accuracy = 100 - mape if not np.isnan(mape) else 0
    
    # ì¶”ê°€ ì§€í‘œë“¤
    if sklearn_available:
        mse = mean_squared_error(testY_original, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(testY_original, y_pred)
        r2 = r2_score(testY_original, y_pred)
    else:
        # ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°
        mse = np.mean((testY_original - y_pred) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(testY_original - y_pred))
        
        # RÂ² ìˆ˜ë™ ê³„ì‚°
        ss_res = np.sum((testY_original - y_pred) ** 2)
        ss_tot = np.sum((testY_original - np.mean(testY_original)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
    
    # ë°©í–¥ì„± ì •í™•ë„ (ìƒìŠ¹/í•˜ë½ ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„)
    if len(testY_original) > 1:
        actual_direction = np.diff(testY_original) > 0
        pred_direction = np.diff(y_pred) > 0
        direction_accuracy = np.mean(actual_direction == pred_direction) * 100
    else:
        direction_accuracy = 0
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼:")
    print(f"   ğŸ¯ MAPE: {mape:.2f}%")
    print(f"   ğŸ“ˆ ì •í™•ë„: {accuracy:.2f}%")
    print(f"   ğŸ“ MAE: {mae:.4f}")
    print(f"   ğŸ“ RMSE: {rmse:.4f}")
    print(f"   ğŸ” RÂ² Score: {r2:.4f}")
    print(f"   ğŸ§­ ë°©í–¥ì„± ì •í™•ë„: {direction_accuracy:.2f}%")
    
    # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
    if accuracy >= 90:
        grade = "ğŸ† ìš°ìˆ˜"
    elif accuracy >= 80:
        grade = "ğŸ¥‡ ì–‘í˜¸"
    elif accuracy >= 70:
        grade = "ğŸ¥ˆ ë³´í†µ"
    elif accuracy >= 60:
        grade = "ğŸ¥‰ ê°œì„ í•„ìš”"
    else:
        grade = "âŒ ë¶ˆëŸ‰"
    
    print(f"   ğŸ“Š ì„±ëŠ¥ ë“±ê¸‰: {grade}")
    
    # ì˜ˆì¸¡ ë²”ìœ„ ë¶„ì„
    pred_min, pred_max = np.min(y_pred), np.max(y_pred)
    actual_min, actual_max = np.min(testY_original), np.max(testY_original)
    print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ ë²”ìœ„ ë¶„ì„:")
    print(f"   ì‹¤ì œê°’ ë²”ìœ„: {actual_min:.3f} ~ {actual_max:.3f}")
    print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {pred_min:.3f} ~ {pred_max:.3f}")
    
    # ê³¼/ì†Œì˜ˆì¸¡ ë¶„ì„
    over_predict = np.sum(y_pred > testY_original) / len(y_pred) * 100
    under_predict = 100 - over_predict
    print(f"   ê³¼ì˜ˆì¸¡ ë¹„ìœ¨: {over_predict:.1f}%")
    print(f"   ì†Œì˜ˆì¸¡ ë¹„ìœ¨: {under_predict:.1f}%")

    # ì„¤ì • ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    with open(os.path.join(model_path, f"{modelName}_config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4, ensure_ascii=False)

    joblib.dump(scaler, os.path.join(model_path, f"{modelName}_scaler.pkl"))

    return {
        "status": "success",
        "modelName": modelName,
        "training_loss_img": f"graphImage/{modelName}_trainingLoss.png",
        "total_graph_img": f"graphImage/{modelName}_totalgraph.png",
        "diff_graph_img": f"graphImage/{modelName}_diffgraph.png",
        "mape": round(mape, 2),
        "accuracy": round(accuracy, 2)
    }

# âœ… ë©€í‹° ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
def run_multiple_experiments(config_file="experiments.json"):
    """ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰"""
    experiments = load_experiments_config(config_file)
    
    if not experiments:
        print("âŒ ì‹¤í–‰í•  ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ”¬ ì´ {len(experiments)}ê°œì˜ ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = []
    total_start_time = time.time()
    
    for i, experiment in enumerate(experiments):
        try:
            result = run_single_experiment(experiment, i)
            results.append(result)
            
            if result['status'] == 'success':
                print(f"âœ… {experiment['name']} ì™„ë£Œ - ì •í™•ë„: {result['accuracy']:.2f}%")
            else:
                print(f"âŒ {experiment['name']} ì‹¤íŒ¨: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
        except Exception as e:
            print(f"âŒ {experiment['name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            results.append({"status": "error", "message": str(e), "experiment_name": experiment['name']})
    
    total_end_time = time.time()
    total_time = round(total_end_time - total_start_time, 2)
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_time}ì´ˆ")
    print(f"âœ… ì„±ê³µ: {len([r for r in results if r['status'] == 'success'])}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {len([r for r in results if r['status'] == 'error'])}ê°œ")
    
    # ì„±ê³µí•œ ì‹¤í—˜ë“¤ì˜ ì •í™•ë„ ìˆœìœ„
    successful_results = [r for r in results if r['status'] == 'success']
    if successful_results:
        successful_results.sort(key=lambda x: x['accuracy'], reverse=True)
        print(f"\nğŸ† ì •í™•ë„ ìˆœìœ„:")
        for i, result in enumerate(successful_results, 1):
            print(f"{i}. {result['experiment_name']}: {result['accuracy']:.2f}% (MAPE: {result['mape']:.2f}%)")
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    with open("experiment_results.json", "w", encoding="utf-8") as f:
        json.dump({
            "summary": {
                "total_experiments": len(experiments),
                "successful": len(successful_results),
                "failed": len(results) - len(successful_results),
                "total_time": total_time,
                "timestamp": datetime.now().isoformat()
            },
            "results": results
        }, f, indent=4, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ê²°ê³¼ê°€ 'experiment_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return results

# âœ… ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    print("ğŸ§ª LSTM ë©€í‹° ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    choice = input("ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\n1. ë©€í‹° ì‹¤í—˜ (JSON íŒŒì¼ ê¸°ë°˜)\n2. ë‹¨ì¼ ì‹¤í—˜ (ìˆ˜ë™ ì…ë ¥)\nì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    if choice == "1":
        config_file = input("ì„¤ì • íŒŒì¼ëª… (ê¸°ë³¸ê°’: experiments.json): ").strip() or "experiments.json"
        run_multiple_experiments(config_file)
    elif choice == "2":
        # ê¸°ì¡´ ë‹¨ì¼ ì‹¤í—˜ ëª¨ë“œ
        print("ë‹¨ì¼ ì‹¤í—˜ ëª¨ë“œëŠ” ê¸°ì¡´ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")