# -*- coding: utf-8 -*-
"""
Title   : LSTM ëª¨ë¸ ì˜ˆì¸¡ ì „ìš© ëª¨ë“ˆ (DB ì—°ë™ ë²„ì „)
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
Purpose : í•™ìŠµëœ LSTM ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ (DBì—ì„œ ëª¨ë¸ ì„ íƒ)
"""

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import json
import joblib
from tensorflow.keras.models import load_model
from sqlalchemy import create_engine
from datetime import datetime, timedelta

# í™˜ê²½ ì„¤ì •
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"
else:
    root = "/app/webfiles/lstm"

model_path = os.path.abspath(root + "/saved_models")
prediction_path = os.path.abspath(root + "/predictions")
os.makedirs(prediction_path, exist_ok=True)


def get_db_engine():
    """SQLAlchemy ì—”ì§„ ìƒì„±"""
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    return create_engine(connection_string)


# ============================================================================
# DB ì¡°íšŒ í•¨ìˆ˜ë“¤
# ============================================================================

def get_available_models_from_db(show_stats=True):
    """
    DBì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    
    Args:
        show_stats: ëª¨ë¸ë³„ ìµœê³  ì„±ëŠ¥ í†µê³„ í¬í•¨ ì—¬ë¶€
    
    Returns:
        DataFrame: ëª¨ë¸ ëª©ë¡
    """
    try:
        engine = get_db_engine()
        
        if show_stats:
            query = """
            SELECT 
                m.model_id,
                m.model_name,
                m.target_column,
                m.epochs,
                m.sequence_length,
                m.prediction_days,
                m.created_at as model_created_at,
                COUNT(e.experiment_id) as total_experiments,
                MAX(e.accuracy) as best_accuracy,
                MIN(e.mape) as best_mape,
                MAX(e.r2_score) as best_r2_score,
                MAX(e.created_at) as last_experiment_date
            FROM carbontwin.lstm_model m
            LEFT JOIN carbontwin.lstm_experiment e ON m.model_id = e.model_id
            WHERE e.status = 'success' OR e.status IS NULL
            GROUP BY m.model_id, m.model_name, m.target_column, m.epochs, 
                     m.sequence_length, m.prediction_days, m.created_at
            ORDER BY best_accuracy DESC NULLS LAST, m.created_at DESC
            """
        else:
            query = """
            SELECT 
                model_id,
                model_name,
                target_column,
                date_column,
                study_columns,
                epochs,
                batch_size,
                validation_split,
                sequence_length,
                prediction_days,
                created_at
            FROM carbontwin.lstm_model
            ORDER BY created_at DESC
            """
        
        models = pd.read_sql_query(query, engine)
        return models
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return None


def get_model_by_id(model_id):
    """model_idë¡œ ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    try:
        engine = get_db_engine()
        
        query = f"""
        SELECT *
        FROM carbontwin.lstm_model
        WHERE model_id = {model_id}
        """
        
        result = pd.read_sql_query(query, engine)
        
        if result.empty:
            print(f"âŒ model_id {model_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        return result.iloc[0].to_dict()
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return None


def get_model_by_name(model_name):
    """model_nameìœ¼ë¡œ ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    try:
        engine = get_db_engine()
        
        query = f"""
        SELECT *
        FROM carbontwin.lstm_model
        WHERE model_name = '{model_name}'
        """
        
        result = pd.read_sql_query(query, engine)
        
        if result.empty:
            print(f"âŒ ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        return result.iloc[0].to_dict()
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return None


def get_best_experiment_for_model(model_id, metric='accuracy'):
    """
    íŠ¹ì • ëª¨ë¸ì˜ ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì¡°íšŒ
    
    Args:
        model_id: ëª¨ë¸ ID
        metric: ì •ë ¬ ê¸°ì¤€ ('accuracy', 'mape', 'r2_score', 'rmse')
    
    Returns:
        dict: ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì •ë³´
    """
    try:
        engine = get_db_engine()
        
        order = 'DESC' if metric in ['accuracy', 'r2_score'] else 'ASC'
        
        query = f"""
        SELECT 
            e.*,
            m.model_name,
            m.target_column,
            m.study_columns,
            m.date_column
        FROM carbontwin.lstm_experiment e
        JOIN carbontwin.lstm_model m ON e.model_id = m.model_id
        WHERE e.model_id = {model_id} AND e.status = 'success'
        ORDER BY e.{metric} {order}
        LIMIT 1
        """
        
        result = pd.read_sql_query(query, engine)
        
        if result.empty:
            print(f"âš ï¸ model_id {model_id}ì— ëŒ€í•œ ì„±ê³µí•œ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        return result.iloc[0].to_dict()
        
    except Exception as e:
        print(f"âŒ ì‹¤í—˜ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return None


def get_experiments_by_model(model_id, limit=10):
    """íŠ¹ì • ëª¨ë¸ì˜ ì‹¤í—˜ ì´ë ¥ ì¡°íšŒ"""
    try:
        engine = get_db_engine()
        
        query = f"""
        SELECT 
            experiment_id,
            experiment_name,
            accuracy,
            mape,
            rmse,
            r2_score,
            execution_time_seconds,
            status,
            created_at
        FROM carbontwin.lstm_experiment
        WHERE model_id = {model_id}
        ORDER BY created_at DESC
        LIMIT {limit}
        """
        
        return pd.read_sql_query(query, engine)
        
    except Exception as e:
        print(f"âŒ ì‹¤í—˜ ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return None


# ============================================================================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (DB ì—°ë™)
# ============================================================================

def load_trained_model_from_db(model_id=None, model_name=None, use_best_experiment=True):
    """
    DBì—ì„œ ëª¨ë¸ ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ ë¡œë“œ
    
    Args:
        model_id: ëª¨ë¸ ID (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        model_name: ëª¨ë¸ ì´ë¦„
        use_best_experiment: ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ì˜ íŒŒì¼ ê²½ë¡œ ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        dict: {
            'model': Keras ëª¨ë¸,
            'scaler': StandardScaler,
            'config': ì„¤ì • ì •ë³´,
            'model_info': DB ëª¨ë¸ ì •ë³´,
            'experiment_info': ì‹¤í—˜ ì •ë³´ (ìˆì„ ê²½ìš°)
        }
    """
    try:
        # ëª¨ë¸ ì •ë³´ ì¡°íšŒ
        if model_id:
            model_info = get_model_by_id(model_id)
        elif model_name:
            model_info = get_model_by_name(model_name)
        else:
            raise ValueError("âŒ model_id ë˜ëŠ” model_nameì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        if not model_info:
            return None
        
        print(f"\nğŸ“‚ ëª¨ë¸ ì •ë³´:")
        print(f"   - ID: {model_info['model_id']}")
        print(f"   - ì´ë¦„: {model_info['model_name']}")
        print(f"   - íƒ€ê²Ÿ: {model_info['target_column']}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {model_info['sequence_length']}")
        print(f"   - Epochs: {model_info['epochs']}")
        
        # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì¡°íšŒ
        experiment_info = None
        if use_best_experiment:
            experiment_info = get_best_experiment_for_model(
                model_info['model_id'], 
                metric='accuracy'
            )
            
            if experiment_info:
                print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ì‹¤í—˜:")
                print(f"   - ì‹¤í—˜ëª…: {experiment_info['experiment_name']}")
                print(f"   - ì •í™•ë„: {experiment_info['accuracy']:.2f}%")
                print(f"   - MAPE: {experiment_info['mape']:.2f}%")
                print(f"   - RÂ² Score: {experiment_info['r2_score']:.4f}")
        
        # íŒŒì¼ ê²½ë¡œ ê²°ì •
        model_name_for_file = model_info['model_name']
        
        # ì‹¤í—˜ì—ì„œ model_file_pathê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if experiment_info and experiment_info.get('model_file_path'):
            model_file = experiment_info['model_file_path']
        else:
            model_file = os.path.join(model_path, f"{model_name_for_file}.h5")
        
        scaler_file = os.path.join(model_path, f"{model_name_for_file}_scaler.pkl")
        config_file = os.path.join(model_path, f"{model_name_for_file}_config.json")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(model_file):
            raise FileNotFoundError(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_file}")
        if not os.path.exists(scaler_file):
            raise FileNotFoundError(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scaler_file}")
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"\nğŸ“¥ íŒŒì¼ ë¡œë”© ì¤‘...")
        model = load_model(model_file, compile=False)
        scaler = joblib.load(scaler_file)
        print(f"âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ (ì—†ìœ¼ë©´ DBì—ì„œ ìƒì„±)
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
        else:
            # DB ì •ë³´ë¡œ config ìƒì„±
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ì´ ì—†ì–´ DB ì •ë³´ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            config = {
                'modelName': model_info['model_name'],
                'dateColumn': model_info['date_column'],
                'studyColumns': model_info['study_columns'],
                'targetColumn': model_info['target_column'],
                'r_epochs': int(model_info['epochs']),
                'r_batchSize': int(model_info['batch_size']),
                'r_validationSplit': float(model_info['validation_split']),
                'r_seqLen': int(model_info['sequence_length']),
                'r_predDays': int(model_info['prediction_days']),
                'tablename': 'lstm_input_1m'  # ê¸°ë³¸ê°’
            }
            
            # config_jsonì´ ìˆìœ¼ë©´ ì‚¬ìš©
            if experiment_info and experiment_info.get('config_json'):
                try:
                    config.update(json.loads(experiment_info['config_json']))
                except:
                    pass
        
        print(f"\nğŸ“‹ ì„¤ì • ì •ë³´:")
        print(f"   - ì…ë ¥ ì»¬ëŸ¼: {config['studyColumns']}")
        print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼: {config['targetColumn']}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {config['r_seqLen']}")
        print(f"   - ì˜ˆì¸¡ ì¼ìˆ˜: {config['r_predDays']}")
        
        return {
            'model': model,
            'scaler': scaler,
            'config': config,
            'model_info': model_info,
            'experiment_info': experiment_info
        }
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


# ============================================================================
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ============================================================================

def load_new_data_from_db(tablename, dateColumn, studyColumns, start_date=None, end_date=None, limit=None, daytime_only=False):
    """
    ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¡œë“œ
    
    Args:
        tablename: í…Œì´ë¸”ëª…
        dateColumn: ë‚ ì§œ ì»¬ëŸ¼ëª…
        studyColumns: ì‚¬ìš©í•  ì»¬ëŸ¼ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´)
        start_date: ì‹œì‘ ë‚ ì§œ (ì„ íƒ, 'YYYY-MM-DD HH:MM:SS' í˜•ì‹)
        end_date: ì¢…ë£Œ ë‚ ì§œ (ì„ íƒ)
        limit: ìµœëŒ€ ë¡œë“œí•  í–‰ ìˆ˜ (ì„ íƒ)
        daytime_only: Trueë©´ has_sunlight=1ì¸ ì£¼ê°„ ë°ì´í„°ë§Œ ë¡œë“œ
    """
    try:
        engine = get_db_engine()
        
        # ê¸°ë³¸ ì¿¼ë¦¬
        query = f"""
        SELECT {studyColumns}, {dateColumn}
        FROM carbontwin.{tablename}
        WHERE {dateColumn} IS NOT NULL
        """
        
        # ì£¼ê°„ ë°ì´í„°ë§Œ í•„í„°ë§ (has_sunlight ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
        if daytime_only and 'has_sunlight' in studyColumns:
            query += " AND has_sunlight = 1"
        
        # ë‚ ì§œ í•„í„° ì¶”ê°€
        if start_date:
            query += f" AND {dateColumn} >= '{start_date}'"
        if end_date:
            query += f" AND {dateColumn} <= '{end_date}'"
        
        query += f" ORDER BY {dateColumn} ASC"
        
        # ì œí•œ ì¶”ê°€
        if limit:
            query += f" LIMIT {limit}"
        
        data = pd.read_sql_query(query, engine)
        
        if daytime_only and 'has_sunlight' in studyColumns:
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}í–‰ (ì£¼ê°„ ë°ì´í„°ë§Œ)")
        else:
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}í–‰")
        
        if len(data) == 0:
            print("âš ï¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
            
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None


# ============================================================================
# ì˜ˆì¸¡ í•¨ìˆ˜
# ============================================================================

def predict_with_model(model_info, new_data):
    """
    ë¡œë“œëœ ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡
    
    Args:
        model_info: load_trained_model_from_db()ì˜ ë°˜í™˜ê°’
        new_data: ì˜ˆì¸¡í•  ë°ì´í„° (DataFrame)
    
    Returns:
        dict: ì˜ˆì¸¡ ê²°ê³¼
    """
    try:
        model = model_info['model']
        scaler = model_info['scaler']
        config = model_info['config']
        
        # ì„¤ì • ì¶”ì¶œ
        dateColumn = config['dateColumn']
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        seq_len = int(config['r_seqLen'])
        pred_days = int(config['r_predDays'])
        
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        target_idx = study_columns_list.index(targetColumn)
        
        print(f"\nğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
        print(f"   - ì˜ˆì¸¡ ì¼ìˆ˜: {pred_days}")
        print(f"   - ì…ë ¥ ë°ì´í„° ìˆ˜: {len(new_data)}")
        
        # ë‚ ì§œ ì²˜ë¦¬
        if dateColumn in new_data.columns:
            dates = pd.to_datetime(new_data[dateColumn], errors='coerce')
        else:
            dates = pd.date_range(start='2025-01-01', periods=len(new_data), freq='1min')
            print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ì–´ ê°€ìƒ ë‚ ì§œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        data_for_prediction = new_data[study_columns_list].astype(float)
        
        # ìŠ¤ì¼€ì¼ë§ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©!)
        data_scaled = scaler.transform(data_for_prediction)
        
        # ì‹œí€€ìŠ¤ ìƒì„± í™•ì¸
        if len(data_scaled) < seq_len + pred_days - 1:
            raise ValueError(
                f"âŒ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ {seq_len + pred_days - 1}ê°œ í•„ìš”, í˜„ì¬ {len(data_scaled)}ê°œ"
            )
        
        # ì˜ˆì¸¡ìš© ì‹œí€€ìŠ¤ ìƒì„±
        predictX = []
        valid_indices = []
        
        for i in range(seq_len, len(data_scaled) - pred_days + 1):
            predictX.append(data_scaled[i - seq_len:i, :])
            valid_indices.append(i + pred_days - 1)
        
        predictX = np.array(predictX)
        print(f"   - ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤: {len(predictX)}ê°œ")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        print(f"\nâ³ ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
        predictions_scaled = model.predict(predictX, verbose=0)
        print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        
        # ì—­ë³€í™˜ (ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›)
        mean_values = np.repeat(scaler.mean_[np.newaxis, :], predictions_scaled.shape[0], axis=0)
        mean_values[:, target_idx] = np.squeeze(predictions_scaled)
        predictions_original = scaler.inverse_transform(mean_values)[:, target_idx]
        
        # âœ… íƒœì–‘ê´‘ ë°œì „ëŸ‰ì€ ìŒìˆ˜ê°€ ë‚˜ì˜¬ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ 0 ì´ìƒìœ¼ë¡œ í´ë¦¬í•‘
        predictions_original = np.maximum(predictions_original, 0)
        print(f"   - ìŒìˆ˜ ì˜ˆì¸¡ê°’ì„ 0ìœ¼ë¡œ ë³´ì • ì™„ë£Œ")
        
        # ì˜ˆì¸¡ ë‚ ì§œ
        prediction_dates = dates.iloc[valid_indices].reset_index(drop=True)
        
        # ì‹¤ì œê°’ì´ ìˆë‹¤ë©´ ë¹„êµ
        actual_values = data_for_prediction[targetColumn].iloc[valid_indices].values
        has_actual = not np.all(pd.isna(actual_values))
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for i in range(len(predictions_original)):
            result_record = {
                "index": i,
                "date": prediction_dates.iloc[i].isoformat() if hasattr(prediction_dates.iloc[i], 'isoformat') else str(prediction_dates.iloc[i]),
                "predicted_value": float(predictions_original[i])
            }
            
            if has_actual and not pd.isna(actual_values[i]):
                result_record["actual_value"] = float(actual_values[i])
                result_record["difference"] = float(predictions_original[i] - actual_values[i])
                result_record["percentage_error"] = float(
                    abs((predictions_original[i] - actual_values[i]) / actual_values[i] * 100) 
                    if actual_values[i] != 0 else 0
                )
            else:
                result_record["actual_value"] = None
                result_record["difference"] = None
                result_record["percentage_error"] = None
            
            results.append(result_record)
        
        # í†µê³„ ê³„ì‚°
        statistics = {
            "predicted_min": float(np.min(predictions_original)),
            "predicted_max": float(np.max(predictions_original)),
            "predicted_mean": float(np.mean(predictions_original)),
            "predicted_std": float(np.std(predictions_original))
        }
        
        if has_actual:
            valid_actuals = actual_values[~pd.isna(actual_values)]
            valid_preds = predictions_original[:len(valid_actuals)]
            
            # âœ… MAPE ê³„ì‚° ì‹œ ì‹¤ì œê°’ì´ 0ì´ ì•„ë‹Œ ê²ƒë§Œ ì‚¬ìš© (division by zero ë°©ì§€)
            non_zero_mask = valid_actuals != 0
            
            if np.sum(non_zero_mask) > 0:
                mape_value = float(np.mean(np.abs((valid_actuals[non_zero_mask] - valid_preds[non_zero_mask]) / valid_actuals[non_zero_mask])) * 100)
            else:
                mape_value = None
            
            statistics.update({
                "actual_min": float(np.min(valid_actuals)),
                "actual_max": float(np.max(valid_actuals)),
                "actual_mean": float(np.mean(valid_actuals)),
                "mae": float(np.mean(np.abs(valid_preds - valid_actuals))),
                "rmse": float(np.sqrt(np.mean((valid_preds - valid_actuals) ** 2))),
                "mape": mape_value,
                "non_zero_count": int(np.sum(non_zero_mask)),
                "total_count": len(valid_actuals),
                "zero_count": int(len(valid_actuals) - np.sum(non_zero_mask))
            })
        
        print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(f"   - ì˜ˆì¸¡ê°’ ë²”ìœ„: {statistics['predicted_min']:.3f} ~ {statistics['predicted_max']:.3f}")
        print(f"   - ì˜ˆì¸¡ê°’ í‰ê· : {statistics['predicted_mean']:.3f}")
        
        if has_actual:
            print(f"   - MAE: {statistics['mae']:.4f}")
            print(f"   - RMSE: {statistics['rmse']:.4f}")
            
            if statistics.get('mape') is not None:
                print(f"   - MAPE: {statistics['mape']:.2f}% (ì‹¤ì œê°’ 0 ì œì™¸)")
                print(f"   - ì •í™•ë„: {100 - statistics['mape']:.2f}%")
            else:
                print(f"   - MAPE: ê³„ì‚° ë¶ˆê°€ (ëª¨ë“  ì‹¤ì œê°’ì´ 0)")
            
            print(f"   - í‰ê°€ ë°ì´í„°: {statistics['non_zero_count']}ê°œ (0ì´ ì•„ë‹Œ ê°’) / {statistics['total_count']}ê°œ (ì „ì²´)")
            
            if statistics['zero_count'] > 0:
                print(f"   âš ï¸  {statistics['zero_count']}ê°œì˜ 0ê°’ ë°ì´í„° ì œì™¸í•˜ê³  í‰ê°€ë¨")
        
        # ëª¨ë¸ ì •ë³´ ì¶”ê°€
        result_data = {
            "status": "success",
            "model_id": model_info['model_info']['model_id'],
            "model_name": config['modelName'],
            "target_column": targetColumn,
            "prediction_count": len(results),
            "timestamp": datetime.now().isoformat(),
            "statistics": statistics,
            "predictions": results
        }
        
        # ì‹¤í—˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if model_info.get('experiment_info'):
            exp = model_info['experiment_info']
            result_data['experiment_info'] = {
                "experiment_id": int(exp['experiment_id']),
                "experiment_name": exp['experiment_name'],
                "training_accuracy": float(exp['accuracy']),
                "training_mape": float(exp['mape']),
                "r2_score": float(exp['r2_score'])
            }
        
        return result_data
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "message": str(e)
        }


def predict_future(model_info, last_data, future_steps=10):
    """
    ë§ˆì§€ë§‰ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¯¸ë˜ ì˜ˆì¸¡ (ì¬ê·€ì  ì˜ˆì¸¡)
    
    Args:
        model_info: ë¡œë“œëœ ëª¨ë¸ ì •ë³´
        last_data: ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ë°ì´í„° (DataFrame, ìµœì†Œ seqLen ê¸¸ì´)
        future_steps: ì˜ˆì¸¡í•  ë¯¸ë˜ ì‹œì  ìˆ˜
    
    Returns:
        list: ë¯¸ë˜ ì˜ˆì¸¡ê°’ë“¤
    """
    try:
        model = model_info['model']
        scaler = model_info['scaler']
        config = model_info['config']
        
        seq_len = int(config['r_seqLen'])
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        target_idx = study_columns_list.index(targetColumn)
        
        print(f"\nğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ì‹œì‘...")
        print(f"   - ì˜ˆì¸¡ ì‹œì  ìˆ˜: {future_steps}")
        
        # ì´ˆê¸° ì‹œí€€ìŠ¤ ì¤€ë¹„
        if len(last_data) < seq_len:
            raise ValueError(f"âŒ ìµœì†Œ {seq_len}ê°œì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ë§ˆì§€ë§‰ seqLen ê°œë§Œ ì‚¬ìš©
        initial_sequence = last_data[study_columns_list].tail(seq_len).astype(float)
        sequence_scaled = scaler.transform(initial_sequence.values)
        
        future_predictions = []
        current_sequence = sequence_scaled.copy()
        
        for step in range(future_steps):
            # í˜„ì¬ ì‹œí€€ìŠ¤ë¡œ ì˜ˆì¸¡
            input_seq = current_sequence[-seq_len:].reshape(1, seq_len, -1)
            pred_scaled = model.predict(input_seq, verbose=0)
            
            # ì—­ë³€í™˜
            mean_values = scaler.mean_.copy()
            mean_values[target_idx] = pred_scaled[0, 0]
            pred_original = scaler.inverse_transform(mean_values.reshape(1, -1))[0, target_idx]
            
            future_predictions.append(float(pred_original))
            
            # ë‹¤ìŒ ì‹œí€€ìŠ¤ ì¤€ë¹„
            next_point = current_sequence[-1].copy()
            next_point[target_idx] = pred_scaled[0, 0]
            
            # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
            current_sequence = np.vstack([current_sequence, next_point])
            
            print(f"   Step {step+1}/{future_steps}: {pred_original:.3f}")
        
        print(f"âœ… ë¯¸ë˜ ì˜ˆì¸¡ ì™„ë£Œ!")
        return future_predictions
        
    except Exception as e:
        print(f"âŒ ë¯¸ë˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
        return None


def save_prediction_results(prediction_result, output_filename=None):
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        if output_filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"{prediction_result['model_name']}_prediction_{timestamp}.json"
        
        output_path = os.path.join(prediction_path, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(prediction_result, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return None


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# ============================================================================
if __name__ == "__main__":
    print("ğŸ”® LSTM ëª¨ë¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (DB ì—°ë™)")
    print("=" * 60)
    
    # DBì—ì„œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    print("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘...")
    models_df = get_available_models_from_db(show_stats=True)
    
    if models_df is None or models_df.empty:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        exit()
    
    print(f"\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ({len(models_df)}ê°œ):")
    print("=" * 100)
    
    # ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    for idx, row in models_df.iterrows():
        print(f"\n{idx + 1}. [{row['model_id']}] {row['model_name']}")
        print(f"   íƒ€ê²Ÿ: {row['target_column']} | ì‹œí€€ìŠ¤: {row['sequence_length']} | Epochs: {row['epochs']}")
        print(f"   ì´ ì‹¤í—˜: {row['total_experiments']}íšŒ", end='')
        
        if pd.notna(row['best_accuracy']):
            print(f" | ìµœê³  ì •í™•ë„: {row['best_accuracy']:.2f}% (MAPE: {row['best_mape']:.2f}%)")
            if pd.notna(row['best_r2_score']):
                print(f"   RÂ² Score: {row['best_r2_score']:.4f}", end='')
            print(f" | ë§ˆì§€ë§‰ ì‹¤í—˜: {row['last_experiment_date']}")
        else:
            print(" | ì„±ê³µí•œ ì‹¤í—˜ ì—†ìŒ")
    
    print("\n" + "=" * 100)
    
    # ëª¨ë¸ ì„ íƒ
    model_choice = input("\nëª¨ë¸ ë²ˆí˜¸ ë˜ëŠ” model_id ì…ë ¥: ").strip()
    
    if model_choice.isdigit():
        choice_num = int(model_choice)
        if choice_num <= len(models_df):
            # ë²ˆí˜¸ë¡œ ì„ íƒ
            selected_model_id = models_df.iloc[choice_num - 1]['model_id']
        else:
            # IDë¡œ ì„ íƒ
            selected_model_id = choice_num
    else:
        print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë‚˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        exit()
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\n{'='*60}")
    model_info = load_trained_model_from_db(model_id=selected_model_id, use_best_experiment=True)
    
    if model_info is None:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()
    
    config = model_info['config']
    
    # ì‚¬ìš© ëª¨ë“œ ì„ íƒ
    print(f"\n{'='*60}")
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“œ:")
    print("1. DBì—ì„œ ìƒˆ ë°ì´í„° ë¡œë“œí•˜ì—¬ ì˜ˆì¸¡")
    print("2. CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œí•˜ì—¬ ì˜ˆì¸¡")
    print("3. ë¯¸ë˜ ì˜ˆì¸¡ (ì¬ê·€ì )")
    print("4. ëª¨ë¸ì˜ ì‹¤í—˜ ì´ë ¥ ì¡°íšŒ")
    
    mode = input("\nëª¨ë“œ ì„ íƒ (1-4): ").strip()
    
    if mode == "1":
        # DBì—ì„œ ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì„¤ì •:")
        print(f"   í…Œì´ë¸”: {config.get('tablename', 'lstm_input_1m')}")
        
        start_date = input("ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD HH:MM:SS, Enter=ì „ì²´): ").strip() or None
        end_date = input("ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD HH:MM:SS, Enter=ì „ì²´): ").strip() or None
        limit = input("ìµœëŒ€ í–‰ ìˆ˜ (Enter=ì œí•œì—†ìŒ): ").strip()
        limit = int(limit) if limit else None
        
        new_data = load_new_data_from_db(
            config.get('tablename', 'lstm_input_1m'),
            config['dateColumn'],
            config['studyColumns'],
            start_date=start_date,
            end_date=end_date,
            limit=limit
        )
        
        if new_data is not None:
            # ì˜ˆì¸¡ ìˆ˜í–‰
            result = predict_with_model(model_info, new_data)
            
            if result['status'] == 'success':
                # ê²°ê³¼ ì €ì¥
                save_prediction_results(result)
                
                # ìµœê·¼ 5ê°œ ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ“‹ ìµœê·¼ 5ê°œ ì˜ˆì¸¡ ê²°ê³¼:")
                for pred in result['predictions'][-5:]:
                    print(f"   {pred['date'][:19]}: ì˜ˆì¸¡={pred['predicted_value']:.3f}", end='')
                    if pred['actual_value'] is not None:
                        print(f", ì‹¤ì œ={pred['actual_value']:.3f}, ì˜¤ì°¨={pred['percentage_error']:.2f}%")
                    else:
                        print()
    
    elif mode == "2":
        # CSV íŒŒì¼ì—ì„œ ë¡œë“œ
        csv_file = input("\nCSV íŒŒì¼ ê²½ë¡œ: ").strip()
        
        try:
            new_data = pd.read_csv(csv_file)
            print(f"âœ… CSV íŒŒì¼ ë¡œë“œ: {len(new_data)}í–‰")
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            result = predict_with_model(model_info, new_data)
            
            if result['status'] == 'success':
                save_prediction_results(result)
                
        except Exception as e:
            print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    elif mode == "3":
        # ë¯¸ë˜ ì˜ˆì¸¡
        print(f"\nğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ëª¨ë“œ")
        
        # ì´ˆê¸° ë°ì´í„° ë¡œë“œ
        limit = config['r_seqLen'] + 100
        
        last_data = load_new_data_from_db(
            config.get('tablename', 'lstm_input_1m'),
            config['dateColumn'],
            config['studyColumns'],
            limit=limit
        )
        
        if last_data is not None:
            future_steps = int(input("ì˜ˆì¸¡í•  ë¯¸ë˜ ì‹œì  ìˆ˜: ").strip() or "10")
            
            future_preds = predict_future(model_info, last_data, future_steps)
            
            if future_preds:
                print(f"\nğŸ“Š ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼:")
                for i, pred in enumerate(future_preds, 1):
                    print(f"   Step {i}: {pred:.3f}")
    
    elif mode == "4":
        # ì‹¤í—˜ ì´ë ¥ ì¡°íšŒ
        print(f"\nğŸ“Š ëª¨ë¸ì˜ ì‹¤í—˜ ì´ë ¥ ì¡°íšŒ")
        limit = int(input("ì¡°íšŒí•  ì‹¤í—˜ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ): ").strip() or "10")
        
        experiments = get_experiments_by_model(selected_model_id, limit)
        
        if experiments is not None and not experiments.empty:
            print(f"\nğŸ“‹ ì‹¤í—˜ ì´ë ¥ ({len(experiments)}ê°œ):")
            print(experiments.to_string(index=False))
        else:
            print("âŒ ì¡°íšŒëœ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    else:
        print("âŒ ì˜ëª»ëœ ëª¨ë“œ ì„ íƒ")
    
    print(f"\n{'='*60}")
    print("âœ… ì‘ì—… ì™„ë£Œ!")