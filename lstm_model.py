# -*- coding: utf-8 -*-
"""
Title   : ì™¸ë¶€ë°ì´í„° ê¸°ë°˜ LSTM ì˜ˆì¸¡ ë° ì‹¤ì‹œê°„ í•™ìŠµ
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
"""

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import psycopg2
from sklearn.preprocessing import StandardScaler
import json
import joblib
from sqlalchemy import create_engine  # âœ… SQLAlchemy ì¶”ê°€

# í™˜ê²½ ì„¤ì •
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"
else:
    root = "/app/webfiles/lstm"

# ê²½ë¡œ ì„¤ì •
graph_path = os.path.abspath(root + "/graphImage")
os.makedirs(graph_path, exist_ok=True)
model_path = os.path.abspath(root + "/saved_models")
os.makedirs(model_path, exist_ok=True)

# âœ… PostgreSQL ì—°ê²° í•¨ìˆ˜ (SQLAlchemy ì‚¬ìš©)
def get_db_engine():
    """SQLAlchemy ì—”ì§„ ìƒì„±"""
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    return create_engine(connection_string)

def get_db_connection():
    """ê¸°ì¡´ psycopg2 ì—°ê²° (í•„ìš”ì‹œ ì‚¬ìš©)"""
    return psycopg2.connect(
        dbname="postgres",
        user="postgres",
        password="mapinus",
        host="10.10.10.201",
        port="5432"
    )

# ğŸ”¸ ì£¼ì‹ ë°ì´í„° ê¸°ë°˜ LSTM ëª¨ë¸ í•™ìŠµ API
def lstmLearningStock():
    """
    Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥í•œ ë²„ì „
    """
    # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ë° ì €ì¥ ê²½ë¡œ ì„¤ì •
    makeRoot = os.getenv("ROOT_PATH", root)
    finance_path = os.path.join(makeRoot, "finance_data")
    os.makedirs(finance_path, exist_ok=True)

    # âœ… ì¶”ê°€ëœ ì»¬ëŸ¼ ê´€ë ¨ íŒŒë¼ë¯¸í„° ìˆ˜ì‹ 
    dateColumn = "time_point"
    studyColumns = "solar_kwh,usage_kwh"
    targetColumn = "solar_kwh"

    lstmData = None

    # DBì—ì„œ ë°ì´í„°ë¥¼ ë½‘ì•„ì„œ data ë³€ìˆ˜ì— ë‹´ê¸°
    try:
        tablename = "lstm_input_5m"
        engine = get_db_engine()  # âœ… SQLAlchemy ì—”ì§„ ì‚¬ìš©

        # âœ… AWS ê´€ì¸¡ ì •ë³´ + ìœ„ì¹˜ ì •ë³´ ì¡°ì¸ ì¿¼ë¦¬
        query = f"""
        SELECT {studyColumns},{dateColumn}
        FROM carbontwin.{tablename}
        WHERE {dateColumn} IS NOT NULL
        ORDER BY {dateColumn} ASC
        """

        # âœ… SQLAlchemy ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ê²½ê³  í•´ê²°
        lstmData = pd.read_sql_query(query, engine)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(lstmData)}í–‰")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        return {"status": "error", "message": str(e)}
        
    # ğŸ”¹ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ í•™ìŠµ í•¨ìˆ˜ ì‹¤í–‰
    def background_task():
        try:
            result = lstmFinance(lstmData, dateColumn, studyColumns, targetColumn)
            print("âœ… ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì„±ê³µ:", result)
            return result
        except Exception as e:
            print("âŒ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì˜¤ë¥˜:", str(e))
            return {"status": "error", "message": str(e)}

    # âœ… ì‹¤ì œë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹¤í–‰
    result = background_task()
    
    return {"status": "success", "message": "í•™ìŠµì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "result": result}

# ë©”ì¸ í•¨ìˆ˜
def lstmFinance(lstmData, dateColumn, studyColumns, targetColumn):

    if not tf.executing_eagerly():
        tf.config.run_functions_eagerly(True)

    # modelName = "training_1"
    try:
        modelName = input("ëª¨ë¸ëª…: ").strip() or "training_1"
        r_epochs = int(input("ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 20): ").strip() or "20")
        r_batchSize = int(input("ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 16): ").strip() or "16")
        r_validationSplit = float(input("ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1): ").strip() or "0.1")
        r_seqLen = int(input("ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 14): ").strip() or "14")
        r_predDays = int(input("ì˜ˆì¸¡ ì¼ìˆ˜ (ê¸°ë³¸ê°’: 1): ").strip() or "1")
    except ValueError:
        print("âŒ ì˜ëª»ëœ ì…ë ¥ê°’ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        modelName = "training_1"
        r_epochs = 20
        r_batchSize = 16
        r_validationSplit = 0.1
        r_seqLen = 14
        r_predDays = 1
    # r_epochs = 20
    # r_batchSize = 16
    # r_validationSplit = 0.1
    # r_seqLen = 14
    # r_predDays = 1
    sessionId = "train_user_1"

    training_loss_path = graph_path + "/" + modelName + "_trainingLoss.png"
    total_graph_path = graph_path + "/" + modelName + "_totalgraph.png"
    diff_graph_path = graph_path + "/" + modelName + "_diffgraph.png"
    model_file_path = os.path.join(model_path, modelName + ".h5")

    stock_data = lstmData
    
    # âœ… ë°ì´í„° ê²€ì¦ ì¶”ê°€
    if stock_data.empty:
        print(f"âŒ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {"status": "error", "message": "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
    
    # âœ… targetColumnì´ studyColumnsì— ìˆëŠ”ì§€ í™•ì¸
    study_columns_list = [col.strip() for col in studyColumns.split(',')]
    if targetColumn not in study_columns_list:
        print(f"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ '{targetColumn}'ì´ í•™ìŠµ ì»¬ëŸ¼ì— ì—†ìŠµë‹ˆë‹¤.")
        return {"status": "error", "message": f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{targetColumn}'ì´ í•™ìŠµ ì»¬ëŸ¼ì— ì—†ìŠµë‹ˆë‹¤."}

    # original_open = stock_data[targetColumn].values  # âœ… ì´ ë¶€ë¶„ ìˆ˜ì • í•„ìš”
    # dates = pd.to_datetime(stock_data[dateColumn], errors='coerce')  # âœ… dateColumnì´ ì—†ì„ ìˆ˜ ìˆìŒ
    
    # âœ… ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬ (ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©)
    if dateColumn in stock_data.columns:
        dates = pd.to_datetime(stock_data[dateColumn], errors='coerce')
    else:
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê°€ìƒì˜ ë‚ ì§œ ìƒì„±
        dates = pd.date_range(start='2023-01-01', periods=len(stock_data), freq='5T')
        print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ '{dateColumn}'ì´ ì—†ì–´ì„œ ê°€ìƒ ë‚ ì§œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    
    # âœ… íƒ€ê²Ÿ ì»¬ëŸ¼ ë°ì´í„° ì¶”ì¶œ
    original_open = stock_data[targetColumn].values
    
    # âœ… í•™ìŠµìš© ë°ì´í„° ì„ íƒ
    stock_data_for_training = stock_data[study_columns_list].astype(float)

    scaler = StandardScaler()
    stock_data_scaled = scaler.fit_transform(stock_data_for_training)

    n_train = int(0.9 * stock_data_scaled.shape[0])
    train_data_scaled = stock_data_scaled[:n_train]
    test_data_scaled = stock_data_scaled[n_train:]
    test_dates = dates[n_train:]

    pred_days = int(r_predDays)
    seq_len = int(r_seqLen)
    input_dim = stock_data_for_training.shape[1]
    target_idx = study_columns_list.index(targetColumn)

    trainX, trainY, testX, testY = [], [], [], []
    for i in range(seq_len, n_train - pred_days + 1):
        trainX.append(train_data_scaled[i - seq_len:i, 0:input_dim])
        trainY.append(train_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    for i in range(seq_len, len(test_data_scaled) - pred_days + 1):
        testX.append(test_data_scaled[i - seq_len:i, 0:input_dim])
        testY.append(test_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    trainX, trainY = np.array(trainX), np.array(trainY)
    testX, testY = np.array(testX), np.array(testY)

    # âœ… Socket.IO ëŒ€ì‹  printë¡œ í•™ìŠµ ì‹œì‘ ì•Œë¦¼
    print(f"================ì ì‹œí›„ {modelName} ëª¨ë¸ í•™ìŠµ ì‹œì‘================")
    print(f"================ì„¤ì • ê°’: epochs={r_epochs}, batchSize={r_batchSize}, validationSplit={r_validationSplit}, seqLen={r_seqLen}, predDays={r_predDays}================")
    print(f"================ë‚ ì§œ ë°ì´í„° ì»¬ëŸ¼: {dateColumn}================")
    print(f"================í•™ìŠµ ë°ì´í„° ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸: {studyColumns}================")
    print(f"================í•™ìŠµ ë°ì´í„° ë°ì´í„° ìˆ˜ : {len(lstmData)}================")
    print(f"================ì˜ˆì¸¡ ë°ì´í„° ì»¬ëŸ¼: {targetColumn}================")

    try:
        model = load_model(model_file_path, compile=False)
        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
        print("âœ… Loaded full model from disk")
    except (OSError, IOError):
        print("ğŸ”„ Training model from scratch...")

        model = Sequential([
            Input(shape=(trainX.shape[1], trainX.shape[2])),
            LSTM(64, return_sequences=True),
            LSTM(32, return_sequences=False),
            Dense(trainY.shape[1])
        ])

        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')

        class TrainingCallback(Callback):
            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                print(f" Epoch {epoch + 1}: loss={logs.get('loss', 0):.4f}, val_loss={logs.get('val_loss', 0):.4f}")

        history = model.fit(
            trainX, trainY,
            epochs=int(r_epochs),
            batch_size=int(r_batchSize),
            validation_split=float(r_validationSplit),
            verbose=1,  # âœ… verbose=1ë¡œ ë³€ê²½í•˜ì—¬ ì§„í–‰ìƒí™© í™•ì¸
            callbacks=[TrainingCallback()]
        )

        model.save(model_file_path)
        print("âœ… Full model saved successfully.")

        plt.figure(figsize=(14, 5))
        plt.plot(history.history['loss'], label='Training loss')
        plt.plot(history.history['val_loss'], label='Validation loss')
        plt.xlabel('Epoch'); plt.ylabel('Loss')
        plt.title('Training Loss Over Epochs')
        plt.legend()
        plt.savefig(training_loss_path)
        plt.close()

    prediction = model.predict(testX)

    mean_values_pred = np.repeat(scaler.mean_[np.newaxis, :], prediction.shape[0], axis=0)
    mean_values_pred[:, target_idx] = np.squeeze(prediction)
    y_pred = scaler.inverse_transform(mean_values_pred)[:, target_idx]

    mean_values_testY = np.repeat(scaler.mean_[np.newaxis, :], testY.shape[0], axis=0)
    mean_values_testY[:, target_idx] = np.squeeze(testY)
    testY_original = scaler.inverse_transform(mean_values_testY)[:, target_idx]
    valid_test_dates = test_dates[seq_len : seq_len + len(testY_original)]

    plt.figure(figsize=(14, 5))
    plt.plot(dates, original_open, color='green', label='Original '+targetColumn+' Value')
    plt.plot(valid_test_dates, testY_original, color='blue', label='Actual '+targetColumn+' Value')
    plt.plot(valid_test_dates, y_pred, color='red', linestyle='--', label='Predicted '+targetColumn+' Value')
    plt.xlabel(dateColumn); plt.ylabel(targetColumn+' Value')
    plt.title('Original, Actual and Predicted '+targetColumn+' Value')
    plt.legend()
    plt.savefig(total_graph_path)
    plt.close()

    zoom_start = max(0, len(valid_test_dates) - 50)
    plt.figure(figsize=(14, 5))
    plt.plot(valid_test_dates[zoom_start:], testY_original[zoom_start:], color='blue', label='Actual '+targetColumn+' Price')
    plt.plot(valid_test_dates[zoom_start:], y_pred[zoom_start:], color='red', linestyle='--', label='Predicted '+targetColumn+' Price')
    plt.xlabel(dateColumn); plt.ylabel(targetColumn+' Price')
    plt.title('Zoomed In Actual vs Predicted '+targetColumn+' Price')
    plt.legend()
    plt.savefig(diff_graph_path)
    plt.close()

    def mean_absolute_percentage_error(y_true, y_pred):
        mask = y_true != 0
        if np.sum(mask) == 0:
            return 999.0
        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

    mape = mean_absolute_percentage_error(testY_original, y_pred)
    accuracy = 100 - mape if not np.isnan(mape) else np.nan

    print(f"âœ… MAPE: {mape:.2f}%")
    print(f"âœ… ì˜ˆì¸¡ ì •í™•ë„: {accuracy:.2f}%")

    # config ì €ì¥
    config = {
        "targetColumn": targetColumn,
        "dateColumn": dateColumn,
        "studyColumns": studyColumns,
        "r_seqLen": r_seqLen,
        "r_predDays": r_predDays
    }
    with open(os.path.join(model_path, modelName + "_config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4, ensure_ascii=False)

    # scaler ì €ì¥
    joblib.dump(scaler, os.path.join(model_path, modelName + "_scaler.pkl"))

    return {
        "status": "success",
        "training_loss_img": "graphImage/" + modelName + "_trainingLoss.png",
        "total_graph_img": "graphImage/" + modelName + "_totalgraph.png",
        "diff_graph_img": "graphImage/" + modelName + "_diffgraph.png",
        "mape": round(mape, 2),
        "accuracy": round(accuracy, 2)
    }

# âœ… ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ì‘ë™
if __name__ == "__main__":
    result = lstmLearningStock()
    print("ìµœì¢… ê²°ê³¼:", result)