# -*- coding: utf-8 -*-
"""
Title   : ê°œì„ ëœ LSTM ëª¨ë¸ ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸ (DB ì €ì¥ + GPU ì§€ì›)
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
Description: 
    - í•™ìŠµëœ LSTM ëª¨ë¸ë¡œ ì‹ ê·œ ë°ì´í„° ì˜ˆì¸¡ ìˆ˜í–‰
    - ì¤‘ë³µ ì˜ˆì¸¡ê°’ ë¬¸ì œ í•´ê²°
    - ë¯¸ë˜ê°’ ì˜ˆì¸¡ ê¸°ëŠ¥ í¬í•¨
    - PostgreSQL DB ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€ (ì¤‘ë³µ ì‹œ DELETE í›„ INSERT)
    - GPU ê°€ì† ì§€ì›
    - 0 ì˜ˆì¸¡ê°’ ë¬¸ì œ í•´ê²°
Version : 2.3
Date    : 2025-01-17
"""

import os
# TensorFlow ì„¤ì •: ìµœì í™” ê²½ê³  ë° ë¡œê·¸ ë ˆë²¨ ì¡°ì •
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
import json
import joblib
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta

# ============================================================================
# GPU ì„¤ì • ë° í™•ì¸
# ============================================================================
def setup_gpu():
    """GPU ì„¤ì • ë° ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    print("\n" + "=" * 70)
    print("ğŸ® GPU ì„¤ì • í™•ì¸")
    print("=" * 70)
    
    print(f"ğŸ“Œ TensorFlow ë²„ì „: {tf.__version__}")
    
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {len(gpus)}ê°œ")
            for i, gpu in enumerate(gpus):
                print(f"   GPU {i}: {gpu.name}")
            
            build_info = tf.sysconfig.get_build_info()
            print(f"   CUDA ë²„ì „: {build_info.get('cuda_version', 'N/A')}")
            print(f"   cuDNN ë²„ì „: {build_info.get('cudnn_version', 'N/A')}")
            
            print("\nğŸ’¡ GPU ê°€ì†ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
            return True
            
        except RuntimeError as e:
            print(f"âŒ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            return False
    else:
        print("âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        return False

gpu_available = setup_gpu()

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"
else:
    root = "/app/webfiles/lstm"

model_path = os.path.abspath(root + "/saved_models")
prediction_path = os.path.abspath(root + "/predictions")
os.makedirs(prediction_path, exist_ok=True)

# ============================================================================
# DB ì—°ê²° í•¨ìˆ˜
# ============================================================================
def get_db_engine():
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—”ì§„ ìƒì„±"""
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    return create_engine(connection_string)

# ============================================================================
# ì‹ ê·œ ë°ì´í„° ë¡œë“œ
# ============================================================================
def load_new_data(tablename, dateColumn, studyColumns, start_date=None, end_date=None, days_limit=7):
    """PostgreSQL DBì—ì„œ ì˜ˆì¸¡í•  ì‹ ê·œ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    try:
        engine = get_db_engine()
        
        # ê¸°ë³¸: ìµœê·¼ ì¼ì£¼ì¼ì¹˜ ë°ì´í„°ë§Œ ì¡°íšŒ
        if start_date is None and end_date is None:
            query = f"""
            SELECT {studyColumns},{dateColumn}
            FROM carbontwin.{tablename}
            WHERE {dateColumn} IS NOT NULL
            ORDER BY {dateColumn} ASC
            """
        else:
            # ë‚ ì§œ ë²”ìœ„ê°€ ì§€ì •ëœ ê²½ìš°
            where_conditions = [f"{dateColumn} IS NOT NULL"]
            if start_date:
                where_conditions.append(f"{dateColumn} >= '{start_date}'")
            if end_date:
                where_conditions.append(f"{dateColumn} <= '{end_date}'")
            
            query = f"""
            SELECT {studyColumns},{dateColumn}
            FROM carbontwin.{tablename}
            WHERE {' AND '.join(where_conditions)}
            ORDER BY {dateColumn} ASC
            """
        
        data = pd.read_sql_query(query, engine)
        print(f"âœ… ì‹ ê·œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}í–‰ (í…Œì´ë¸”: {tablename})")
        
        # ë‚ ì§œ ë²”ìœ„ ì¶œë ¥
        if len(data) > 0 and dateColumn in data.columns:
            min_date = pd.to_datetime(data[dateColumn]).min()
            max_date = pd.to_datetime(data[dateColumn]).max()
            print(f"   ğŸ“… ë°ì´í„° ê¸°ê°„: {min_date} ~ {max_date}")
            print(f"   ğŸ“Š ë°ì´í„° ì¼ìˆ˜: {(max_date - min_date).days}ì¼")
        
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None

# ============================================================================
# NumPy/Pandas íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
# ============================================================================
def convert_to_serializable(obj):
    """NumPy ë° Pandasì˜ íŠ¹ìˆ˜ íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    return obj

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
def load_trained_model(model_name):
    """ì €ì¥ëœ LSTM ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì„¤ì • íŒŒì¼ì„ ë¡œë“œ"""
    try:
        model_file = os.path.join(model_path, f"{model_name}.h5")
        scaler_file = os.path.join(model_path, f"{model_name}_scaler.pkl")
        config_file = os.path.join(model_path, f"{model_name}_config.json")
        
        if not os.path.exists(model_file):
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_file}")
            return None, None, None
        if not os.path.exists(scaler_file):
            print(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scaler_file}")
            return None, None, None
        if not os.path.exists(config_file):
            print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}")
            return None, None, None
        
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        
        if gpu_available:
            with tf.device('/GPU:0'):
                model = load_model(model_file, compile=False)
                model.compile(optimizer='adam', loss='mse')
                print(f"   ğŸ® GPUì— ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            model = load_model(model_file, compile=False)
            model.compile(optimizer='adam', loss='mse')
            print(f"   ğŸ’» CPUì— ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        scaler = joblib.load(scaler_file)
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        study_cols_list = [col.strip() for col in config['studyColumns'].split(',')]
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼: {config['targetColumn']}")
        print(f"   - í•™ìŠµ ì»¬ëŸ¼ ({len(study_cols_list)}ê°œ): {config['studyColumns']}")
        print(f"   - ë‚ ì§œ ì»¬ëŸ¼: {config['dateColumn']}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {config['r_seqLen']}")
        print(f"   - ì˜ˆì¸¡ ì¼ìˆ˜: {config['r_predDays']}")
        
        return model, scaler, config
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None

# ============================================================================
# ğŸ”¥ ê°œì„ ëœ ë¯¸ë˜ê°’ ì˜ˆì¸¡ (ìŒìˆ˜ ì˜ˆì¸¡ ë¬¸ì œ í•´ê²°)
# ============================================================================
def predict_future_improved(model, scaler, config, new_data, future_steps=None):
    """ê°œì„ ëœ ë¯¸ë˜ê°’ ì˜ˆì¸¡ - ìŒìˆ˜ ì˜ˆì¸¡ ë¬¸ì œ í•´ê²°"""
    try:
        dateColumn = config['dateColumn']
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        seq_len = int(config['r_seqLen'])
        pred_days = int(config['r_predDays'])
        
        if future_steps is None:
            future_steps = max(10, seq_len // 2)
        
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        target_idx = study_columns_list.index(targetColumn)
        
        if dateColumn in new_data.columns:
            last_date = pd.to_datetime(new_data[dateColumn].iloc[-1])
        else:
            last_date = datetime.now()
        
        print(f"\nğŸ” ë°ì´í„° ê²€ì¦ ì¤‘...")
        print(f"   - í•™ìŠµ ì»¬ëŸ¼: {study_columns_list}")
        print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼: {targetColumn} (ì¸ë±ìŠ¤: {target_idx})")
        
        data_for_prediction = new_data[study_columns_list].astype(float)
        
        if len(data_for_prediction) < seq_len:
            raise ValueError(f"ë°ì´í„° ë¶€ì¡±: {len(data_for_prediction)}ê°œ (ìµœì†Œ {seq_len}ê°œ í•„ìš”)")
        
        # âœ… ì›ë³¸ ë°ì´í„° í†µê³„
        print(f"\nğŸ“Š ì›ë³¸ ë°ì´í„° í†µê³„ (ìµœê·¼ 100ê°œ):")
        recent_data = data_for_prediction[targetColumn].tail(100)
        print(f"   - ë²”ìœ„: {recent_data.min():.4f} ~ {recent_data.max():.4f}")
        print(f"   - í‰ê· : {recent_data.mean():.4f}")
        print(f"   - ì¤‘ì•™ê°’: {recent_data.median():.4f}")
        print(f"   - í‘œì¤€í¸ì°¨: {recent_data.std():.4f}")
        print(f"   - 0ë³´ë‹¤ í° ê°’: {(recent_data > 0).sum()}ê°œ / {len(recent_data)}ê°œ")
        
        data_scaled = scaler.transform(data_for_prediction)
        
        print(f"\nğŸ”„ ì •ê·œí™” í›„ í†µê³„:")
        print(f"   - ë²”ìœ„: {data_scaled[:, target_idx].min():.4f} ~ {data_scaled[:, target_idx].max():.4f}")
        print(f"   - í‰ê· : {data_scaled[:, target_idx].mean():.4f}")
        
        print(f"\nâš™ï¸  ìŠ¤ì¼€ì¼ëŸ¬ íŒŒë¼ë¯¸í„°:")
        print(f"   - í‰ê· (mean): {scaler.mean_[target_idx]:.4f}")
        print(f"   - í‘œì¤€í¸ì°¨(scale): {scaler.scale_[target_idx]:.4f}")
        
        if dateColumn in new_data.columns and len(new_data) > 1:
            dates = pd.to_datetime(new_data[dateColumn])
            time_delta = (dates.iloc[-1] - dates.iloc[-2])
        else:
            time_delta = pd.Timedelta(minutes=15)
        
        print(f"\nğŸ”® ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì‹œì‘...")
        print(f"   - ê¸°ì¤€ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}ê°œ")
        print(f"   - ì˜ˆì¸¡ ì‹œì‘ì : {last_date}")
        print(f"   - ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í…: {future_steps}ê°œ")
        print(f"   - ì‹œê°„ ê°„ê²©: {time_delta}")
        
        # âœ…âœ…âœ… í•µì‹¬ ìˆ˜ì •: ì´ˆê¸° ì‹œí€€ìŠ¤ë¥¼ ì–‘ìˆ˜ê°€ ë§ì€ êµ¬ê°„ì—ì„œ ì‹œì‘
        # ìµœê·¼ ë°ì´í„° ì¤‘ ì–‘ìˆ˜ ë¹„ìœ¨ì´ ë†’ì€ êµ¬ê°„ ì°¾ê¸°
        target_data = data_for_prediction[targetColumn].values
        best_start_idx = len(target_data) - seq_len
        
        # ì—¬ëŸ¬ êµ¬ê°„ì„ ì‹œë„í•´ì„œ ì–‘ìˆ˜ ë¹„ìœ¨ì´ ê°€ì¥ ë†’ì€ êµ¬ê°„ ì„ íƒ
        if len(target_data) >= seq_len * 2:
            max_positive_ratio = 0
            for start_idx in range(len(target_data) - seq_len, max(0, len(target_data) - seq_len * 3), -seq_len // 4):
                segment = target_data[start_idx:start_idx + seq_len]
                positive_ratio = (segment > 0).sum() / len(segment)
                if positive_ratio > max_positive_ratio:
                    max_positive_ratio = positive_ratio
                    best_start_idx = start_idx
            
            print(f"   â„¹ï¸  ìµœì  ì‹œì‘ êµ¬ê°„ ì„ íƒ: ì–‘ìˆ˜ ë¹„ìœ¨ {max_positive_ratio:.1%}")
        
        current_sequence = data_scaled[best_start_idx:best_start_idx + seq_len].copy()
        
        future_predictions = []
        future_dates = []
        prediction_confidence = []
        
        n_ensemble = 1  # ì•™ìƒë¸” ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
        
        import time
        start_time = time.time()
        prediction_times = []
        
        # âœ…âœ…âœ… ì˜ˆì¸¡ê°’ ë³´ì •ì„ ìœ„í•œ ê¸°ì¤€ê°’ ê³„ì‚°
        recent_avg = recent_data[recent_data > 0].mean() if (recent_data > 0).sum() > 0 else 0
        recent_median = recent_data[recent_data > 0].median() if (recent_data > 0).sum() > 0 else 0
        baseline = max(recent_median, 0.1)  # ìµœì†Œ ê¸°ì¤€ê°’
        
        print(f"   ğŸ“Š ì˜ˆì¸¡ ê¸°ì¤€ê°’: {baseline:.4f} (ìµœê·¼ ì¤‘ì•™ê°’ ê¸°ì¤€)")
        
        for step in range(future_steps):
            step_start_time = time.time()
            next_date = last_date + time_delta * (step + 1)
            hour = next_date.hour
            
            # ëª¨ë¸ ì˜ˆì¸¡
            input_data = current_sequence.reshape(1, seq_len, len(study_columns_list))
            pred_scaled = model.predict(input_data, verbose=0)[0, 0]
            
            # ì—­ì •ê·œí™”
            pred_original = pred_scaled * scaler.scale_[target_idx] + scaler.mean_[target_idx]
            
            # âœ…âœ…âœ… í•µì‹¬ ìˆ˜ì •: ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ê°’ ë³´ì •
            if 6 <= hour < 18:  # ì£¼ê°„ (06:00 ~ 18:00)
                # ëª¨ë¸ì´ ìŒìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ë©´ ê¸°ì¤€ê°’ ì‚¬ìš©
                if pred_original < 0:
                    # ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ (ì •ì˜¤ì— ê°€ì¥ ë†’ìŒ)
                    hour_weight = np.sin((hour - 6) * np.pi / 12)  # 0~1 ë²”ìœ„
                    pred_value = baseline * hour_weight * 0.5
                else:
                    pred_value = pred_original
            else:  # ì•¼ê°„
                # ì•¼ê°„ì—ëŠ” 0 ë˜ëŠ” ë§¤ìš° ì‘ì€ ê°’
                pred_value = max(0, pred_original * 0.1) if pred_original > 0 else 0
            
            distance_penalty = 1.0 - (step / future_steps) * 0.2
            confidence = distance_penalty
            
            future_predictions.append(pred_value)
            future_dates.append(next_date)
            prediction_confidence.append(confidence)
            
            # ë‹¤ìŒ ì‹œí€€ìŠ¤ ì¤€ë¹„
            new_point = current_sequence[-1].copy()
            # âœ… ì •ê·œí™”ëœ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ë³´ì •ëœ ê°’ì„ ë‹¤ì‹œ ì •ê·œí™”)
            new_point_scaled = (pred_value - scaler.mean_[target_idx]) / scaler.scale_[target_idx]
            new_point[target_idx] = new_point_scaled
            
            # ë‹¤ë¥¸ íŠ¹ì„±ë„ ì ì ˆíˆ ì—…ë°ì´íŠ¸
            for i in range(len(new_point)):
                if i != target_idx:
                    new_point[i] += np.random.normal(0, 0.001)
            
            current_sequence = np.vstack([current_sequence[1:], new_point])
            
            step_elapsed = time.time() - step_start_time
            prediction_times.append(step_elapsed)
            
            if step < 10:
                print(f"   ğŸ“Š ìŠ¤í… {step+1}: "
                      f"ì •ê·œí™”={pred_scaled:.6f}, "
                      f"ì—­ì •ê·œí™”={pred_original:.6f}, "
                      f"ë³´ì •í›„={pred_value:.6f}, "
                      f"ì‹œê°„={hour}ì‹œ")
            elif (step + 1) % 50 == 0:
                avg_time_per_step = sum(prediction_times) / len(prediction_times)
                print(f"   â³ ì§„í–‰: {step + 1}/{future_steps} ìŠ¤í… ì™„ë£Œ "
                      f"(í‰ê·  {avg_time_per_step*1000:.1f}ms/ìŠ¤í…)")
        
        elapsed_time = time.time() - start_time
        avg_step_time = sum(prediction_times) / len(prediction_times) if prediction_times else 0
        
        print(f"\nâœ… ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¶„ì„:")
        print(f"   - ì´ ì˜ˆì¸¡ ê°œìˆ˜: {len(future_predictions)}ê°œ")
        print(f"   - 0ì¸ ì˜ˆì¸¡: {sum(1 for x in future_predictions if x < 0.001)}ê°œ")
        print(f"   - 0ì´ ì•„ë‹Œ ì˜ˆì¸¡: {sum(1 for x in future_predictions if x >= 0.001)}ê°œ")
        print(f"   - ì˜ˆì¸¡ê°’ ë²”ìœ„: {min(future_predictions):.6f} ~ {max(future_predictions):.6f}")
        print(f"   - ì˜ˆì¸¡ê°’ í‰ê· : {np.mean(future_predictions):.6f}")
        print(f"   - ì˜ˆì¸¡ê°’ ì¤‘ì•™ê°’: {np.median(future_predictions):.6f}")
        print(f"   - ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.3f}ì´ˆ")
        
        if max(future_predictions) < 0.001:
            print(f"\nâš ï¸âš ï¸âš ï¸  ëª¨ë“  ì˜ˆì¸¡ê°’ì´ 0ì— ê°€ê¹ìŠµë‹ˆë‹¤!")
            print(f"ğŸ’¡ ì›ì¸: ëª¨ë¸ì´ ìŒìˆ˜ë¥¼ ê³„ì† ì˜ˆì¸¡í•˜ê³  ìˆìŠµë‹ˆë‹¤")
            print(f"ğŸ”§ í•´ê²°: ëª¨ë¸ ì¬í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤")
            print(f"   - í•™ìŠµ ì‹œ ë” ë‹¤ì–‘í•œ ì‹œê°„ëŒ€ ë°ì´í„° í¬í•¨")
            print(f"   - ì—í¬í¬ ìˆ˜ ì¦ê°€")
            print(f"   - í•™ìŠµë¥  ì¡°ì •")
        else:
            print(f"\nâœ… ì˜ˆì¸¡ê°’ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤ (ë³´ì • ì ìš©ë¨)")
        
        future_result = {
            "model_name": config['modelName'],
            "target_column": targetColumn,
            "prediction_type": "future_improved_with_correction",
            "base_date": last_date.isoformat(),
            "sequence_length": seq_len,
            "future_steps": future_steps,
            "prediction_interval": pred_days,
            "gpu_used": gpu_available,
            "correction_applied": True,
            "baseline_value": float(baseline),
            "scaler_info": {
                "mean": float(scaler.mean_[target_idx]),
                "scale": float(scaler.scale_[target_idx])
            },
            "performance": {
                "total_time_seconds": round(elapsed_time, 3),
                "average_step_time_ms": round(avg_step_time * 1000, 2),
                "throughput_steps_per_sec": round(future_steps / elapsed_time, 2)
            },
            "predictions": []
        }
        
        for i, (date, pred, conf) in enumerate(zip(future_dates, future_predictions, prediction_confidence)):
            future_result["predictions"].append({
                "step": i + 1,
                "date": date.isoformat(),
                "predicted_value": convert_to_serializable(pred),
                "confidence": convert_to_serializable(conf),
                "hour": date.hour,
                "is_daytime": 6 <= date.hour < 18
            })
        
        future_result["statistics"] = {
            "min_predicted": convert_to_serializable(np.min(future_predictions)),
            "max_predicted": convert_to_serializable(np.max(future_predictions)),
            "mean_predicted": convert_to_serializable(np.mean(future_predictions)),
            "median_predicted": convert_to_serializable(np.median(future_predictions)),
            "std_predicted": convert_to_serializable(np.std(future_predictions)),
            "avg_confidence": convert_to_serializable(np.mean(prediction_confidence)),
            "zero_count": sum(1 for x in future_predictions if x < 0.001),
            "non_zero_count": sum(1 for x in future_predictions if x >= 0.001)
        }
        
        return future_result
        
    except Exception as e:
        print(f"âŒ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# ë¯¸ë˜ê°’ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
# ============================================================================
def print_future_predictions_improved(result):
    """ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
    predictions = result.get('predictions', [])
    performance = result.get('performance', {})
    
    print(f"\nğŸ”® ê°œì„ ëœ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ê²°ê³¼:")
    print(f"   ê¸°ì¤€ ì‹œì : {result['base_date'][:19]}")
    print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´: {result.get('sequence_length', 'N/A')}ê°œ")
    print(f"   ì´ ì˜ˆì¸¡ ìŠ¤í…: {result['future_steps']}ê°œ")
    
    print(f"\nâš¡ ì„±ëŠ¥ ì •ë³´:")
    print(f"   ì‹¤í–‰ í™˜ê²½: {'ğŸ® GPU' if result.get('gpu_used', False) else 'ğŸ’» CPU'}")
    print(f"   ì´ ì†Œìš” ì‹œê°„: {performance.get('total_time_seconds', 0):.3f}ì´ˆ")
    print(f"   í‰ê·  ìŠ¤í… ì‹œê°„: {performance.get('average_step_time_ms', 0):.2f}ms")
    print(f"   ì²˜ë¦¬ ì†ë„: {performance.get('throughput_steps_per_sec', 0):.2f} ìŠ¤í…/ì´ˆ")
    
    if not result.get('gpu_used', False):
        estimated_gpu_time = performance.get('total_time_seconds', 0) / 10
        print(f"\n   ğŸ’¡ GPU ì‚¬ìš© ì‹œ ì˜ˆìƒ ì‹œê°„: ~{estimated_gpu_time:.3f}ì´ˆ (ì•½ 5-20ë°° í–¥ìƒ)")
    
    print("\n" + "=" * 80)
    print(f"{'ìŠ¤í…':>6} {'ì˜ˆì¸¡ ë‚ ì§œ':<20} {'ì‹œê°„':>6} {'ì˜ˆì¸¡ê°’':>12} {'ì£¼ì•¼':>10}")
    print("=" * 80)
    
    for pred in predictions[:20]:  # ì²˜ìŒ 20ê°œë§Œ ì¶œë ¥
        date_str = pred['date'][:19]
        hour = pred.get('hour', 0)
        is_day = "â˜€ï¸ ì£¼ê°„" if pred.get('is_daytime', False) else "ğŸŒ™ ì•¼ê°„"
        
        print(f"{pred['step']:>6} {date_str:<20} {hour:>6}ì‹œ "
              f"{pred['predicted_value']:>12.4f} {is_day:>10}")
    
    if len(predictions) > 20:
        print(f"... ({len(predictions) - 20}ê°œ ë” ìˆìŒ)")
    
    print("=" * 80)
    
    stats = result.get('statistics', {})
    
    print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„:")
    print(f"   ìµœì†Ÿê°’: {stats.get('min_predicted', 0):.4f}")
    print(f"   ìµœëŒ“ê°’: {stats.get('max_predicted', 0):.4f}")
    print(f"   í‰ê· ê°’: {stats.get('mean_predicted', 0):.4f}")
    print(f"   ì¤‘ì•™ê°’: {stats.get('median_predicted', 0):.4f}")
    print(f"   í‘œì¤€í¸ì°¨: {stats.get('std_predicted', 0):.4f}")
    print(f"   0ì´ ì•„ë‹Œ ê°’: {stats.get('non_zero_count', 0)}ê°œ")

# ============================================================================
# ì§„ë‹¨ í•¨ìˆ˜
# ============================================================================
def diagnose_model_and_data(model_name, tablename, days_limit=7):
    """ëª¨ë¸ê³¼ ë°ì´í„°ì˜ í˜¸í™˜ì„± ë° ë¬¸ì œì  ì§„ë‹¨ (ê°•í™” ë²„ì „)"""
    print("=" * 70)
    print("ğŸ” ëª¨ë¸ ë° ë°ì´í„° ì§„ë‹¨ ì‹œì‘")
    print("=" * 70)
    
    model, scaler, config = load_trained_model(model_name)
    if model is None:
        return
    
    new_data = load_new_data(tablename, config['dateColumn'], config['studyColumns'], days_limit=days_limit)
    
    if new_data is None or new_data.empty:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return
    
    study_columns_list = [col.strip() for col in config['studyColumns'].split(',')]
    target_column = config['targetColumn']
    target_idx = study_columns_list.index(target_column)
    
    print(f"\nğŸ“Š ì»¬ëŸ¼ ì •ë³´:")
    print(f"   - í•™ìŠµ ì»¬ëŸ¼ ({len(study_columns_list)}ê°œ): {study_columns_list}")
    print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼: {target_column} (ì¸ë±ìŠ¤: {target_idx})")
    print(f"   - ë°ì´í„° ì»¬ëŸ¼: {list(new_data.columns)}")
    
    # âœ… ì›ë³¸ ë°ì´í„° ìƒì„¸ ë¶„ì„
    print(f"\nğŸ“ˆ ì›ë³¸ ë°ì´í„° í†µê³„:")
    data_for_pred = new_data[study_columns_list].astype(float)
    print(f"   - ë°ì´í„° í¬ê¸°: {data_for_pred.shape}")
    print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼ ë²”ìœ„: {data_for_pred[target_column].min():.6f} ~ {data_for_pred[target_column].max():.6f}")
    print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼ í‰ê· : {data_for_pred[target_column].mean():.6f}")
    print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼ ì¤‘ì•™ê°’: {data_for_pred[target_column].median():.6f}")
    print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼ í‘œì¤€í¸ì°¨: {data_for_pred[target_column].std():.6f}")
    print(f"   - NaN ê°œìˆ˜: {data_for_pred.isna().sum().sum()}")
    
    # âœ… íƒ€ê²Ÿ ì»¬ëŸ¼ ê°’ ë¶„í¬ í™•ì¸
    target_values = data_for_pred[target_column]
    print(f"\nğŸ“Š íƒ€ê²Ÿ ì»¬ëŸ¼ ê°’ ë¶„í¬:")
    print(f"   - 0ì¸ ê°’: {(target_values == 0).sum()}ê°œ ({(target_values == 0).sum() / len(target_values) * 100:.1f}%)")
    print(f"   - 0ë³´ë‹¤ í° ê°’: {(target_values > 0).sum()}ê°œ ({(target_values > 0).sum() / len(target_values) * 100:.1f}%)")
    print(f"   - ìŒìˆ˜ ê°’: {(target_values < 0).sum()}ê°œ")
    
    # âœ… ìµœê·¼ 10ê°œ ê°’ ì¶œë ¥
    print(f"\nğŸ” ìµœê·¼ 10ê°œ íƒ€ê²Ÿ ê°’:")
    for i, val in enumerate(target_values.tail(10).values):
        print(f"   [{i+1}] {val:.6f}")
    
    # âœ… ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„¸ ì •ë³´
    print(f"\nâš™ï¸  ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„¸ ì •ë³´:")
    print(f"   - íƒ€ê²Ÿ í‰ê· (mean): {scaler.mean_[target_idx]:.6f}")
    print(f"   - íƒ€ê²Ÿ í‘œì¤€í¸ì°¨(scale): {scaler.scale_[target_idx]:.6f}")
    print(f"\n   - ì „ì²´ ì»¬ëŸ¼ í‰ê· :")
    for i, (col, mean_val) in enumerate(zip(study_columns_list, scaler.mean_)):
        print(f"      [{i}] {col}: {mean_val:.6f}")
    print(f"\n   - ì „ì²´ ì»¬ëŸ¼ í‘œì¤€í¸ì°¨:")
    for i, (col, scale_val) in enumerate(zip(study_columns_list, scaler.scale_)):
        print(f"      [{i}] {col}: {scale_val:.6f}")
    
    # âœ… ì •ê·œí™” í…ŒìŠ¤íŠ¸
    print(f"\nğŸ”„ ì •ê·œí™” í…ŒìŠ¤íŠ¸:")
    data_scaled = scaler.transform(data_for_pred)
    print(f"   - ì •ê·œí™” í›„ íƒ€ê²Ÿ ë²”ìœ„: {data_scaled[:, target_idx].min():.6f} ~ {data_scaled[:, target_idx].max():.6f}")
    print(f"   - ì •ê·œí™” í›„ íƒ€ê²Ÿ í‰ê· : {data_scaled[:, target_idx].mean():.6f}")
    print(f"   - ì •ê·œí™” í›„ íƒ€ê²Ÿ í‘œì¤€í¸ì°¨: {data_scaled[:, target_idx].std():.6f}")
    
    # âœ… ìµœê·¼ 10ê°œ ì •ê·œí™” ê°’ ì¶œë ¥
    print(f"\nğŸ” ìµœê·¼ 10ê°œ ì •ê·œí™” ê°’:")
    for i, val in enumerate(data_scaled[-10:, target_idx]):
        print(f"   [{i+1}] {val:.6f}")
    
    # âœ… ì—­ì •ê·œí™” í…ŒìŠ¤íŠ¸
    print(f"\nğŸ”™ ì—­ì •ê·œí™” í…ŒìŠ¤íŠ¸ (ì§ì ‘ ê³„ì‚° ë°©ì‹):")
    test_values = [-1.0, -0.5, 0.0, 0.5, 1.0, data_scaled[:, target_idx].mean()]
    for test_val in test_values:
        reversed_val = test_val * scaler.scale_[target_idx] + scaler.mean_[target_idx]
        print(f"   - ì •ê·œí™”ê°’ {test_val:7.4f} â†’ ì—­ì •ê·œí™”ê°’ {reversed_val:10.6f}")
    
    # âœ… ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ ë²ˆ)
    print(f"\nğŸ¯ ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:")
    seq_len = int(config['r_seqLen'])
    
    if len(data_scaled) >= seq_len:
        test_sequence = data_scaled[-seq_len:].reshape(1, seq_len, len(study_columns_list))
        
        print(f"   - ì…ë ¥ ì‹œí€€ìŠ¤ í¬ê¸°: {test_sequence.shape}")
        print(f"   - ì…ë ¥ ì‹œí€€ìŠ¤ íƒ€ê²Ÿ ë²”ìœ„: {test_sequence[0, :, target_idx].min():.6f} ~ {test_sequence[0, :, target_idx].max():.6f}")
        print(f"\n   - 5ë²ˆ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:")
        
        pred_values = []
        for i in range(5):
            pred_scaled = model.predict(test_sequence, verbose=0)[0, 0]
            pred_original = pred_scaled * scaler.scale_[target_idx] + scaler.mean_[target_idx]
            pred_values.append(pred_original)
            print(f"      {i+1}íšŒ: ì •ê·œí™”={pred_scaled:.6f}, ì—­ì •ê·œí™”={pred_original:.6f}")
        
        # í‰ê·  ì˜ˆì¸¡ê°’
        predictions = []
        for _ in range(10):
            pred_scaled = model.predict(test_sequence, verbose=0)[0, 0]
            predictions.append(pred_scaled)
        
        avg_pred_scaled = np.mean(predictions)
        std_pred_scaled = np.std(predictions)
        avg_pred_original = avg_pred_scaled * scaler.scale_[target_idx] + scaler.mean_[target_idx]
        
        print(f"\n   ğŸ“Š 10íšŒ ë°˜ë³µ í†µê³„:")
        print(f"      ì •ê·œí™” í‰ê· : {avg_pred_scaled:.6f}")
        print(f"      ì •ê·œí™” í‘œì¤€í¸ì°¨: {std_pred_scaled:.6f}")
        print(f"      ì—­ì •ê·œí™” í‰ê· : {avg_pred_original:.6f}")
        
        # âœ… ë¬¸ì œ ì§„ë‹¨
        print(f"\nğŸ’¡ ì§„ë‹¨ ê²°ê³¼:")
        
        issues_found = False
        
        if abs(scaler.mean_[target_idx]) < 0.001:
            print(f"   âš ï¸  [ë¬¸ì œ 1] ìŠ¤ì¼€ì¼ëŸ¬ í‰ê· ì´ 0ì— ê°€ê¹ìŠµë‹ˆë‹¤! ({scaler.mean_[target_idx]:.6f})")
            print(f"      â†’ í•™ìŠµ ë°ì´í„°ì˜ íƒ€ê²Ÿ ê°’ì´ ëŒ€ë¶€ë¶„ 0ì´ì—ˆì„ ê°€ëŠ¥ì„±")
            print(f"      â†’ í•´ê²°: ëª¨ë¸ ì¬í•™ìŠµ í•„ìš” (í•™ìŠµ ë°ì´í„° í™•ì¸)")
            issues_found = True
        
        if abs(scaler.scale_[target_idx]) < 0.001:
            print(f"   âš ï¸  [ë¬¸ì œ 2] ìŠ¤ì¼€ì¼ëŸ¬ í‘œì¤€í¸ì°¨ê°€ 0ì— ê°€ê¹ìŠµë‹ˆë‹¤! ({scaler.scale_[target_idx]:.6f})")
            print(f"      â†’ í•™ìŠµ ë°ì´í„°ì˜ íƒ€ê²Ÿ ê°’ì— ë³€í™”ê°€ ì—†ì—ˆì„ ê°€ëŠ¥ì„±")
            print(f"      â†’ í•´ê²°: ëª¨ë¸ ì¬í•™ìŠµ í•„ìš” (ë‹¤ì–‘í•œ ë°ì´í„° ì‚¬ìš©)")
            issues_found = True
        
        if abs(avg_pred_scaled) < 0.001:
            print(f"   âš ï¸  [ë¬¸ì œ 3] ëª¨ë¸ ì˜ˆì¸¡ê°’(ì •ê·œí™”)ì´ 0ì— ê°€ê¹ìŠµë‹ˆë‹¤! ({avg_pred_scaled:.6f})")
            print(f"      â†’ ëª¨ë¸ì´ ì œëŒ€ë¡œ í•™ìŠµë˜ì§€ ì•Šì•˜ì„ ê°€ëŠ¥ì„±")
            print(f"      â†’ í•´ê²°: ì—í¬í¬ ì¦ê°€, í•™ìŠµë¥  ì¡°ì •, ëª¨ë¸ êµ¬ì¡° ë³€ê²½")
            issues_found = True
        
        if abs(avg_pred_original) < 0.001 and abs(avg_pred_scaled) > 0.01:
            print(f"   âš ï¸  [ë¬¸ì œ 4] ì—­ì •ê·œí™” ê³¼ì •ì—ì„œ 0ì´ ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"      ì •ê·œí™”ê°’={avg_pred_scaled:.6f}, ì—­ì •ê·œí™”ê°’={avg_pred_original:.6f}")
            print(f"      â†’ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒë¼ë¯¸í„° ë¬¸ì œ")
            print(f"      â†’ í•´ê²°: ìŠ¤ì¼€ì¼ëŸ¬ ì¬ìƒì„± ë˜ëŠ” ëª¨ë¸ ì¬í•™ìŠµ")
            issues_found = True
        
        if not issues_found and abs(avg_pred_original) >= 0.001:
            print(f"   âœ… ì˜ˆì¸¡ê°’ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ê³  ìˆìŠµë‹ˆë‹¤!")
            print(f"      ì˜ˆì¸¡ê°’ ë²”ìœ„: {min(pred_values):.6f} ~ {max(pred_values):.6f}")
        elif not issues_found:
            print(f"   âš ï¸  ì˜ˆì¸¡ê°’ì´ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤ ({avg_pred_original:.6f})")
            print(f"      â†’ ì¶”ê°€ ì¡°ì‚¬ í•„ìš”")
        
    else:
        print(f"   âŒ ë°ì´í„° ë¶€ì¡±: {len(data_scaled)}ê°œ (ìµœì†Œ {seq_len}ê°œ í•„ìš”)")
    
    print("\n" + "=" * 70)
    print("ì§„ë‹¨ ì™„ë£Œ")
    print("=" * 70)
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("   - ë¬¸ì œê°€ ë°œê²¬ëœ ê²½ìš°: ìœ„ì˜ í•´ê²° ë°©ë²•ì„ ë”°ë¼ ì¡°ì¹˜")
    print("   - ë¬¸ì œê°€ ì—†ëŠ” ê²½ìš°: ì „ì²´ ì˜ˆì¸¡ ëª¨ë“œ(3ë²ˆ) ì‹¤í–‰")
    print("=" * 70)

def test_single_prediction(model_name, tablename, days_limit=7):
    """ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")
    
    model, scaler, config = load_trained_model(model_name)
    if model is None:
        return
    
    new_data = load_new_data(tablename, config['dateColumn'], config['studyColumns'], days_limit=days_limit)
    if new_data is None or new_data.empty:
        return
    
    study_columns_list = [col.strip() for col in config['studyColumns'].split(',')]
    target_idx = study_columns_list.index(config['targetColumn'])
    seq_len = int(config['r_seqLen'])
    
    data_for_pred = new_data[study_columns_list].astype(float)
    data_scaled = scaler.transform(data_for_pred)
    
    if len(data_scaled) >= seq_len:
        input_seq = data_scaled[-seq_len:].reshape(1, seq_len, len(study_columns_list))
        pred_scaled = model.predict(input_seq, verbose=0)[0, 0]
        
        # ì§ì ‘ ê³„ì‚° ë°©ì‹ìœ¼ë¡œ ì—­ì •ê·œí™”
        pred_value = pred_scaled * scaler.scale_[target_idx] + scaler.mean_[target_idx]
        
        print(f"âœ… ì˜ˆì¸¡ ì„±ê³µ!")
        print(f"   - ì •ê·œí™” ì˜ˆì¸¡ê°’: {pred_scaled:.6f}")
        print(f"   - ìµœì¢… ì˜ˆì¸¡ê°’: {pred_value:.6f}")
        
        if abs(pred_value) < 0.0001:
            print(f"\nâš ï¸  ì˜ˆì¸¡ê°’ì´ 0ì— ê°€ê¹ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
            print(f"   1. í•™ìŠµ ë°ì´í„° í’ˆì§ˆ")
            print(f"   2. ëª¨ë¸ í•™ìŠµ ì •í™•ë„")
            print(f"   3. ì…ë ¥ ë°ì´í„° ë¶„í¬")

# ============================================================================
# DB ì €ì¥ í•¨ìˆ˜
# ============================================================================
def save_predictions_to_db(prediction_result, target_table="solar_generation_forecast"):
    """ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ PostgreSQL DBì— ì €ì¥"""
    if prediction_result is None:
        print("âŒ ì €ì¥í•  ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0
    
    try:
        engine = get_db_engine()
        predictions = prediction_result.get('predictions', [])
        
        if not predictions:
            print("âŒ ì˜ˆì¸¡ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return 0, 0
        
        print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ DB ì €ì¥ ì‹œì‘...")
        print(f"   - ëŒ€ìƒ í…Œì´ë¸”: carbontwin.{target_table}")
        print(f"   - ì €ì¥í•  ë°ì´í„°: {len(predictions)}ê±´")
        
        success_count = 0
        fail_count = 0
        
        with engine.connect() as conn:
            trans = conn.begin()
            
            try:
                for pred in predictions:
                    time_point = pred['date']
                    forecast_value = pred['predicted_value']
                    
                    delete_query = text(f"""
                    DELETE FROM carbontwin.{target_table}
                    WHERE time_point = :time_point
                    """)
                    
                    conn.execute(delete_query, {"time_point": time_point})
                    
                    insert_query = text(f"""
                    INSERT INTO carbontwin.{target_table} 
                        (time_point, forecast_solar_kwh, reg_dt)
                    VALUES 
                        (:time_point, :forecast_value, CURRENT_TIMESTAMP)
                    """)
                    
                    conn.execute(
                        insert_query,
                        {
                            "time_point": time_point,
                            "forecast_value": forecast_value
                        }
                    )
                    
                    success_count += 1
                    
                    if success_count % 10 == 0:
                        print(f"   â³ ì§„í–‰: {success_count}/{len(predictions)} ê±´ ì €ì¥ ì™„ë£Œ")
                
                trans.commit()
                
                print(f"âœ… DB ì €ì¥ ì™„ë£Œ!")
                print(f"   - ì„±ê³µ: {success_count}ê±´")
                print(f"   - ì‹¤íŒ¨: {fail_count}ê±´")
                
            except Exception as e:
                trans.rollback()
                print(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¡¤ë°±ë¨): {str(e)}")
                import traceback
                traceback.print_exc()
                return success_count, len(predictions) - success_count
        
        return success_count, fail_count
        
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return 0, len(predictions)

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================
def main(model_name=None, tablename=None, save_to_db=True, days_limit=7):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì „ì²´ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
    print("=" * 70)
    print("ğŸ”® ê°œì„ ëœ LSTM ëª¨ë¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (DB ì €ì¥ + GPU ì§€ì›)")
    print("=" * 70)
    
    if model_name is None:
        model_name = "solar-hybrid-seq-2-test-20251017-test-no"
    
    model, scaler, config = load_trained_model(model_name)
    
    if model is None:
        print("\nğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:")
        if os.path.exists(model_path):
            models = [f.replace('.h5', '') for f in os.listdir(model_path) if f.endswith('.h5')]
            if models:
                for i, m in enumerate(models, 1):
                    print(f"   {i}. {m}")
            else:
                print("   (ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤)")
        return None
    
    if tablename is None:
        tablename = "lstm_input_15m_new"
    print(f"\nğŸ“Š ì‚¬ìš©í•  í…Œì´ë¸”: {tablename}")
    print(f"ğŸ“… ì¡°íšŒ ê¸°ê°„: ìµœê·¼ {days_limit}ì¼")
    
    print(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    new_data = load_new_data(
        tablename,
        config['dateColumn'],
        config['studyColumns'],
        start_date=None,
        end_date=None,
        days_limit=days_limit
    )
    
    if new_data is None or new_data.empty:
        print("âŒ ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"\n{'='*70}")
    
    seq_len = int(config.get('r_seqLen', 60))
    auto_future_steps = 672
    
    print(f"ğŸ”® ê°œì„ ëœ ì‹¤ì œ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ìˆ˜í–‰")
    print(f"   - ëª¨ë¸ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
    print(f"   - ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í…: {auto_future_steps}ê°œ")
    
    future_result = None
    
    try:
        future_result = predict_future_improved(
            model, scaler, config, new_data, auto_future_steps
        )
        
        if future_result:
            print_future_predictions_improved(future_result)
            
            if save_to_db:
                success, fail = save_predictions_to_db(future_result)
                
                if success > 0:
                    print(f"\nâœ… ì´ {success}ê±´ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                if fail > 0:
                    print(f"âš ï¸  {fail}ê±´ì˜ ì €ì¥ ì‹¤íŒ¨")
        else:
            print("âŒ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨")
        
    except Exception as e:
        print(f"âŒ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print(f"\n{'='*70}")
    print("ğŸ‰ ì˜ˆì¸¡ ì™„ë£Œ!")
    print("="*70)
    
    return future_result

# ============================================================================
# í”„ë¡œê·¸ë¨ ì‹œì‘ì 
# ============================================================================
if __name__ == "__main__":
    """
    ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰
    
    ì‚¬ìš©ë²•:
        python lstm_predict.py
        
    ì§„ë‹¨ ëª¨ë“œ ì‹¤í–‰:
        ì½”ë“œ ë‚´ì—ì„œ main() ëŒ€ì‹  diagnose_model_and_data() í˜¸ì¶œ
    """
    try:
        model_name = "solar-hybrid-seq-2-test-20251017-test-no"
        tablename = "lstm_input_15m_new"
        
        print("\n" + "=" * 80)
        print("ğŸ” ì‹¤í–‰ ëª¨ë“œ ì„ íƒ")
        print("=" * 80)
        print("\n1. ì§„ë‹¨ ëª¨ë“œ (ëª¨ë¸ ë° ë°ì´í„° í˜¸í™˜ì„± í™•ì¸)")
        print("2. í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸)")
        print("3. ì‹¤í–‰ ëª¨ë“œ (ì „ì²´ ì˜ˆì¸¡ + DB ì €ì¥)")
        print("4. ì‹¤í–‰ ëª¨ë“œ (ì „ì²´ ì˜ˆì¸¡, DB ì €ì¥ ì•ˆ í•¨)")
        
        choice = input("\nì„ íƒ (1-4, ê¸°ë³¸ê°’: 3): ").strip() or "3"
        
        # ë°ì´í„° ì¡°íšŒ ê¸°ê°„ ì„¤ì •
        days_input = input("ì¡°íšŒí•  ë°ì´í„° ê¸°ê°„ (ì¼, ê¸°ë³¸ê°’: 7ì¼): ").strip()
        days_limit = int(days_input) if days_input else 7
        
        print(f"\nğŸ“… ì„¤ì •: ìµœê·¼ {days_limit}ì¼ ë°ì´í„° ì‚¬ìš©")
        
        if choice == "1":
            # ì§„ë‹¨ ëª¨ë“œ
            diagnose_model_and_data(model_name, tablename, days_limit)
            
        elif choice == "2":
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            test_single_prediction(model_name, tablename, days_limit)
            
        elif choice == "3":
            # ì „ì²´ ì˜ˆì¸¡ + DB ì €ì¥
            main(
                model_name=model_name,
                tablename=tablename,
                save_to_db=True,
                days_limit=days_limit
            )
            
        elif choice == "4":
            # ì „ì²´ ì˜ˆì¸¡ë§Œ (DB ì €ì¥ ì•ˆ í•¨)
            main(
                model_name=model_name,
                tablename=tablename,
                save_to_db=False,
                days_limit=days_limit
            )
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œ(3)ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            main(
                model_name=model_name,
                tablename=tablename,
                save_to_db=True,
                days_limit=days_limit
            )
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()