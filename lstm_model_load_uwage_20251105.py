# -*- coding: utf-8 -*-
"""
Title   : LSTM ì˜ˆì¸¡ - ê°œì„ ëœ ì—­ì •ê·œí™” ë°©ì‹
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
Description: 
    - âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë…ë¦½ì ì¸ ì—­ì •ê·œí™” ë°©ì‹
    - âœ… ì „ì²´ í”¼ì²˜ ë²¡í„°ë¥¼ í™œìš©í•œ ì•ˆì „í•œ ë³€í™˜
    - âœ… StandardScaler, MinMaxScaler ë“± ëª¨ë‘ ì§€ì›
Version : 9.0 (ì—­ì •ê·œí™” ê°œì„ )
Date    : 2025-10-28
"""

# ============================================================================
# í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================================================

import os
# TensorFlow ìµœì í™” ì˜µì…˜ ë¹„í™œì„±í™” (ê²½ê³  ë©”ì‹œì§€ ì–µì œ)
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
# TensorFlow ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ERRORë§Œ ì¶œë ¥)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import warnings
# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
import json
import joblib
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta

# ============================================================================
# í™˜ê²½ë³„ ê²½ë¡œ ì„¤ì •
# ============================================================================

# Flask í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ ë¡œì»¬/ì„œë²„ í™˜ê²½ êµ¬ë¶„
root = "D:/work/lstm"

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
model_path = os.path.abspath(root + "/saved_models")
prediction_path = os.path.abspath(root + "/predictions")
os.makedirs(prediction_path, exist_ok=True)

# ============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜
# ============================================================================

def get_db_engine():
    """
    PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—”ì§„ ìƒì„±
    
    Returns:
        SQLAlchemy Engine ê°ì²´
    """
    # connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    connection_string = "postgresql://postgres:mapinus%401004@10.10.10.201:5434/postgres"
    # connection_string = "postgresql://postgres:carbontwin@221.150.43.89:15432/postgres"
    return create_engine(connection_string)

# ============================================================================
# ë°ì´í„° ì§ë ¬í™” í•¨ìˆ˜
# ============================================================================

def convert_to_serializable(obj):
    """
    NumPy, Pandas ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    
    Args:
        obj: ë³€í™˜í•  ê°ì²´ (ndarray, int64, float64, Timestamp ë“±)
    
    Returns:
        ì§ë ¬í™” ê°€ëŠ¥í•œ Python ê¸°ë³¸ íƒ€ì… (list, int, float, str)
    """
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    return obj

# ============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ
# ============================================================================

def load_new_data(tablename, dateColumn, studyColumns):
    """
    ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•™ìŠµ/ì˜ˆì¸¡ì— í•„ìš”í•œ ë°ì´í„° ë¡œë“œ
    
    Args:
        tablename (str): í…Œì´ë¸”ëª… (ì˜ˆ: lstm_input_15m_new)
        dateColumn (str): ë‚ ì§œ ì»¬ëŸ¼ëª… (ì˜ˆ: time_point)
        studyColumns (str): ë¶„ì„í•  ì»¬ëŸ¼ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 'usage_kwh,week_code,is_weekend')
    
    Returns:
        pandas.DataFrame: ì‹œê³„ì—´ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„°
        None: ë¡œë“œ ì‹¤íŒ¨ ì‹œ
    """
    try:
        engine = get_db_engine()
        
        # SQL ì¿¼ë¦¬ ì‘ì„± (ë‚ ì§œ ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬)
        query = f"""
            SELECT {studyColumns},{dateColumn}
            FROM carbontwin.{tablename}
            WHERE {dateColumn} IS NOT NULL
              AND time_point >= (
                    SELECT MAX(time_point) - INTERVAL '1 days'
                    FROM carbontwin.{tablename}
                    WHERE time_point IS NOT null
                )
            ORDER BY {dateColumn} ASC
            """
        
        # ë°ì´í„° ë¡œë“œ
        data = pd.read_sql_query(query, engine)
        print(f"âœ… ë°ì´í„° ë¡œë“œ: {len(data)}í–‰")
        
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜ ë° ê¸°ê°„ ì¶œë ¥
        if len(data) > 0 and dateColumn in data.columns:
            data[dateColumn] = pd.to_datetime(data[dateColumn])
            min_date = data[dateColumn].min()
            max_date = data[dateColumn].max()
            print(f"   ğŸ“… ê¸°ê°„: {min_date} ~ {max_date}")
        
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None

# ============================================================================
# í•™ìŠµëœ LSTM ëª¨ë¸ ë¡œë“œ
# ============================================================================

def load_trained_model(model_name):
    """
    ì €ì¥ëœ LSTM ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì„¤ì • íŒŒì¼ ë¡œë“œ
    
    Args:
        model_name (str): ëª¨ë¸ëª… (ì˜ˆ: usage-kwh-model-4)
    
    Returns:
        tuple: (model, scaler, config)
            - model: Keras LSTM ëª¨ë¸
            - scaler: sklearn ìŠ¤ì¼€ì¼ëŸ¬ (StandardScaler/MinMaxScaler)
            - config: ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì„¤ì • ì •ë³´ (dict)
        (None, None, None): ë¡œë“œ ì‹¤íŒ¨ ì‹œ
    """
    try:
        # í•„ìš”í•œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        model_file = os.path.join(model_path, f"{model_name}.h5")
        scaler_file = os.path.join(model_path, f"{model_name}_scaler.pkl")
        config_file = os.path.join(model_path, f"{model_name}_config.json")
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not all(os.path.exists(f) for f in [model_file, scaler_file, config_file]):
            print(f"âŒ í•„ìš”í•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None
        
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ: {model_name}")
        
        # ê²½ê³  ë©”ì‹œì§€ ì–µì œí•˜ë©´ì„œ ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            model = load_model(model_file, compile=False)
            model.compile(optimizer='adam', loss='mse')
            scaler = joblib.load(scaler_file)
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ (JSON)
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # ë¡œë“œëœ ì •ë³´ ì¶œë ¥
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"   - íƒ€ê²Ÿ: {config['targetColumn']}")
        print(f"   - ì‹œí€€ìŠ¤: {config['r_seqLen']}")
        print(f"   - ìŠ¤ì¼€ì¼ëŸ¬: {type(scaler).__name__}")
        
        return model, scaler, config
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None, None, None

# ============================================================================
# ì‹¤ì œ ë°ì´í„°ë¡œ ëª¨ë¸ ê²€ì¦
# ============================================================================

def validate_with_actual_data(model, scaler, config, data, validation_days=1):
    """
    ìµœê·¼ Nì¼ì˜ ì‹¤ì œ ë°ì´í„°ë¡œ ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ (ê°œì„ ëœ ì—­ì •ê·œí™” ë°©ì‹)
    
    Args:
        model: LSTM ëª¨ë¸
        scaler: ë°ì´í„° ìŠ¤ì¼€ì¼ëŸ¬
        config (dict): ëª¨ë¸ ì„¤ì • ì •ë³´
        data (DataFrame): ì „ì²´ ë°ì´í„°
        validation_days (int): ê²€ì¦í•  ìµœê·¼ ì¼ìˆ˜ (ê¸°ë³¸ 7ì¼)
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼
            - status: ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ
            - statistics: ì •í™•ë„, MAPE, MAE, RMSE
            - historical_mean: ê²€ì¦ ë°ì´í„°ì˜ í‰ê· ê°’
            - historical_std: ê²€ì¦ ë°ì´í„°ì˜ í‘œì¤€í¸ì°¨
        None: ê²€ì¦ ì‹¤íŒ¨ ì‹œ
    
    ê²€ì¦ í”„ë¡œì„¸ìŠ¤:
        1. ìµœê·¼ Nì¼ ë°ì´í„°ë¥¼ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„ë¦¬
        2. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (seq_len ê¸¸ì´ì˜ ì…ë ¥ â†’ r_predDays í›„ì˜ ì¶œë ¥)
        3. ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
        4. ê°œì„ ëœ ì—­ì •ê·œí™” ë°©ì‹ ì ìš© (ì „ì²´ í”¼ì²˜ ë²¡í„° í™œìš©)
        5. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì •í™•ë„, MAPE, MAE, RMSE)
    """
    try:
        print(f"\n{'='*80}")
        print(f"ğŸ” ëª¨ë¸ ê²€ì¦ ì‹œì‘ (ìµœê·¼ {validation_days}ì¼)")
        print(f"{'='*80}")
        
        # ì„¤ì • ì •ë³´ ì¶”ì¶œ
        dateColumn = config['dateColumn']
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        seq_len = int(config['r_seqLen'])  # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì˜ˆ: 96 = 1ì¼ì¹˜ 15ë¶„ ë°ì´í„°)
        r_predDays = int(config.get('r_predDays', 1))  # ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜
        
        # ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° íƒ€ê²Ÿ ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        target_idx = study_columns_list.index(targetColumn)
        
        # ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„ (ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜)
        data_for_prediction = data[study_columns_list].astype(float)
        dates = pd.to_datetime(data[dateColumn])
        
        # ê²€ì¦ ë°ì´í„° ë²”ìœ„ ê³„ì‚° (96ê°œ/ì¼ * Nì¼)
        validation_points = 96 * validation_days
        validation_start_idx = len(data) - validation_points - r_predDays
        
        # ë°ì´í„° ì •ê·œí™” (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©)
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            data_scaled = scaler.transform(data_for_prediction)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (ê²€ì¦ìš©)
        testX, testY = [], []
        test_range = range(seq_len, len(data_scaled) - r_predDays + 1)
        
        for i in test_range:
            # ê²€ì¦ ì‹œì‘ ì§€ì  ì´ì „ ë°ì´í„°ëŠ” ì œì™¸
            if i < validation_start_idx:
                continue
            # X: ê³¼ê±° seq_len ìŠ¤í…ì˜ ëª¨ë“  í”¼ì²˜
            testX.append(data_scaled[i - seq_len:i, :].astype(np.float32))
            # Y: r_predDays í›„ì˜ íƒ€ê²Ÿ ê°’
            testY.append(data_scaled[i + r_predDays - 1:i + r_predDays, target_idx].astype(np.float32))
        
        testX = np.array(testX, dtype=np.float32)
        testY = np.array(testY, dtype=np.float32)
        
        print(f"\nğŸ”„ ì—­ì •ê·œí™” ë°©ì‹: ì „ì²´ í”¼ì²˜ ë²¡í„° í™œìš© (ìŠ¤ì¼€ì¼ëŸ¬ ë…ë¦½ì )")
        
        # ëª¨ë¸ ì˜ˆì¸¡ ë° ì—­ì •ê·œí™”
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            # 1. ëª¨ë¸ ì˜ˆì¸¡ (ì •ê·œí™”ëœ ê°’)
            prediction = model.predict(testX, verbose=0)
            
            # âœ… ê°œì„ ëœ ì—­ì •ê·œí™”: ì „ì²´ í”¼ì²˜ ë²¡í„° ë°©ì‹
            # - StandardScaler, MinMaxScaler ë“± ëª¨ë“  ìŠ¤ì¼€ì¼ëŸ¬ì— ëŒ€í•´ ì•ˆì „í•˜ê²Œ ì‘ë™
            # - ê° í”¼ì²˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì—­ë³€í™˜
            
            # ì˜ˆì¸¡ê°’ ì—­ì •ê·œí™”
            y_pred = []
            for i, pred_scaled in enumerate(prediction):
                # testX[i]ì˜ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì„ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©
                full_scaled = testX[i, -1, :].copy()  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì „ì²´ í”¼ì²˜
                full_scaled[target_idx] = pred_scaled[0]  # íƒ€ê²Ÿ ìœ„ì¹˜ì— ì˜ˆì¸¡ê°’ ì‚½ì…
                # ì „ì²´ í”¼ì²˜ë¥¼ í•œ ë²ˆì— ì—­ë³€í™˜
                full_original = scaler.inverse_transform(full_scaled.reshape(1, -1))[0]
                y_pred.append(full_original[target_idx])
            y_pred = np.array(y_pred)
            
            # ì‹¤ì œê°’ ì—­ì •ê·œí™”
            testY_original = []
            for i, y_scaled in enumerate(testY):
                full_scaled = testX[i, -1, :].copy()  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì „ì²´ í”¼ì²˜
                full_scaled[target_idx] = y_scaled[0]  # íƒ€ê²Ÿ ìœ„ì¹˜ì— ì‹¤ì œê°’ ì‚½ì…
                # ì „ì²´ í”¼ì²˜ë¥¼ í•œ ë²ˆì— ì—­ë³€í™˜
                full_original = scaler.inverse_transform(full_scaled.reshape(1, -1))[0]
                testY_original.append(full_original[target_idx])
            testY_original = np.array(testY_original)
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        eps = 9  
        mask = testY_original > eps  # ì„ê³„ê°’ë³´ë‹¤ í° ê°’ë§Œ ì‚¬ìš©
        # MAPE: Mean Absolute Percentage Error (í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨)
        mape = np.mean(np.abs((y_pred[mask] - testY_original[mask]) / testY_original[mask])) * 100 if np.sum(mask) > 0 else 999.0
        
        # ì •í™•ë„ = 100 - MAPE
        accuracy = 100 - mape
        # MAE: Mean Absolute Error (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨)
        mae = np.mean(np.abs(y_pred - testY_original))
        # RMSE: Root Mean Square Error (í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨)
        rmse = np.sqrt(np.mean((y_pred - testY_original) ** 2))
        
        # ê²€ì¦ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")
        print(f"   ì •í™•ë„: {accuracy:.2f}%")
        print(f"   MAPE:   {mape:.2f}%")
        print(f"   MAE:    {mae:.4f}")
        print(f"   RMSE:   {rmse:.4f}")
        
        return {
            "status": "success",
            "statistics": {"accuracy": accuracy, "mape": mape, "mae": mae, "rmse": rmse},
            "historical_mean": np.mean(testY_original),
            "historical_std": np.std(testY_original)
        }
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# ë¯¸ë˜ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ (ì•ˆì •í™” ìµœì†Œí™” ë²„ì „)
# ============================================================================

def predict_future_stable(model, scaler, config, data, future_steps=672, historical_mean=None, historical_std=None):
    """
    LSTM ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¯¸ë˜ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ (ì•ˆì •í™” ìµœì†Œí™”)
    
    Args:
        model: LSTM ëª¨ë¸
        scaler: ë°ì´í„° ìŠ¤ì¼€ì¼ëŸ¬
        config (dict): ëª¨ë¸ ì„¤ì •
        data (DataFrame): ì „ì²´ ê³¼ê±° ë°ì´í„°
        future_steps (int): ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ 672 = 7ì¼ * 96)
        historical_mean (float): ê³¼ê±° ë°ì´í„° í‰ê·  (ê²€ì¦ì—ì„œ ì „ë‹¬)
        historical_std (float): ê³¼ê±° ë°ì´í„° í‘œì¤€í¸ì°¨ (ê²€ì¦ì—ì„œ ì „ë‹¬)
    
    Returns:
        dict: ì˜ˆì¸¡ ê²°ê³¼
            - metadata: ëª¨ë¸ ì •ë³´, ì˜ˆì¸¡ ë°©ë²•, í•™ìŠµëœ íŒ¨í„´ ë“±
            - predictions: ê° ì‹œì ë³„ ì˜ˆì¸¡ê°’ ë¦¬ìŠ¤íŠ¸
            - statistics: ì˜ˆì¸¡ê°’ì˜ í†µê³„ (ìµœì†Œ, ìµœëŒ€, í‰ê· , í‘œì¤€í¸ì°¨)
        None: ì˜ˆì¸¡ ì‹¤íŒ¨ ì‹œ
    
    ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤:
        1. í‰ì¼/íœ´ì¼ íŒ¨í„´ í•™ìŠµ (ê³¼ê±° ë°ì´í„° ë¶„ì„)
        2. ë§ˆì§€ë§‰ seq_len ê¸¸ì´ì˜ ë°ì´í„°ë¡œ ì´ˆê¸° ì‹œí€€ìŠ¤ êµ¬ì„±
        3. ë°˜ë³µì ìœ¼ë¡œ ë‹¤ìŒ ìŠ¤í… ì˜ˆì¸¡:
           - ëª¨ë¸ë¡œ ì˜ˆì¸¡ (ì •ê·œí™”ëœ ê°’)
           - ì—­ì •ê·œí™” (ì „ì²´ í”¼ì²˜ ë²¡í„° ë°©ì‹)
           - ê·¹ë‹¨ì  ì´ìƒì¹˜ë§Œ ì œê±° (5Ïƒ ë²”ìœ„)
           - ì‹œê°„ íŠ¹ì„± ì—…ë°ì´íŠ¸ (ìš”ì¼, ì£¼ë§ ì—¬ë¶€ ë“±)
           - ì˜ˆì¸¡ê°’ì„ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•˜ì—¬ ë‹¤ìŒ ì˜ˆì¸¡ ì¤€ë¹„
        4. ê²°ê³¼ í†µê³„ ë° íŒ¨í„´ ë¹„êµ
    """
    
    # ------------------------------------------------------------------------
    # ë‚´ë¶€ í•¨ìˆ˜ 1: í‰ì¼/íœ´ì¼ íŒ¨í„´ í•™ìŠµ
    # ------------------------------------------------------------------------
    def calculate_workday_holiday_patterns(data_for_prediction, dates, targetColumn):
        """
        ê³¼ê±° ë°ì´í„°ì—ì„œ í‰ì¼/íœ´ì¼ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ í•™ìŠµ
        (ë°ì´í„° ë¶€ì¡± ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        """
        print(f"   ğŸ” í‰ì¼/íœ´ì¼ íŒ¨í„´ í•™ìŠµ ì¤‘...")
        
        target_values = data_for_prediction[targetColumn].values
        # í‰ì¼ ë§ˆìŠ¤í¬ (ì›”~ê¸ˆ: 0~4)
        weekday_mask = dates.dt.weekday < 5
        # ì£¼ë§ ë§ˆìŠ¤í¬ (í† ~ì¼: 5~6)
        weekend_mask = dates.dt.weekday >= 5
        
        weekday_values = target_values[weekday_mask]
        weekend_values = target_values[weekend_mask]
        
        # ğŸ”¥ í‰ì¼/íœ´ì¼ í†µê³„ ì •ë³´ ê³„ì‚° (ë°ì´í„° ì—†ì„ ê²½ìš° ëŒ€ë¹„)
        def safe_stats(values, name):
            """ë¹ˆ ë°°ì—´ì—ë„ ì•ˆì „í•œ í†µê³„ ê³„ì‚°"""
            if len(values) == 0:
                print(f"      âš ï¸  {name} ë°ì´í„° ì—†ìŒ â†’ ê¸°ë³¸ê°’ ì‚¬ìš©")
                return {
                    "mean": 0.0,
                    "std": 0.0,
                    "median": 0.0,
                    "min": 0.0,
                    "max": 0.0,
                    "q25": 0.0,
                    "q75": 0.0,
                    "zero_ratio": 1.0,
                    "count": 0
                }
            
            return {
                "mean": float(np.mean(values)),
                "std": float(np.std(values)),
                "median": float(np.median(values)),
                "min": float(np.min(values)),
                "max": float(np.max(values)),
                "q25": float(np.percentile(values, 25)),
                "q75": float(np.percentile(values, 75)),
                "zero_ratio": float(np.sum(values == 0) / len(values)),
                "count": len(values)
            }
        
        patterns = {
            "workday": safe_stats(weekday_values, "í‰ì¼"),
            "holiday": safe_stats(weekend_values, "íœ´ì¼")
        }
        
        # ìš”ì¼ë³„ ìƒì„¸ ì •ë³´ ê³„ì‚°
        weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        weekday_details = {}
        
        for day_idx in range(7):
            day_mask = dates.dt.weekday == day_idx
            day_values = target_values[day_mask]
            
            if len(day_values) > 0:
                weekday_details[day_idx] = {
                    "name": weekday_names[day_idx],
                    "mean": float(np.mean(day_values)),
                    "std": float(np.std(day_values)),
                    "zero_ratio": float(np.sum(day_values == 0) / len(day_values)),
                    "count": len(day_values),
                    "is_workday": day_idx < 5
                }
        
        return patterns, weekday_details
    
    # ------------------------------------------------------------------------
    # ë‚´ë¶€ í•¨ìˆ˜ 2: ì˜ˆì¸¡ê°’ ì•ˆì •í™” (ê·¹ë‹¨ì  ì´ìƒì¹˜ë§Œ ì œê±°)
    # ------------------------------------------------------------------------
    def adaptive_stabilization(pred_original, next_date, patterns):
        """
        âœ… ì˜ˆì¸¡ê°’ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê·¹ë‹¨ì  ì´ìƒì¹˜ë§Œ ì œê±°)
        (ë°ì´í„° ë¶€ì¡± ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        """
        day_of_week = next_date.weekday()
        is_workday = day_of_week < 5
        
        # í‰ì¼/íœ´ì¼ íŒ¨í„´ ì„ íƒ
        if is_workday:
            pattern = patterns["workday"]
            day_type = "í‰ì¼"
            icon = "ğŸ¢"
        else:
            pattern = patterns["holiday"]
            day_type = "íœ´ì¼"
            icon = "ğŸ–ï¸"
        
        weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        weekday_name = weekday_names[day_of_week]
        
        mean = pattern["mean"]
        std = pattern["std"]
        
        stabilization_applied = False
        stabilization_reason = ""
        
        # ğŸ”¥ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° (count == 0) ë˜ëŠ” í‘œì¤€í¸ì°¨ê°€ 0ì¸ ê²½ìš°
        if pattern["count"] == 0 or std == 0:
            # ì•ˆì •í™” ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë‹¨, ìŒìˆ˜ë§Œ ì œê±°)
            pred_original = max(0, pred_original)
            return pred_original, False, "ë°ì´í„° ë¶€ì¡±(ì•ˆì •í™” ìŠ¤í‚µ)", weekday_name, day_type, icon
        
        # ì •ìƒì ì¸ ì•ˆì •í™” (5Ïƒ ë²”ìœ„)
        safe_min = max(0, mean - 5 * std)
        safe_max = mean + 5 * std
        
        # ê·¹ë‹¨ì  ì´ìƒì¹˜ë§Œ ì œê±°
        if pred_original < safe_min:
            pred_original = safe_min
            stabilization_applied = True
            stabilization_reason = f"ê·¹ë‹¨ì  ìµœì†Œê°’ ({safe_min:.1f} ë¯¸ë§Œ)"
        elif pred_original > safe_max:
            pred_original = safe_max
            stabilization_applied = True
            stabilization_reason = f"ê·¹ë‹¨ì  ìµœëŒ€ê°’ ({safe_max:.1f} ì´ˆê³¼)"
        
        # ìŒìˆ˜ ë°©ì§€
        pred_original = max(0, pred_original)
        
        return pred_original, stabilization_applied, stabilization_reason, weekday_name, day_type, icon
    
    # ------------------------------------------------------------------------
    # ë‚´ë¶€ í•¨ìˆ˜ 3: ì‹œê°„ íŠ¹ì„± ì—…ë°ì´íŠ¸
    # ------------------------------------------------------------------------
    def update_time_features(next_row, next_date, study_columns_list):
        """
        ì‹œê°„ ê´€ë ¨ íŠ¹ì„± ë™ì  ì—…ë°ì´íŠ¸
        
        Args:
            next_row (array): ë‹¤ìŒ ì‹œì ì˜ í”¼ì²˜ ë²¡í„°
            next_date (datetime): ë‹¤ìŒ ì‹œì ì˜ ë‚ ì§œ
            study_columns_list (list): í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            array: ì‹œê°„ íŠ¹ì„±ì´ ì—…ë°ì´íŠ¸ëœ í”¼ì²˜ ë²¡í„°
        
        ì—…ë°ì´íŠ¸ í•­ëª©:
        - week_code: ìš”ì¼ ì½”ë“œ (1~7)
        - is_weekend: ì£¼ë§ ì—¬ë¶€ (0 or 1)
        - is_workday: í‰ì¼ ì—¬ë¶€ (0 or 1)
        - day_sin, day_cos: ìš”ì¼ì˜ ìˆœí™˜ íŠ¹ì„± (ì‚¬ì¸/ì½”ì‚¬ì¸ ì¸ì½”ë”©)
        
        ì´ìœ : ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œ ìš”ì¼ ì •ë³´ê°€ ì¤‘ìš”í•œ í”¼ì²˜ì´ë¯€ë¡œ
              ë‹¤ìŒ ì‹œì ì˜ ìš”ì¼ ì •ë³´ë¥¼ ì •í™•íˆ ë°˜ì˜í•´ì•¼ í•¨
        """
        day_of_week = next_date.weekday()
        
        # week_code: ìš”ì¼ ì½”ë“œ ì—…ë°ì´íŠ¸ (1=ì›” ~ 7=ì¼)
        # if 'week_code' in study_columns_list:
        #     idx = study_columns_list.index('week_code')
        #     next_row[idx] = min(day_of_week + 1, 6)
        
        # is_weekend: ì£¼ë§ ì—¬ë¶€ (í† , ì¼ = 1)
        if 'is_weekend' in study_columns_list:
            idx = study_columns_list.index('is_weekend')
            next_row[idx] = 1 if day_of_week >= 5 else 0
        
        # is_workday: í‰ì¼ ì—¬ë¶€ (ì›”~ê¸ˆ = 1)
        if 'is_workday' in study_columns_list:
            idx = study_columns_list.index('is_workday')
            next_row[idx] = 1 if day_of_week < 5 else 0
        
        # day_sin, day_cos: ìš”ì¼ì˜ ìˆœí™˜ì  íŠ¹ì„± ì¸ì½”ë”©
        # (7ì¼ ì£¼ê¸°ë¥¼ ì›í˜•ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ ì›”ìš”ì¼ê³¼ ì¼ìš”ì¼ì˜ ì—°ì†ì„± ë°˜ì˜)
        if 'day_sin' in study_columns_list:
            idx = study_columns_list.index('day_sin')
            next_row[idx] = np.sin(2 * np.pi * day_of_week / 7)
        
        if 'day_cos' in study_columns_list:
            idx = study_columns_list.index('day_cos')
            next_row[idx] = np.cos(2 * np.pi * day_of_week / 7)
        
        return next_row
    
    # ========================================================================
    # ë©”ì¸ ì˜ˆì¸¡ ë¡œì§ ì‹œì‘
    # ========================================================================
    try:
        print(f"\n{'='*80}")
        print(f"ğŸ”® í‰ì¼/íœ´ì¼ ê¸°ë°˜ ì˜ˆì¸¡ ({future_steps}ê°œ ìŠ¤í… = {future_steps//96}ì¼)")
        print(f"{'='*80}")
        
        # ì„¤ì • ì •ë³´ ì¶”ì¶œ
        dateColumn = config['dateColumn']
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        seq_len = int(config['r_seqLen'])
        r_predDays = int(config.get('r_predDays', 1))
        
        # ì»¬ëŸ¼ ì •ë³´ ì¤€ë¹„
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        target_idx = study_columns_list.index(targetColumn)
        
        # ë°ì´í„° ì¤€ë¹„
        data_for_prediction = data[study_columns_list].astype(float)
        dates = pd.to_datetime(data[dateColumn])
        last_date = dates.iloc[-1]  # ë§ˆì§€ë§‰ ì•Œë ¤ì§„ ë‚ ì§œ
        
        # í‰ì¼/íœ´ì¼ íŒ¨í„´ í•™ìŠµ
        patterns, weekday_details = calculate_workday_holiday_patterns(
            data_for_prediction, dates, targetColumn
        )
        
        # ê³¼ê±° ë°ì´í„° í†µê³„ (ê²€ì¦ì—ì„œ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš° ê³„ì‚°)
        if historical_mean is None:
            historical_mean = data_for_prediction[targetColumn].mean()
        if historical_std is None:
            historical_std = data_for_prediction[targetColumn].std()
        
        # í•™ìŠµëœ íŒ¨í„´ ì¶œë ¥
        # í•™ìŠµëœ íŒ¨í„´ ì¶œë ¥ (ë©”ì¸ ì˜ˆì¸¡ ë¡œì§ ë‚´)
        print(f"\n   ğŸ“Š í•™ìŠµëœ íŒ¨í„´:")
        print(f"      ğŸ¢ í‰ì¼ (ì›”~ê¸ˆ):")
        if patterns['workday']['count'] > 0:
            print(f"         - í‰ê· : {patterns['workday']['mean']:6.2f} kWh (Â±{patterns['workday']['std']:5.2f})")
            print(f"         - ë²”ìœ„: [{patterns['workday']['min']:.2f}, {patterns['workday']['max']:.2f}]")
            print(f"         - 0ê°’ ë¹„ìœ¨: {patterns['workday']['zero_ratio']*100:4.1f}%")
            print(f"         - ë°ì´í„° ìˆ˜: {patterns['workday']['count']:,}ê°œ")
        else:
            print(f"         âš ï¸  ë°ì´í„° ì—†ìŒ (ê¸°ë³¸ê°’ ì‚¬ìš©)")

        print(f"\n      ğŸ–ï¸ íœ´ì¼ (í† , ì¼):")
        if patterns['holiday']['count'] > 0:
            print(f"         - í‰ê· : {patterns['holiday']['mean']:6.2f} kWh (Â±{patterns['holiday']['std']:5.2f})")
            print(f"         - ë²”ìœ„: [{patterns['holiday']['min']:.2f}, {patterns['holiday']['max']:.2f}]")
            print(f"         - 0ê°’ ë¹„ìœ¨: {patterns['holiday']['zero_ratio']*100:4.1f}%")
            print(f"         - ë°ì´í„° ìˆ˜: {patterns['holiday']['count']:,}ê°œ")
        else:
            print(f"         âš ï¸  ë°ì´í„° ì—†ìŒ (ê¸°ë³¸ê°’ ì‚¬ìš©)")

        print(f"\n   ğŸ“… ìš”ì¼ë³„ ìƒì„¸:")
        for day_idx in range(7):
            if day_idx in weekday_details:
                detail = weekday_details[day_idx]
                icon = "ğŸ¢" if detail["is_workday"] else "ğŸ–ï¸"
                print(f"      {icon} {detail['name']}ìš”ì¼: {detail['mean']:6.2f} kWh "
                    f"(Â±{detail['std']:5.2f}) | 0ê°’: {detail['zero_ratio']*100:4.1f}%")
        
        print(f"\n   ğŸ”„ ì—­ì •ê·œí™”: ì „ì²´ í”¼ì²˜ ë²¡í„° ë°©ì‹")
        print(f"   âœ… ì•ˆì •í™”: 5Ïƒ ë²”ìœ„ (ê·¹ë‹¨ì  ì´ìƒì¹˜ë§Œ ì œê±°)")
        
        # ë°ì´í„° ì •ê·œí™”
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            data_scaled = scaler.transform(data_for_prediction)
        
        # ì´ˆê¸° ì‹œí€€ìŠ¤ êµ¬ì„± (ë§ˆì§€ë§‰ seq_len ê¸¸ì´ì˜ ë°ì´í„°)
        current_sequence = data_scaled[-seq_len:, :].copy()
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        future_predictions = []
        future_dates = []
        stabilization_log = []  # ì•ˆì •í™” ì ìš© ì´ë ¥
        max_log = 10  # ë¡œê·¸ ìµœëŒ€ ê°œìˆ˜
        
        # ====================================================================
        # ë°˜ë³µì  ì˜ˆì¸¡ ë£¨í”„ (future_steps ë§Œí¼ ë°˜ë³µ)
        # ====================================================================
        for step in range(future_steps):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # 1) ëª¨ë¸ ì˜ˆì¸¡ (ì •ê·œí™”ëœ ê°’)
                X = current_sequence.reshape(1, seq_len, -1)
                pred_scaled = model.predict(X, verbose=0)[0, 0]
                
                # 2) ì—­ì •ê·œí™” (ì „ì²´ í”¼ì²˜ ë²¡í„° ë°©ì‹)
                full_scaled = current_sequence[-1].copy()
                full_scaled[target_idx] = pred_scaled
                full_original = scaler.inverse_transform(full_scaled.reshape(1, -1))[0]
                pred_original = float(full_original[target_idx])
            
            # 3) ë‹¤ìŒ ì‹œì ì˜ ë‚ ì§œ ê³„ì‚°
            next_date = last_date + timedelta(minutes=15 * (step + 1))
            
            # 4) ì•ˆì •í™” ì ìš© (ê·¹ë‹¨ì  ì´ìƒì¹˜ë§Œ ì œê±°)
            pred_original, stabilized, reason, weekday_name, day_type, icon = adaptive_stabilization(
                pred_original, next_date, patterns
            )
            
            # 5) ì•ˆì •í™” ë¡œê·¸ ê¸°ë¡ (ì²˜ìŒ 10ê±´ë§Œ)
            if stabilized and len(stabilization_log) < max_log:
                stabilization_log.append({
                    "step": step,
                    "date": next_date.strftime("%m-%d %H:%M"),
                    "weekday": weekday_name,
                    "type": day_type,
                    "icon": icon,
                    "value": pred_original,
                    "reason": reason
                })
            
            # 6) ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            future_predictions.append(pred_original)
            future_dates.append(next_date)
            
            # 7) ë‹¤ìŒ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # ì˜ˆì¸¡ê°’ì„ í¬í•¨í•œ ë‹¤ìŒ í–‰ ìƒì„±
                next_row = data_for_prediction.iloc[-1].copy().values
                next_row[target_idx] = pred_original
                # ì‹œê°„ íŠ¹ì„± ì—…ë°ì´íŠ¸ (ìš”ì¼ ë“±)
                next_row = update_time_features(next_row, next_date, study_columns_list)
                # ì •ê·œí™”
                next_row_scaled = scaler.transform(next_row.reshape(1, -1))[0].astype(np.float32)
                
                # NaN/Inf ì²´í¬ ë° ì•ˆì „ ì²˜ë¦¬
                if np.any(np.isnan(next_row_scaled)) or np.any(np.isinf(next_row_scaled)):
                    next_row_scaled = np.mean(current_sequence[-10:], axis=0)
            
            # ì‹œí€€ìŠ¤ ìŠ¬ë¼ì´ë”© (ë§¨ ì• ì œê±°, ë§¨ ë’¤ ì¶”ê°€)
            current_sequence = np.vstack([current_sequence[1:], next_row_scaled.reshape(1, -1)])
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (1ì¼ ë‹¨ìœ„)
            if (step + 1) % 96 == 0:
                print(f"   â³ {step + 1}/{future_steps} ì™„ë£Œ ({(step+1)//96}ì¼)")
        
        # ì•ˆì •í™” ë¡œê·¸ ì¶œë ¥
        if stabilization_log:
            print(f"\n   âš ï¸  ì•ˆì •í™” ì ìš© ì‚¬ë¡€ (ì´ {len(stabilization_log)}ê±´):")
            for log in stabilization_log[:5]:
                print(f"      {log['icon']} {log['date']} ({log['weekday']}, {log['type']}): "
                      f"{log['value']:.2f} kWh - {log['reason']}")
            if len(stabilization_log) > 5:
                print(f"      ... ì™¸ {len(stabilization_log) - 5}ê±´")
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        future_predictions = np.array(future_predictions)
        
        # ====================================================================
        # ì˜ˆì¸¡ ê²°ê³¼ í†µê³„ ì¶œë ¥
        # ====================================================================
        print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"   - ìµœì†Œ: {np.min(future_predictions):.2f} kWh")
        print(f"   - ìµœëŒ€: {np.max(future_predictions):.2f} kWh")
        print(f"   - í‰ê· : {np.mean(future_predictions):.2f} kWh")
        print(f"   - í‘œì¤€í¸ì°¨: {np.std(future_predictions):.2f} kWh")
        
        # í‰ì¼/íœ´ì¼ë³„ ì˜ˆì¸¡ ë¶„ë¦¬
        workday_predictions = []
        holiday_predictions = []
        
        for pred_val, pred_date in zip(future_predictions, future_dates):
            if pred_date.weekday() < 5:
                workday_predictions.append(pred_val)
            else:
                holiday_predictions.append(pred_val)
        
        # í•™ìŠµ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„° ë¹„êµ
        print(f"\n   ğŸ“… ì˜ˆì¸¡ëœ í‰ì¼/íœ´ì¼ í‰ê·  (vs í•™ìŠµ ë°ì´í„°):")
        
        if workday_predictions:
            pred_workday_avg = np.mean(workday_predictions)
            actual_workday_avg = patterns["workday"]["mean"]
            diff = pred_workday_avg - actual_workday_avg
            diff_pct = (diff / actual_workday_avg * 100) if actual_workday_avg > 0 else 0
            print(f"      ğŸ¢ í‰ì¼: {pred_workday_avg:6.2f} kWh "
                  f"(í•™ìŠµ: {actual_workday_avg:6.2f}, ì°¨ì´: {diff:+6.2f} / {diff_pct:+5.1f}%)")
        
        if holiday_predictions:
            pred_holiday_avg = np.mean(holiday_predictions)
            actual_holiday_avg = patterns["holiday"]["mean"]
            diff = pred_holiday_avg - actual_holiday_avg
            diff_pct = (diff / actual_holiday_avg * 100) if actual_holiday_avg > 0 else 0
            print(f"      ğŸ–ï¸ íœ´ì¼: {pred_holiday_avg:6.2f} kWh "
                  f"(í•™ìŠµ: {actual_holiday_avg:6.2f}, ì°¨ì´: {diff:+6.2f} / {diff_pct:+5.1f}%)")
        
        # ====================================================================
        # ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        # ====================================================================
        predictions_list = []
        for pred_val, pred_date in zip(future_predictions, future_dates):
            predictions_list.append({
                "date": convert_to_serializable(pred_date),
                "predicted_value": convert_to_serializable(pred_val)
            })
        
        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        return {
            "metadata": {
                "model_name": config.get('modelName', 'unknown'),
                "target_column": targetColumn,
                "prediction_steps": future_steps,
                "last_known_date": convert_to_serializable(last_date),
                "method": "ìµœì†Œ ê°œì… ì˜ˆì¸¡ (5Ïƒ ë²”ìœ„ë§Œ ì œí•œ)",
                "historical_mean": historical_mean,
                "historical_std": historical_std,
                "learned_patterns": {
                    "workday": patterns["workday"],
                    "holiday": patterns["holiday"]
                }
            },
            "predictions": predictions_list,
            "statistics": {
                "min_predicted": convert_to_serializable(np.min(future_predictions)),
                "max_predicted": convert_to_serializable(np.max(future_predictions)),
                "mean_predicted": convert_to_serializable(np.mean(future_predictions)),
                "std_predicted": convert_to_serializable(np.std(future_predictions))
            }
        }
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
# ============================================================================

def save_predictions_to_db(prediction_result, target_table="usage_generation_forecast"):
    """
    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    
    Args:
        prediction_result (dict): predict_future_stable()ì˜ ë°˜í™˜ê°’
        target_table (str): ì €ì¥í•  í…Œì´ë¸”ëª… (ê¸°ë³¸: usage_generation_forecast)
    
    Returns:
        tuple: (ì„±ê³µ ê±´ìˆ˜, ì‹¤íŒ¨ ê±´ìˆ˜)
    
    ì €ì¥ í”„ë¡œì„¸ìŠ¤:
        1. ê¸°ì¡´ ë™ì¼ ì‹œì  ë°ì´í„° ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
        2. ìƒˆë¡œìš´ ì˜ˆì¸¡ê°’ ì‚½ì…
        3. íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë¬¶ì–´ì„œ all-or-nothing ë³´ì¥
    
    í…Œì´ë¸” êµ¬ì¡°:
        - time_point: ì˜ˆì¸¡ ì‹œì  (datetime)
        - forecast_usage_kwh: ì˜ˆì¸¡ ì „ë ¥ ì‚¬ìš©ëŸ‰ (float)
        - reg_dt: ë“±ë¡ ì¼ì‹œ (timestamp)
    """
    if prediction_result is None:
        return 0, 0
    
    try:
        engine = get_db_engine()
        predictions = prediction_result.get('predictions', [])
        
        if not predictions:
            return 0, 0
        
        print(f"\nğŸ’¾ DB ì €ì¥ ì‹œì‘...")
        
        success_count = 0
        
        # íŠ¸ëœì­ì…˜ ì‹œì‘
        with engine.connect() as conn:
            trans = conn.begin()

            try:
                conn.execute(text("SET timezone = 'Asia/Seoul'"))
                # ê° ì˜ˆì¸¡ê°’ì— ëŒ€í•´
                for pred in predictions:
                    # 1) ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ë™ì¼ ì‹œì )
                    delete_query = text(f"DELETE FROM carbontwin.{target_table} WHERE time_point = :time_point")
                    conn.execute(delete_query, {"time_point": pred['date']})

                    # 2) ìƒˆë¡œìš´ ì˜ˆì¸¡ê°’ ì‚½ì…
                    insert_query = text(f"""
                    INSERT INTO carbontwin.{target_table} (time_point, forecast_usage_kwh, reg_dt)
                    VALUES (:time_point, :forecast_value, now())
                    """)
                    
                    conn.execute(insert_query, {
                        "time_point": pred['date'],
                        "forecast_value": pred['predicted_value']
                    })
                    
                    success_count += 1
                
                # íŠ¸ëœì­ì…˜ ì»¤ë°‹
                trans.commit()
                print(f"âœ… DB ì €ì¥ ì™„ë£Œ: {success_count}ê±´")
                
            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
                trans.rollback()
                print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                return success_count, len(predictions) - success_count
        
        return success_count, 0
        
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        return 0, len(predictions) if predictions else 0

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main(model_name, tablename, future_steps=672, save_to_db_flag=True, validation_days=1):
    """
    ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    
    Args:
        model_name (str): ëª¨ë¸ëª… (ì˜ˆ: usage-kwh-model-4)
        tablename (str): ë°ì´í„° í…Œì´ë¸”ëª… (ì˜ˆ: lstm_input_15m_new)
        future_steps (int): ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ 672 = 7ì¼)
        save_to_db_flag (bool): DB ì €ì¥ ì—¬ë¶€ (ê¸°ë³¸ True)
        validation_days (int): ê²€ì¦í•  ìµœê·¼ ì¼ìˆ˜ (ê¸°ë³¸ 7ì¼)
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼ì™€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
            - validation: ê²€ì¦ ê²°ê³¼
            - future_prediction: ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼
        None: ì‹¤íŒ¨ ì‹œ
    
    ì‹¤í–‰ ìˆœì„œ:
        1. ëª¨ë¸ ë¡œë“œ (LSTM ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì„¤ì •)
        2. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ
        3. ìµœê·¼ Nì¼ ë°ì´í„°ë¡œ ëª¨ë¸ ê²€ì¦ (ì •í™•ë„ ì¸¡ì •)
        4. ë¯¸ë˜ ì˜ˆì¸¡ ìˆ˜í–‰
        5. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    """
    print("=" * 80)
    print("âš¡ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ (ê°œì„ ëœ ì—­ì •ê·œí™” ë°©ì‹)")
    print("=" * 80)
    
    # 1) ëª¨ë¸ ë¡œë“œ
    model, scaler, config = load_trained_model(model_name)
    if model is None:
        return None
    
    # 2) ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
    new_data = load_new_data(tablename, config['dateColumn'], config['studyColumns'])
    if new_data is None or new_data.empty:
        return None
    
    # ğŸ”¥ ë°ì´í„° ì¶©ë¶„ì„± ì²´í¬
    seq_len = int(config['r_seqLen'])
    r_predDays = int(config.get('r_predDays', 1))
    min_required_for_validation = seq_len + (validation_days * 96) + r_predDays
    min_required_for_prediction = seq_len  # ì˜ˆì¸¡ë§Œ í•˜ë ¤ë©´ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œ ìˆìœ¼ë©´ ë¨
    
    print(f"\nğŸ“ ë°ì´í„° ì²´í¬:")
    print(f"   í˜„ì¬ ë°ì´í„°: {len(new_data)}í–‰")
    print(f"   ì˜ˆì¸¡ ìµœì†Œ ìš”êµ¬: {min_required_for_prediction}í–‰")
    print(f"   ê²€ì¦ ìµœì†Œ ìš”êµ¬: {min_required_for_validation}í–‰")
    
    # ì˜ˆì¸¡ì¡°ì°¨ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
    if len(new_data) < min_required_for_prediction:
        print(f"\nâŒ ë°ì´í„° ë¶€ì¡±: ì˜ˆì¸¡ ë¶ˆê°€")
        print(f"   ìµœì†Œ {min_required_for_prediction}í–‰ í•„ìš” (í˜„ì¬: {len(new_data)}í–‰)")
        return None
    
    # ê²€ì¦ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨
    validation_result = None
    if len(new_data) >= min_required_for_validation:
        print(f"\nâœ… ê²€ì¦ ê°€ëŠ¥ â†’ ê²€ì¦ ìˆ˜í–‰")
        # 3) ëª¨ë¸ ê²€ì¦
        validation_result = validate_with_actual_data(
            model, scaler, config, new_data, validation_days
        )
        
        if validation_result:
            val_accuracy = validation_result['statistics']['accuracy']
            print(f"\nâœ… ê²€ì¦ ì •í™•ë„: {val_accuracy:.2f}%")
    else:
        print(f"\nâš ï¸  ë°ì´í„° ë¶€ì¡±: ê²€ì¦ ê±´ë„ˆë›°ê³  ì˜ˆì¸¡ë§Œ ìˆ˜í–‰")
        print(f"   (ê²€ì¦í•˜ë ¤ë©´ {min_required_for_validation}í–‰ í•„ìš”)")
    
    # 4) ë¯¸ë˜ ì˜ˆì¸¡ ìˆ˜í–‰ (ê²€ì¦ ê²°ê³¼ ìˆìœ¼ë©´ í™œìš©, ì—†ìœ¼ë©´ None)
    print(f"\nğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ì‹œì‘ ({future_steps}ìŠ¤í… = {future_steps//96}ì¼)")
    
    future_result = predict_future_stable(
        model, scaler, config, new_data, future_steps,
        historical_mean=validation_result.get('historical_mean') if validation_result else None,
        historical_std=validation_result.get('historical_std') if validation_result else None
    )
    
    # 5) DB ì €ì¥
    if future_result and save_to_db_flag:
        success, fail = save_predictions_to_db(future_result)
        if success > 0:
            print(f"\nâœ… {success}ê±´ ì €ì¥")
        if fail > 0:
            print(f"âš ï¸  {fail}ê±´ ì €ì¥ ì‹¤íŒ¨")
    
    print(f"\n{'='*80}")
    print("ğŸ‰ ì™„ë£Œ!")
    print("="*80)
    
    return {
        "validation": validation_result,
        "future_prediction": future_result
    }

# ============================================================================
# ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ
# ============================================================================

if __name__ == "__main__":
    try:
        # ì‹¤í–‰ ì„¤ì •
        model_name = "usage_kwh_model"  # ì‚¬ìš©í•  ëª¨ë¸ëª…
        tablename = "lstm_input_15m"   # ë°ì´í„° í…Œì´ë¸”ëª…
        
        print("\n" + "=" * 80)
        print("âš¡ ê°œì„ ëœ ì—­ì •ê·œí™” ë°©ì‹ ì ìš©")
        print("=" * 80)
        
        # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
        result = main(
            model_name=model_name,
            tablename=tablename,
            future_steps=672,      # 7ì¼ ì˜ˆì¸¡ (96 * 7)
            save_to_db_flag=True,  # DB ì €ì¥ í™œì„±í™”
            validation_days=1      # ê²€ì¦ ì¼ìˆ˜ (ë°ì´í„° ë¶€ì¡± ì‹œ ìë™ ìŠ¤í‚µ)
        )
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        if result:
            if result.get('validation'):
                val_stats = result['validation']['statistics']
                print(f"\n{'='*80}")
                print(f"ğŸ“Š ìµœì¢… ìš”ì•½")
                print(f"{'='*80}")
                print(f"   ì •í™•ë„: {val_stats['accuracy']:.2f}%")
                print(f"   MAPE:   {val_stats['mape']:.2f}%")
                print(f"{'='*80}")
            elif result.get('future_prediction'):
                print(f"\n{'='*80}")
                print(f"ğŸ“Š ìµœì¢… ìš”ì•½ (ê²€ì¦ ì—†ìŒ)")
                print(f"{'='*80}")
                stats = result['future_prediction']['statistics']
                print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {stats['min_predicted']:.2f} ~ {stats['max_predicted']:.2f} kWh")
                print(f"   ì˜ˆì¸¡ê°’ í‰ê· : {stats['mean_predicted']:.2f} kWh")
                print(f"{'='*80}")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        print(f"\nâŒ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()