# -*- coding: utf-8 -*-
"""
Title   : ì™¸ë¶€ë°ì´í„° ê¸°ë°˜ LSTM ì˜ˆì¸¡ ë° ë©€í‹° ì‹¤í—˜ ìë™í™” ëª¨ë“ˆ (ì˜ˆì¸¡ê°’ JSON ê¸°ë¡ ê¸°ëŠ¥ ì¶”ê°€)
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
"""

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import psycopg2
from sklearn.preprocessing import StandardScaler
import json
import joblib
from sqlalchemy import create_engine
from datetime import datetime

# í™˜ê²½ ì„¤ì •
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"
else:
    root = "/app/webfiles/lstm"

# ê²½ë¡œ ì„¤ì •
graph_path = os.path.abspath(root + "/graphImage")
os.makedirs(graph_path, exist_ok=True)
model_path = os.path.abspath(root + "/saved_models")
os.makedirs(model_path, exist_ok=True)
# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì¶”ê°€
prediction_path = os.path.abspath(root + "/predictions")
os.makedirs(prediction_path, exist_ok=True)

# âœ… PostgreSQL ì—°ê²° í•¨ìˆ˜ (SQLAlchemy ì‚¬ìš©)
def get_db_engine():
    """SQLAlchemy ì—”ì§„ ìƒì„±"""
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    # connection_string = "postgresql://postgres:7926@localhost:5432/postgres"
    return create_engine(connection_string)

# âœ… JSON ì„¤ì • íŒŒì¼ ë¡œë“œ
def load_experiments_config(config_file="experiments.json"):
    """ì‹¤í—˜ ì„¤ì • JSON íŒŒì¼ ë¡œë“œ"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config['experiments']
    except FileNotFoundError:
        print(f"âŒ ì„¤ì • íŒŒì¼ '{config_file}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {config_file}")
        return []

# âœ… ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (07:00~16:45 í•„í„°ë§ ì¶”ê°€)
def load_data_from_db(tablename, dateColumn, studyColumns):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ (07:00~16:45ë§Œ)"""
    try:
        engine = get_db_engine()
        # 
        query = f"""
        SELECT {studyColumns},{dateColumn}
        FROM carbontwin.{tablename}
        WHERE {dateColumn} IS NOT NULL
        ORDER BY {dateColumn} ASC
        """
        
        data = pd.read_sql_query(query, engine)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(data)}í–‰ (07:00~16:45 ë°ì´í„°ë§Œ)")
        
        # âœ… ì‹œê°„ëŒ€ ë¶„í¬ í™•ì¸
        if dateColumn in data.columns and len(data) > 0:
            data[dateColumn] = pd.to_datetime(data[dateColumn])
            hours = data[dateColumn].dt.hour
            print(f"   ğŸ“Š ì‹œê°„ ë²”ìœ„: {hours.min()}ì‹œ ~ {hours.max()}ì‹œ")
            hour_counts = hours.value_counts().sort_index()
            print(f"   ğŸ“Š ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜:")
            for hour, count in hour_counts.items():
                print(f"      {hour:2d}ì‹œ: {count:5d}ê°œ")
        
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        return None

# âœ… NumPy ë°°ì—´ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_numpy_to_json_serializable(obj):
    """NumPy ë°°ì—´ê³¼ íŠ¹ìˆ˜ íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    else:
        return obj

# âœ… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_predictions_to_json(modelName, dates, actual_values, predicted_values, target_column):
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        # ì˜ˆì¸¡ ë°ì´í„° êµ¬ì„± - ê° ì‹œì ë³„ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¹„êµ
        predictions_data = []
        
        for i in range(len(actual_values)):
            prediction_record = {
                "index": i,
                "date": convert_numpy_to_json_serializable(dates.iloc[i] if hasattr(dates, 'iloc') else dates[i]),
                "actual_value": convert_numpy_to_json_serializable(actual_values[i]),
                "predicted_value": convert_numpy_to_json_serializable(predicted_values[i]),
                "difference": convert_numpy_to_json_serializable(predicted_values[i] - actual_values[i]),
                "percentage_error": convert_numpy_to_json_serializable(
                    abs((predicted_values[i] - actual_values[i]) / actual_values[i] * 100) if actual_values[i] != 0 else 0
                )
            }
            predictions_data.append(prediction_record)
        
        prediction_file_path = os.path.join(prediction_path, f"{modelName}_predictions.json")
        
        prediction_summary = {
            "model_name": modelName,
            "target_column": target_column,
            "prediction_count": len(predictions_data),
            "timestamp": datetime.now().isoformat(),
            "statistics": {
                "actual_min": convert_numpy_to_json_serializable(np.min(actual_values)),
                "actual_max": convert_numpy_to_json_serializable(np.max(actual_values)),
                "actual_mean": convert_numpy_to_json_serializable(np.mean(actual_values)),
                "predicted_min": convert_numpy_to_json_serializable(np.min(predicted_values)),
                "predicted_max": convert_numpy_to_json_serializable(np.max(predicted_values)),
                "predicted_mean": convert_numpy_to_json_serializable(np.mean(predicted_values)),
                "mean_absolute_error": convert_numpy_to_json_serializable(np.mean(np.abs(predicted_values - actual_values))),
                "rmse": convert_numpy_to_json_serializable(np.sqrt(np.mean((predicted_values - actual_values) ** 2)))
            },
            "predictions": predictions_data
        }
        
        with open(prediction_file_path, 'w', encoding='utf-8') as f:
            json.dump(prediction_summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {prediction_file_path}")
        return prediction_summary
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ============================================================================
# save_experiment_to_db í•¨ìˆ˜
# ============================================================================
def save_experiment_to_db(result, config, is_new_model):
    """ì‹¤í—˜ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
    try:
        engine = get_db_engine()
        model_name = result.get('modelName')
        
        if is_new_model:
            check_query = f"SELECT model_id FROM carbontwin.lstm_model WHERE model_name = '{model_name}'"
            existing = pd.read_sql_query(check_query, engine)
            
            if existing.empty:
                model_data = {
                    'model_name': model_name,
                    'target_column': config.get('targetColumn'),
                    'date_column': config.get('dateColumn'),
                    'study_columns': config.get('studyColumns'),
                    'epochs': config.get('r_epochs'),
                    'batch_size': config.get('r_batchSize'),
                    'validation_split': config.get('r_validationSplit'),
                    'sequence_length': config.get('r_seqLen'),
                    'prediction_days': config.get('r_predDays'),
                    'created_at': datetime.now()
                }
                
                df_model = pd.DataFrame([model_data])
                df_model.to_sql('lstm_model', engine, schema='carbontwin',
                              if_exists='append', index=False)
                print(f"âœ… ì‹ ê·œ ëª¨ë¸ ë“±ë¡: {model_name}")
            else:
                print(f"â„¹ï¸  ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©: {model_name}")
        
        query = f"SELECT model_id FROM carbontwin.lstm_model WHERE model_name = '{model_name}'"
        model_id = pd.read_sql_query(query, engine).iloc[0]['model_id']
        
        experiment_data = {
            'model_id': model_id,
            'experiment_name': result.get('experiment_name', config.get('name')),
            'accuracy': result.get('accuracy'),
            'mape': result.get('mape'),
            'rmse': result.get('rmse'),
            'r2_score': result.get('r2_score'),
            'model_file_path': os.path.abspath(os.path.join(model_path, f"{model_name}.h5")),
            'training_loss_img_path': os.path.abspath(os.path.join(root, result.get('training_loss_img'))),
            'total_graph_img_path': os.path.abspath(os.path.join(root, result.get('total_graph_img'))),
            'diff_graph_img_path': os.path.abspath(os.path.join(root, result.get('diff_graph_img'))),
            'prediction_file_path': os.path.abspath(os.path.join(root, result.get('prediction_file'))),
            'execution_time_seconds': result.get('execution_time'),
            'status': result.get('status'),
            'config_json': json.dumps(config, ensure_ascii=False),
            'created_at': datetime.now()
        }
        
        df_experiment = pd.DataFrame([experiment_data])
        df_experiment.to_sql('lstm_experiment', engine, schema='carbontwin',
                           if_exists='append', index=False)
        
        print(f"ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ (Model ID: {model_id})")
        return True
        
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def run_single_experiment(experiment_config, experiment_index):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ ë° DB ì €ì¥"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ ì‹¤í—˜ {experiment_index + 1} ì‹œì‘: {experiment_config['name']}")
    print(f"{'='*60}")
    
    # ë°ì´í„° ë¡œë“œ
    data = load_data_from_db(
        experiment_config['tablename'],
        experiment_config['dateColumn'], 
        experiment_config['studyColumns']
    )
    
    if data is None:
        return {"status": "error", "message": "ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"}
    
    # í•™ìŠµ ì‹¤í–‰
    start_time = time.time()
    result = lstmFinance(data, experiment_config)
    end_time = time.time()
    
    result['execution_time'] = round(end_time - start_time, 2)
    result['experiment_name'] = experiment_config['name']
    
    print(f"â±ï¸  ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {result['execution_time']}ì´ˆ")
    
    if result['status'] == 'success':
        print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ê²°ê³¼ ì €ì¥ ì¤‘...")
        save_success = save_experiment_to_db(
            result, 
            experiment_config,
            is_new_model=result.get('is_new_model', False)
        )
        
        if save_success:
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
    
    return result

# âœ… LSTM í•™ìŠµ í•¨ìˆ˜ (ì‹œê°„ í•„í„°ë§ ë°˜ì˜)
def lstmFinance(lstmData, config):
    """LSTM ëª¨ë¸ í•™ìŠµ (ì„¤ì • ê°ì²´ ê¸°ë°˜, ì˜ˆì¸¡ê°’ ì €ì¥ í¬í•¨)"""
    
    if not tf.executing_eagerly():
        tf.config.run_functions_eagerly(True)

    # ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    modelName = config['modelName']
    dateColumn = config['dateColumn']
    studyColumns = config['studyColumns']
    targetColumn = config['targetColumn']
    r_epochs = config['r_epochs']
    r_batchSize = config['r_batchSize']
    r_validationSplit = config['r_validationSplit']
    r_seqLen = config['r_seqLen']
    r_predDays = config['r_predDays']

    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    training_loss_path = os.path.join(graph_path, f"{modelName}_trainingLoss.png")
    total_graph_path = os.path.join(graph_path, f"{modelName}_totalgraph.png")
    diff_graph_path = os.path.join(graph_path, f"{modelName}_diffgraph.png")
    model_file_path = os.path.join(model_path, f"{modelName}.h5")

    stock_data = lstmData
    
    # ë°ì´í„° ê²€ì¦
    if stock_data.empty:
        return {"status": "error", "message": "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
    
    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ì •ë³´:")
    print(f"   - ì´ ë°ì´í„° ìˆ˜: {len(stock_data)}ê°œ")
    
    study_columns_list = [col.strip() for col in studyColumns.split(',')]
    if targetColumn not in study_columns_list:
        return {"status": "error", "message": f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{targetColumn}'ì´ í•™ìŠµ ì»¬ëŸ¼ì— ì—†ìŠµë‹ˆë‹¤."}

    # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
    if dateColumn in stock_data.columns:
        dates = pd.to_datetime(stock_data[dateColumn], errors='coerce')
        
        # âœ… ì‹œê°„ ë²”ìœ„ í™•ì¸
        hours = dates.dt.hour
        print(f"   - ì‹œê°„ ë²”ìœ„: {hours.min()}ì‹œ ~ {hours.max()}ì‹œ")
        print(f"   - ê³ ìœ  ì‹œê°„ëŒ€: {sorted(hours.unique())}")
    else:
        dates = pd.date_range(start='2023-01-01', periods=len(stock_data), freq='15T')
        print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ '{dateColumn}'ì´ ì—†ì–´ì„œ ê°€ìƒ ë‚ ì§œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    
    original_open = stock_data[targetColumn].values
    stock_data_for_training = stock_data[study_columns_list].astype(float)

    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    stock_data_scaled = scaler.fit_transform(stock_data_for_training)

    # âœ… 80/20 split (ê¸°ì¡´ 90/10ì—ì„œ ë³€ê²½)
    split_index = int(len(stock_data_scaled) * 0.8)
    train_data_scaled = stock_data_scaled[:split_index]
    test_data_scaled = stock_data_scaled[split_index:]
    test_dates = dates[split_index:]

    pred_days = int(r_predDays)
    seq_len = int(r_seqLen)
    input_dim = stock_data_for_training.shape[1]
    target_idx = study_columns_list.index(targetColumn)

    # âœ… ë°ì´í„° ì¶©ë¶„ì„± ê²€ì¦
    print(f"\nğŸ” ì‹œí€€ìŠ¤ ìƒì„± ê²€ì¦:")
    print(f"   - ì „ì²´ ë°ì´í„°: {len(stock_data_scaled)}ê°œ")
    print(f"   - í•™ìŠµ ë°ì´í„°: {len(train_data_scaled)}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data_scaled)}ê°œ")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´(seq_len): {seq_len}")
    print(f"   - ì˜ˆì¸¡ ì¼ìˆ˜(pred_days): {pred_days}")
    
    min_required = seq_len + pred_days
    print(f"   - í•„ìš”í•œ ìµœì†Œ ë°ì´í„°: {min_required}ê°œ")
    
    if len(train_data_scaled) < min_required:
        error_msg = f"í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(train_data_scaled)}ê°œ (ìµœì†Œ {min_required}ê°œ í•„ìš”)"
        print(f"âŒ {error_msg}")
        return {"status": "error", "message": error_msg}
    
    if len(test_data_scaled) < min_required:
        error_msg = f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ì¡±: {len(test_data_scaled)}ê°œ (ìµœì†Œ {min_required}ê°œ í•„ìš”)"
        print(f"âŒ {error_msg}")
        return {"status": "error", "message": error_msg}

    # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    trainX, trainY, testX, testY = [], [], [], []
    
    train_range = range(seq_len, len(train_data_scaled) - pred_days + 1)
    test_range = range(seq_len, len(test_data_scaled) - pred_days + 1)
    
    print(f"\nğŸ“Š ì‹œí€€ìŠ¤ ìƒì„± ë²”ìœ„:")
    print(f"   - í•™ìŠµ ì‹œí€€ìŠ¤: {len(train_range)}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤: {len(test_range)}ê°œ")
    
    if len(train_range) == 0:
        return {"status": "error", "message": "í•™ìŠµ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    if len(test_range) == 0:
        return {"status": "error", "message": "í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    for i in train_range:
        trainX.append(train_data_scaled[i - seq_len:i, 0:input_dim])
        trainY.append(train_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    for i in test_range:
        testX.append(test_data_scaled[i - seq_len:i, 0:input_dim])
        testY.append(test_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    trainX, trainY = np.array(trainX), np.array(trainY)
    testX, testY = np.array(testX), np.array(testY)

    print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
    print(f"   - trainX: {trainX.shape}, trainY: {trainY.shape}")
    print(f"   - testX: {testX.shape}, testY: {testY.shape}")

    print(f"\nğŸ”„ {modelName} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    is_new_model = False

    # ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    try:
        model = load_model(model_file_path, compile=False)
        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
        print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œë¨")
        is_new_model = False
    except (OSError, IOError):
        print("ğŸ”„ ìƒˆ ëª¨ë¸ ìƒì„± ì¤‘...")
        is_new_model = True

        model = Sequential([
            Input(shape=(trainX.shape[1], trainX.shape[2])),
            LSTM(64, return_sequences=True),
            LSTM(32, return_sequences=False),
            Dense(trainY.shape[1])
        ])

        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')

        class TrainingCallback(Callback):
            def __init__(self, total_epochs, batch_size):
                super().__init__()
                self.total_epochs = total_epochs
                self.batch_size = batch_size
                self.prev_val_loss = None
                
            def on_train_begin(self, logs=None):
                print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - ì´ {self.total_epochs} ì—í¬í¬")
                print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
                
            def on_epoch_begin(self, epoch, logs=None):
                print(f"\nâ³ Epoch {epoch + 1}/{self.total_epochs} ì‹œì‘...")
                
            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                loss = logs.get('loss', 0)
                val_loss = logs.get('val_loss', 0)
                
                progress = (epoch + 1) / self.total_epochs * 100
                bar_length = 30
                filled_length = int(bar_length * (epoch + 1) // self.total_epochs)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                
                print(f"âœ… Epoch {epoch + 1}/{self.total_epochs} [{bar}] {progress:.1f}%")
                print(f"   ğŸ“‰ Loss: {loss:.6f} | Val_Loss: {val_loss:.6f}")
                
                if epoch > 0 and self.prev_val_loss is not None:
                    if val_loss < self.prev_val_loss:
                        print(f"   ğŸ“ˆ ê²€ì¦ ì†ì‹¤ ê°œì„ : {self.prev_val_loss:.6f} â†’ {val_loss:.6f}")
                    elif val_loss > self.prev_val_loss * 1.1:
                        print(f"   âš ï¸  ê²€ì¦ ì†ì‹¤ ì¦ê°€: {self.prev_val_loss:.6f} â†’ {val_loss:.6f}")
                
                self.prev_val_loss = val_loss
                
            def on_train_end(self, logs=None):
                print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")

        history = model.fit(
            trainX, trainY,
            epochs=int(r_epochs),
            batch_size=int(r_batchSize),
            validation_split=float(r_validationSplit),
            verbose=1,
            callbacks=[TrainingCallback(int(r_epochs), int(r_batchSize))]
        )

        model.save(model_file_path)
        print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

        # í•™ìŠµ ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥
        plt.figure(figsize=(12, 4))
        plt.plot(history.history['loss'], label='Training loss')
        plt.plot(history.history['val_loss'], label='Validation loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(f'{modelName} - Training Loss')
        plt.legend()
        plt.savefig(training_loss_path)
        plt.close()

    # ì˜ˆì¸¡ ìˆ˜í–‰
    print(f"\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    print(f"ğŸ“Š ì˜ˆì¸¡í•  ìƒ˜í”Œ ìˆ˜: {len(testX)}")
    
    batch_size_pred = 32
    predictions = []
    total_batches = (len(testX) + batch_size_pred - 1) // batch_size_pred
    
    for i in range(0, len(testX), batch_size_pred):
        batch_end = min(i + batch_size_pred, len(testX))
        batch_data = testX[i:batch_end]
        
        batch_pred = model.predict(batch_data, verbose=0)
        predictions.append(batch_pred)
        
        current_batch = (i // batch_size_pred) + 1
        progress = current_batch / total_batches * 100
        bar_length = 25
        filled_length = int(bar_length * current_batch // total_batches)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        print(f"\râ³ ì˜ˆì¸¡ ì§„í–‰: [{bar}] {progress:.1f}% ({current_batch}/{total_batches} ë°°ì¹˜)", end='', flush=True)
    
    prediction = np.vstack(predictions)
    print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ! ì´ {len(prediction)}ê°œ ìƒ˜í”Œ ì˜ˆì¸¡ë¨")

    # ì˜ˆì¸¡ ê²°ê³¼ ì—­ë³€í™˜
    mean_values_pred = np.repeat(scaler.mean_[np.newaxis, :], prediction.shape[0], axis=0)
    mean_values_pred[:, target_idx] = np.squeeze(prediction)
    y_pred = scaler.inverse_transform(mean_values_pred)[:, target_idx]

    mean_values_testY = np.repeat(scaler.mean_[np.newaxis, :], testY.shape[0], axis=0)
    mean_values_testY[:, target_idx] = np.squeeze(testY)
    testY_original = scaler.inverse_transform(mean_values_testY)[:, target_idx]
    valid_test_dates = test_dates[seq_len : seq_len + len(testY_original)]

    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
    prediction_summary = save_predictions_to_json(
        modelName, 
        valid_test_dates, 
        testY_original, 
        y_pred, 
        targetColumn
    )

    # ì „ì²´ ê·¸ë˜í”„ ì €ì¥
    plt.figure(figsize=(15, 5))
    plt.plot(dates, original_open, color='green', label=f'Original {targetColumn}', alpha=0.7)
    plt.plot(valid_test_dates, testY_original, color='blue', label=f'Actual {targetColumn}')
    plt.plot(valid_test_dates, y_pred, color='red', linestyle='--', label=f'Predicted {targetColumn}')
    plt.xlabel(dateColumn)
    plt.ylabel(f'{targetColumn} Value')
    plt.title(f'{modelName} - Prediction Results')
    plt.legend()
    plt.savefig(total_graph_path)
    plt.close()

    # í™•ëŒ€ ê·¸ë˜í”„ ì €ì¥
    zoom_start = max(0, len(valid_test_dates) - 50)
    plt.figure(figsize=(15, 5))
    plt.plot(valid_test_dates[zoom_start:], testY_original[zoom_start:], color='blue', label=f'Actual {targetColumn}')
    plt.plot(valid_test_dates[zoom_start:], y_pred[zoom_start:], color='red', linestyle='--', label=f'Predicted {targetColumn}')
    plt.xlabel(dateColumn)
    plt.ylabel(f'{targetColumn} Value')
    plt.title(f'{modelName} - Recent Predictions (Last 50 points)')
    plt.legend()
    plt.savefig(diff_graph_path)
    plt.close()

    # ì •í™•ë„ ê³„ì‚°
    print(f"\nğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    
    # âœ… MAPE í•¨ìˆ˜ ë‹¨ìˆœí™” (DBì—ì„œ ì´ë¯¸ ì‹œê°„ í•„í„°ë§ë¨)
    def mean_absolute_percentage_error(y_true, y_pred, valid_test_dates):
        print("valid_test_dates : ", valid_test_dates);
        eps = 9  # ì„ê³„ê°’
        mask = y_true > eps
        
        print(f"\nğŸ“Š MAPE ê³„ì‚° ì •ë³´:")
        print(f"   - ì„ê³„ê°’(eps): {eps}")
        print(f"   - ì „ì²´ ë°ì´í„°: {len(y_true)}ê°œ")
        print(f"   - ì„ê³„ê°’ ì´ˆê³¼ ë°ì´í„°: {np.sum(mask)}ê°œ")
        
        if np.sum(mask) == 0:
            print("   âš ï¸ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 999.0
        
        mape_value = np.mean(np.abs((y_pred[mask] - y_true[mask]) / y_true[mask])) * 100
        print(f"   - ê³„ì‚°ëœ MAPE: {mape_value:.2f}%")
        
        return mape_value

    try:
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        sklearn_available = True
    except ImportError:
        print("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì§€í‘œë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.")
        sklearn_available = False
    
    mape = mean_absolute_percentage_error(testY_original, y_pred, valid_test_dates)
    accuracy = 100 - mape if not np.isnan(mape) else 0
    
    # ì¶”ê°€ ì§€í‘œë“¤
    if sklearn_available:
        mse = mean_squared_error(testY_original, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(testY_original, y_pred)
        r2 = r2_score(testY_original, y_pred)
    else:
        # ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°
        mse = np.mean((testY_original - y_pred) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(testY_original - y_pred))
        
        # RÂ² ìˆ˜ë™ ê³„ì‚°
        ss_res = np.sum((testY_original - y_pred) ** 2)
        ss_tot = np.sum((testY_original - np.mean(testY_original)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
    
    # ë°©í–¥ì„± ì •í™•ë„ (ìƒìŠ¹/í•˜ë½ ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„)
    if len(testY_original) > 1:
        actual_direction = np.diff(testY_original) > 0
        pred_direction = np.diff(y_pred) > 0
        direction_accuracy = np.mean(actual_direction == pred_direction) * 100
    else:
        direction_accuracy = 0
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼:")
    print(f"   ğŸ¯ MAPE: {mape:.2f}%")
    print(f"   ğŸ“ˆ ì •í™•ë„: {accuracy:.2f}%")
    print(f"   ğŸ“ MAE: {mae:.4f}")
    print(f"   ğŸ“ RMSE: {rmse:.4f}")
    print(f"   ğŸ” RÂ² Score: {r2:.4f}")
    print(f"   ğŸ§­ ë°©í–¥ì„± ì •í™•ë„: {direction_accuracy:.2f}%")
    
    # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
    if accuracy >= 90:
        grade = "ğŸ† ìš°ìˆ˜"
    elif accuracy >= 80:
        grade = "ğŸ¥‡ ì–‘í˜¸"
    elif accuracy >= 70:
        grade = "ğŸ¥ˆ ë³´í†µ"
    elif accuracy >= 60:
        grade = "ğŸ¥‰ ê°œì„ í•„ìš”"
    else:
        grade = "âŒ ë¶ˆëŸ‰"
    
    print(f"   ğŸ“Š ì„±ëŠ¥ ë“±ê¸‰: {grade}")
    
    # ì˜ˆì¸¡ ë²”ìœ„ ë¶„ì„
    pred_min, pred_max = np.min(y_pred), np.max(y_pred)
    actual_min, actual_max = np.min(testY_original), np.max(testY_original)
    print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ ë²”ìœ„ ë¶„ì„:")
    print(f"   ì‹¤ì œê°’ ë²”ìœ„: {actual_min:.3f} ~ {actual_max:.3f}")
    print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {pred_min:.3f} ~ {pred_max:.3f}")
    
    # ê³¼/ì†Œì˜ˆì¸¡ ë¶„ì„
    over_predict = np.sum(y_pred > testY_original) / len(y_pred) * 100
    under_predict = 100 - over_predict
    print(f"   ê³¼ì˜ˆì¸¡ ë¹„ìœ¨: {over_predict:.1f}%")
    print(f"   ì†Œì˜ˆì¸¡ ë¹„ìœ¨: {under_predict:.1f}%")

    # ì„¤ì • ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    with open(os.path.join(model_path, f"{modelName}_config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4, ensure_ascii=False)

    joblib.dump(scaler, os.path.join(model_path, f"{modelName}_scaler.pkl"))

    # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°˜í™˜ê°’
    result = {
        "status": "success",
        "modelName": modelName,
        "training_loss_img": f"graphImage/{modelName}_trainingLoss.png",
        "total_graph_img": f"graphImage/{modelName}_totalgraph.png",
        "diff_graph_img": f"graphImage/{modelName}_diffgraph.png",
        "mape": round(mape, 2),
        "accuracy": round(accuracy, 2),
        "mae": round(mae, 4),
        "rmse": round(rmse, 4),
        "r2_score": round(r2, 4),
        "direction_accuracy": round(direction_accuracy, 2),
        "prediction_file": f"predictions/{modelName}_predictions.json",
        "prediction_summary": {
            "total_predictions": len(y_pred),
            "prediction_period": {
                "start_date": convert_numpy_to_json_serializable(valid_test_dates.iloc[0]) if len(valid_test_dates) > 0 else None,
                "end_date": convert_numpy_to_json_serializable(valid_test_dates.iloc[-1]) if len(valid_test_dates) > 0 else None
            },
            "value_statistics": {
                "actual_min": convert_numpy_to_json_serializable(np.min(testY_original)),
                "actual_max": convert_numpy_to_json_serializable(np.max(testY_original)),
                "actual_mean": convert_numpy_to_json_serializable(np.mean(testY_original)),
                "predicted_min": convert_numpy_to_json_serializable(np.min(y_pred)),
                "predicted_max": convert_numpy_to_json_serializable(np.max(y_pred)),
                "predicted_mean": convert_numpy_to_json_serializable(np.mean(y_pred))
            }
        }
    }
    
    # ìµœê·¼ Nê°œ ì˜ˆì¸¡ê°’ì„ ì§ì ‘ ê²°ê³¼ì— í¬í•¨
    recent_predictions_count = min(10, len(y_pred))
    if recent_predictions_count > 0:
        result["recent_predictions"] = []
        for i in range(-recent_predictions_count, 0):
            result["recent_predictions"].append({
                "date": convert_numpy_to_json_serializable(valid_test_dates.iloc[i]),
                "actual": convert_numpy_to_json_serializable(testY_original[i]),
                "predicted": convert_numpy_to_json_serializable(y_pred[i]),
                "error": convert_numpy_to_json_serializable(abs(y_pred[i] - testY_original[i]))
            })

    result['is_new_model'] = is_new_model
    return result

# âœ… ë©€í‹° ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
def run_multiple_experiments(config_file="experiments.json"):
    """ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (ì˜ˆì¸¡ê°’ í¬í•¨)"""
    experiments = load_experiments_config(config_file)
    
    if not experiments:
        print("âŒ ì‹¤í–‰í•  ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ”¬ ì´ {len(experiments)}ê°œì˜ ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = []
    total_start_time = time.time()
    
    for i, experiment in enumerate(experiments):
        try:
            result = run_single_experiment(experiment, i)
            results.append(result)
            
            if result['status'] == 'success':
                print(f"âœ… {experiment['name']} ì™„ë£Œ - ì •í™•ë„: {result['accuracy']:.2f}%")
                print(f"   ğŸ“Š ì˜ˆì¸¡ ë°ì´í„° ìˆ˜: {result['prediction_summary']['total_predictions']}ê°œ")
            else:
                print(f"âŒ {experiment['name']} ì‹¤íŒ¨: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
        except Exception as e:
            print(f"âŒ {experiment['name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            results.append({"status": "error", "message": str(e), "experiment_name": experiment['name']})
    
    total_end_time = time.time()
    total_time = round(total_end_time - total_start_time, 2)
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_time}ì´ˆ")
    print(f"âœ… ì„±ê³µ: {len([r for r in results if r['status'] == 'success'])}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {len([r for r in results if r['status'] == 'error'])}ê°œ")
    
    # ì„±ê³µí•œ ì‹¤í—˜ë“¤ì˜ ì •í™•ë„ ìˆœìœ„
    successful_results = [r for r in results if r['status'] == 'success']
    if successful_results:
        successful_results.sort(key=lambda x: x['accuracy'], reverse=True)
        print(f"\nğŸ† ì •í™•ë„ ìˆœìœ„:")
        for i, result in enumerate(successful_results, 1):
            print(f"{i}. {result['experiment_name']}: {result['accuracy']:.2f}% (MAPE: {result['mape']:.2f}%)")
            print(f"   ğŸ“ˆ RÂ² Score: {result.get('r2_score', 'N/A')}, ë°©í–¥ì„± ì •í™•ë„: {result.get('direction_accuracy', 'N/A'):.1f}%")
    
    # ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ë° ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í¬í•¨í•œ ì¢…í•© JSON ì €ì¥
    comprehensive_results = {
        "experiment_summary": {
            "total_experiments": len(experiments),
            "successful_experiments": len(successful_results),
            "failed_experiments": len(results) - len(successful_results),
            "total_execution_time_seconds": total_time,
            "start_timestamp": datetime.now().isoformat(),
            "completion_timestamp": datetime.now().isoformat()
        },
        "performance_ranking": [
            {
                "rank": i + 1,
                "experiment_name": result['experiment_name'],
                "model_name": result['modelName'],
                "accuracy": result['accuracy'],
                "mape": result['mape'],
                "mae": result.get('mae', None),
                "rmse": result.get('rmse', None),
                "r2_score": result.get('r2_score', None),
                "direction_accuracy": result.get('direction_accuracy', None),
                "total_predictions": result['prediction_summary']['total_predictions'] if 'prediction_summary' in result else 0
            }
            for i, result in enumerate(successful_results)
        ],
        "detailed_results": results,
        "prediction_files": [
            {
                "experiment_name": result['experiment_name'],
                "model_name": result['modelName'],
                "prediction_file_path": result.get('prediction_file', 'N/A'),
                "prediction_count": result['prediction_summary']['total_predictions'] if 'prediction_summary' in result else 0,
                "recent_predictions": result.get('recent_predictions', [])
            }
            for result in successful_results
        ]
    }
    
    # ì¢…í•© ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    comprehensive_results_file = "comprehensive_experiment_results.json"
    with open(comprehensive_results_file, "w", encoding="utf-8") as f:
        json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=convert_numpy_to_json_serializable)
    
    print(f"\nğŸ’¾ ì¢…í•© ê²°ê³¼ê°€ '{comprehensive_results_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“ ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” 'predictions/' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì˜ˆì¸¡ íŒŒì¼ ëª©ë¡ ì¶œë ¥
    if successful_results:
        print(f"\nğŸ“„ ìƒì„±ëœ ì˜ˆì¸¡ íŒŒì¼ ëª©ë¡:")
        for result in successful_results:
            if 'prediction_file' in result:
                print(f"   - {result['prediction_file']}")
    
    return results

# âœ… ê°œë³„ ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„ í•¨ìˆ˜
def analyze_prediction_file(prediction_file_path):
    """ì €ì¥ëœ ì˜ˆì¸¡ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½ ì •ë³´ ì¶œë ¥"""
    try:
        with open(prediction_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"\nğŸ“Š ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„: {prediction_file_path}")
        print(f"{'='*50}")
        print(f"ëª¨ë¸ëª…: {data['model_name']}")
        print(f"íƒ€ê²Ÿ ì»¬ëŸ¼: {data['target_column']}")
        print(f"ì˜ˆì¸¡ ê°œìˆ˜: {data['prediction_count']}")
        print(f"ìƒì„± ì‹œê°„: {data['timestamp']}")
        
        stats = data['statistics']
        print(f"\nğŸ“ˆ í†µê³„ ì •ë³´:")
        print(f"   ì‹¤ì œê°’ ë²”ìœ„: {stats['actual_min']:.3f} ~ {stats['actual_max']:.3f}")
        print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {stats['predicted_min']:.3f} ~ {stats['predicted_max']:.3f}")
        print(f"   í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {stats['mean_absolute_error']:.4f}")
        print(f"   ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨ (RMSE): {stats['rmse']:.4f}")
        
        # ìµœê·¼ 5ê°œ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        predictions = data['predictions']
        print(f"\nğŸ” ìµœê·¼ 5ê°œ ì˜ˆì¸¡ ê²°ê³¼:")
        for pred in predictions[-5:]:
            error_pct = pred['percentage_error']
            print(f"   {pred['date'][:19]}: ì‹¤ì œ={pred['actual_value']:.3f}, ì˜ˆì¸¡={pred['predicted_value']:.3f}, ì˜¤ì°¨={error_pct:.2f}%")
            
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ============================================================================
# ì¡°íšŒ í•¨ìˆ˜
# ============================================================================
def get_model_history(model_name=None, limit=10):
    """ëª¨ë¸ë³„ ì‹¤í—˜ ì´ë ¥ ì¡°íšŒ"""
    try:
        engine = get_db_engine()
        
        if model_name:
            query = f"""
            SELECT 
                m.model_name,
                m.epochs as model_epochs,
                m.sequence_length,
                e.experiment_id,
                e.experiment_name,
                e.accuracy,
                e.mape,
                e.r2_score,
                e.created_at
            FROM carbontwin.lstm_experiment e
            JOIN carbontwin.lstm_model m ON e.model_id = m.model_id
            WHERE m.model_name = '{model_name}'
            ORDER BY e.created_at DESC
            LIMIT {limit}
            """
        else:
            query = f"""
            SELECT 
                m.model_name,
                m.epochs as model_epochs,
                e.experiment_id,
                e.experiment_name,
                e.accuracy,
                e.mape,
                e.r2_score,
                e.created_at
            FROM carbontwin.lstm_experiment e
            JOIN carbontwin.lstm_model m ON e.model_id = m.model_id
            ORDER BY e.created_at DESC
            LIMIT {limit}
            """
        
        return pd.read_sql_query(query, engine)
        
    except Exception as e:
        print(f"âŒ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return None


def get_best_models(metric='accuracy', top_n=5):
    """ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì¡°íšŒ"""
    try:
        engine = get_db_engine()
        order = 'ASC' if metric in ['mape', 'rmse'] else 'DESC'
        
        query = f"""
        SELECT 
            m.model_name,
            m.epochs,
            m.sequence_length,
            m.prediction_days,
            e.experiment_name,
            e.accuracy,
            e.mape,
            e.rmse,
            e.r2_score,
            e.model_file_path,
            e.prediction_file_path,
            e.created_at
        FROM carbontwin.lstm_experiment e
        JOIN carbontwin.lstm_model m ON e.model_id = m.model_id
        WHERE e.status = 'success'
        ORDER BY e.{metric} {order}
        LIMIT {top_n}
        """
        
        return pd.read_sql_query(query, engine)
        
    except Exception as e:
        print(f"âŒ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return None

# âœ… ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    print("ğŸ§ª LSTM ë©€í‹° ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œ (ì˜ˆì¸¡ê°’ ê¸°ë¡ + DB ì €ì¥)")
    print("=" * 60)
    print("ğŸ“‹ ì´ ì‹œìŠ¤í…œì˜ ê¸°ëŠ¥:")
    print("   1. ì—¬ëŸ¬ LSTM ëª¨ë¸ì„ ìë™ìœ¼ë¡œ í•™ìŠµ ë° í‰ê°€")
    print("   2. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒì„¸í•œ JSON íŒŒì¼ë¡œ ì €ì¥")
    print("   3. ì„±ëŠ¥ ì§€í‘œë³„ ëª¨ë¸ ìˆœìœ„ ìë™ ìƒì„±")
    print("   4. ì‹œê°í™” ê·¸ë˜í”„ ë° ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±")
    print("   5. PostgreSQL DBì— ì‹¤í—˜ ê²°ê³¼ ìë™ ì €ì¥")
    print("   6. 07:00~16:45 ì‹œê°„ëŒ€ ë°ì´í„°ë§Œ ì‚¬ìš© (80/20 split)")
    print("=" * 60)
    
    choice = input("ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\n"
                  "1. ë©€í‹° ì‹¤í—˜ (JSON íŒŒì¼ ê¸°ë°˜)\n"
                  "2. ë‹¨ì¼ ì‹¤í—˜ (ìˆ˜ë™ ì…ë ¥)\n"
                  "3. ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„\n"
                  "4. DBì—ì„œ ëª¨ë¸ ì´ë ¥ ì¡°íšŒ\n"
                  "5. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ\n"
                  "ì„ íƒ (1-5): ").strip()
    
    if choice == "1":
        print("\nğŸ“– ë©€í‹° ì‹¤í—˜ ëª¨ë“œ ì„¤ëª…:")
        print("   - experiments.json íŒŒì¼ì˜ ì„¤ì •ì— ë”°ë¼ ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ ì‹¤í–‰")
        print("   - ê° ì‹¤í—˜ë³„ë¡œ ëª¨ë¸ í•™ìŠµ, ì˜ˆì¸¡, ì„±ëŠ¥ í‰ê°€ë¥¼ ìë™í™”")
        print("   - ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì„±ëŠ¥ ìˆœìœ„í‘œ ìë™ ìƒì„±")
        
        config_file = input("ì„¤ì • íŒŒì¼ëª… (ê¸°ë³¸ê°’: experiments.json): ").strip() or "experiments.json"
        results = run_multiple_experiments(config_file)
        
        if results and any(r['status'] == 'success' for r in results):
            print(f"\nğŸ‰ ëª¨ë“  ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
            print(f"   - comprehensive_experiment_results.json (ì¢…í•© ê²°ê³¼)")
            print(f"   - predictions/ í´ë” (ê°œë³„ ì˜ˆì¸¡ íŒŒì¼ë“¤)")
            print(f"   - graphImage/ í´ë” (ì‹œê°í™” ê·¸ë˜í”„ë“¤)")
            print(f"   - saved_models/ í´ë” (í•™ìŠµëœ ëª¨ë¸ë“¤)")
            
    elif choice == "2":
        print("\nğŸ“– ë‹¨ì¼ ì‹¤í—˜ ëª¨ë“œ ì•ˆë‚´:")
        print("   í˜„ì¬ ë‹¨ì¼ ì‹¤í—˜ì€ JSON ì„¤ì • íŒŒì¼ì„ í†µí•´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        print("   experiments.json íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
        
    elif choice == "3":
        print("\nğŸ“– ì˜ˆì¸¡ íŒŒì¼ ë¶„ì„ ëª¨ë“œ")
        prediction_file = input("ë¶„ì„í•  ì˜ˆì¸¡ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if prediction_file and os.path.exists(prediction_file):
            analyze_prediction_file(prediction_file)
        else:
            print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            if os.path.exists(prediction_path):
                pred_files = [f for f in os.listdir(prediction_path) if f.endswith('_predictions.json')]
                if pred_files:
                    print(f"\nğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ íŒŒì¼ë“¤:")
                    for i, file in enumerate(pred_files, 1):
                        print(f"   {i}. {file}")
                        
    elif choice == "4":
        print("\nğŸ“Š ëª¨ë¸ í•™ìŠµ ì´ë ¥ ì¡°íšŒ")
        model_name = input("ëª¨ë¸ëª… ì…ë ¥ (ì „ì²´: Enter): ").strip() or None
        limit = input("ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ): ").strip() or "10"
        
        history = get_model_history(model_name, int(limit))
        if history is not None and not history.empty:
            print(f"\nğŸ“‹ ì¡°íšŒ ê²°ê³¼ ({len(history)}ê°œ):")
            print(history.to_string(index=False))
        else:
            print("âŒ ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    elif choice == "5":
        print("\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ")
        metric = input("ì •ë ¬ ê¸°ì¤€ (accuracy/mape/r2_score/rmse): ").strip() or "accuracy"
        top_n = input("ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ 5ê°œ): ").strip() or "5"

        best_models = get_best_models(metric, int(top_n))
        if best_models is not None and not best_models.empty:
            print(f"\nğŸ¯ {metric} ê¸°ì¤€ ìƒìœ„ {top_n}ê°œ:")
            print(best_models.to_string(index=False))
        else:
            print("âŒ ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")