# -*- coding: utf-8 -*-
"""
Title   : ì™¸ë¶€ë°ì´í„° ê¸°ë°˜ LSTM ì˜ˆì¸¡ ë° ë©€í‹° ì‹¤í—˜ ìë™í™” ëª¨ë“ˆ (ì˜ˆì¸¡ê°’ JSON ê¸°ë¡ ê¸°ëŠ¥ ì¶”ê°€)
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
"""

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import psycopg2
from sklearn.preprocessing import StandardScaler
import json
import joblib
from sqlalchemy import create_engine
from datetime import datetime

# í™˜ê²½ ì„¤ì •
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"
else:
    root = "/app/webfiles/lstm"

# ê²½ë¡œ ì„¤ì •
graph_path = os.path.abspath(root + "/graphImage")
os.makedirs(graph_path, exist_ok=True)
model_path = os.path.abspath(root + "/saved_models")
os.makedirs(model_path, exist_ok=True)
# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì¶”ê°€
prediction_path = os.path.abspath(root + "/predictions")
os.makedirs(prediction_path, exist_ok=True)

# âœ… PostgreSQL ì—°ê²° í•¨ìˆ˜ (SQLAlchemy ì‚¬ìš©)
def get_db_engine():
    """SQLAlchemy ì—”ì§„ ìƒì„±"""
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    # connection_string = "postgresql://postgres:7926@localhost:5432/postgres"
    return create_engine(connection_string)

# âœ… JSON ì„¤ì • íŒŒì¼ ë¡œë“œ
def load_experiments_config(config_file="experiments.json"):
    """ì‹¤í—˜ ì„¤ì • JSON íŒŒì¼ ë¡œë“œ"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config['experiments']
    except FileNotFoundError:
        print(f"âŒ ì„¤ì • íŒŒì¼ '{config_file}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {config_file}")
        return []

# âœ… ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (07:00~16:45 í•„í„°ë§ ì¶”ê°€)
def load_data_from_db(tablename, dateColumn, studyColumns):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ (07:00~16:45ë§Œ)"""
    try:
        engine = get_db_engine()
        # 
        query = f"""
        SELECT {studyColumns},{dateColumn}
        FROM carbontwin.{tablename}
        WHERE {dateColumn} IS NOT NULL
          AND TO_CHAR({dateColumn}, 'MM-DD') NOT IN (
            '06-01', '07-28', '07-29', '07-30', '07-31'
        )
        ORDER BY {dateColumn} ASC
        """
        # query = f"""
        # SELECT {studyColumns},{dateColumn}
        # FROM carbontwin.{tablename}
        # WHERE {dateColumn} IS NOT NULL
        #   AND TO_CHAR({dateColumn}, 'MM-DD') NOT IN (
        #     '06-01', '07-28', '07-29', '07-30', '07-31'
        # )
        # ORDER BY {dateColumn} ASC
        # """
        
        data = pd.read_sql_query(query, engine)

        # âœ… ì‹œê°„ëŒ€ ë¶„í¬ í™•ì¸
        if dateColumn in data.columns and len(data) > 0:
            data[dateColumn] = pd.to_datetime(data[dateColumn])
            hours = data[dateColumn].dt.hour
            print(f"   ğŸ“Š ì‹œê°„ ë²”ìœ„: {hours.min()}ì‹œ ~ {hours.max()}ì‹œ")
            hour_counts = hours.value_counts().sort_index()
            print(f"   ğŸ“Š ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜:")
            for hour, count in hour_counts.items():
                print(f"      {hour:2d}ì‹œ: {count:5d}ê°œ")
        
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        return None

# âœ… NumPy ë°°ì—´ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_numpy_to_json_serializable(obj):
    """NumPy ë°°ì—´ê³¼ íŠ¹ìˆ˜ íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    else:
        return obj

# âœ… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_predictions_to_json(modelName, dates, actual_values, predicted_values, target_column):
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        # ì˜ˆì¸¡ ë°ì´í„° êµ¬ì„± - ê° ì‹œì ë³„ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¹„êµ
        predictions_data = []
        
        for i in range(len(actual_values)):
            prediction_record = {
                "index": i,
                "date": convert_numpy_to_json_serializable(dates.iloc[i] if hasattr(dates, 'iloc') else dates[i]),
                "actual_value": convert_numpy_to_json_serializable(actual_values[i]),
                "predicted_value": convert_numpy_to_json_serializable(predicted_values[i]),
                "difference": convert_numpy_to_json_serializable(predicted_values[i] - actual_values[i]),
                "percentage_error": convert_numpy_to_json_serializable(
                    abs((predicted_values[i] - actual_values[i]) / actual_values[i] * 100) if actual_values[i] != 0 else 0
                )
            }
            predictions_data.append(prediction_record)
        
        prediction_file_path = os.path.join(prediction_path, f"{modelName}_predictions.json")
        
        prediction_summary = {
            "model_name": modelName,
            "target_column": target_column,
            "prediction_count": len(predictions_data),
            "timestamp": datetime.now().isoformat(),
            "statistics": {
                "actual_min": convert_numpy_to_json_serializable(np.min(actual_values)),
                "actual_max": convert_numpy_to_json_serializable(np.max(actual_values)),
                "actual_mean": convert_numpy_to_json_serializable(np.mean(actual_values)),
                "predicted_min": convert_numpy_to_json_serializable(np.min(predicted_values)),
                "predicted_max": convert_numpy_to_json_serializable(np.max(predicted_values)),
                "predicted_mean": convert_numpy_to_json_serializable(np.mean(predicted_values)),
                "mean_absolute_error": convert_numpy_to_json_serializable(np.mean(np.abs(predicted_values - actual_values))),
                "rmse": convert_numpy_to_json_serializable(np.sqrt(np.mean((predicted_values - actual_values) ** 2)))
            },
            "predictions": predictions_data
        }
        
        with open(prediction_file_path, 'w', encoding='utf-8') as f:
            json.dump(prediction_summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {prediction_file_path}")
        return prediction_summary
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ============================================================================
# save_experiment_to_db í•¨ìˆ˜
# ============================================================================
def save_experiment_to_db(result, config, is_new_model):
    """ì‹¤í—˜ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
    try:
        engine = get_db_engine()
        model_name = result.get('modelName')
        
        if is_new_model:
            check_query = f"SELECT model_id FROM carbontwin.lstm_model WHERE model_name = '{model_name}'"
            existing = pd.read_sql_query(check_query, engine)
            
            if existing.empty:
                model_data = {
                    'model_name': model_name,
                    'target_column': config.get('targetColumn'),
                    'date_column': config.get('dateColumn'),
                    'study_columns': config.get('studyColumns'),
                    'epochs': config.get('r_epochs'),
                    'batch_size': config.get('r_batchSize'),
                    'validation_split': config.get('r_validationSplit'),
                    'sequence_length': config.get('r_seqLen'),
                    'prediction_days': config.get('r_predDays'),
                    'created_at': datetime.now()
                }
                
                df_model = pd.DataFrame([model_data])
                df_model.to_sql('lstm_model', engine, schema='carbontwin',
                              if_exists='append', index=False)
                print(f"âœ… ì‹ ê·œ ëª¨ë¸ ë“±ë¡: {model_name}")
            else:
                print(f"â„¹ï¸  ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©: {model_name}")
        
        query = f"SELECT model_id FROM carbontwin.lstm_model WHERE model_name = '{model_name}'"
        model_id = pd.read_sql_query(query, engine).iloc[0]['model_id']
        
        experiment_data = {
            'model_id': model_id,
            'experiment_name': result.get('experiment_name', config.get('name')),
            'accuracy': result.get('accuracy'),
            'mape': result.get('mape'),
            'rmse': result.get('rmse'),
            'r2_score': result.get('r2_score'),
            'model_file_path': os.path.abspath(os.path.join(model_path, f"{model_name}.h5")),
            'training_loss_img_path': os.path.abspath(os.path.join(root, result.get('training_loss_img'))),
            'total_graph_img_path': os.path.abspath(os.path.join(root, result.get('total_graph_img'))),
            'diff_graph_img_path': os.path.abspath(os.path.join(root, result.get('diff_graph_img'))),
            'prediction_file_path': os.path.abspath(os.path.join(root, result.get('prediction_file'))),
            'execution_time_seconds': result.get('execution_time'),
            'status': result.get('status'),
            'config_json': json.dumps(config, ensure_ascii=False),
            'created_at': datetime.now()
        }
        
        df_experiment = pd.DataFrame([experiment_data])
        df_experiment.to_sql('lstm_experiment', engine, schema='carbontwin',
                           if_exists='append', index=False)
        
        print(f"ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ (Model ID: {model_id})")
        return True
        
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def run_single_experiment(experiment_config, experiment_index):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ ë° DB ì €ì¥"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ ì‹¤í—˜ {experiment_index + 1} ì‹œì‘: {experiment_config['name']}")
    print(f"{'='*60}")
    
    # ë°ì´í„° ë¡œë“œ
    data = load_data_from_db(
        experiment_config['tablename'],
        experiment_config['dateColumn'], 
        experiment_config['studyColumns']
    )
    
    if data is None:
        return {"status": "error", "message": "ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"}
    
    # í•™ìŠµ ì‹¤í–‰
    start_time = time.time()
    result = lstmFinance(data, experiment_config)
    end_time = time.time()
    
    result['execution_time'] = round(end_time - start_time, 2)
    result['experiment_name'] = experiment_config['name']
    
    print(f"â±ï¸  ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {result['execution_time']}ì´ˆ")
    
    if result['status'] == 'success':
        print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ê²°ê³¼ ì €ì¥ ì¤‘...")
        save_success = save_experiment_to_db(
            result, 
            experiment_config,
            is_new_model=result.get('is_new_model', False)
        )
        
        if save_success:
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
    
    return result

# âœ… LSTM í•™ìŠµ í•¨ìˆ˜ (ì‹œê°„ í•„í„°ë§ ë°˜ì˜)
def lstmFinance(lstmData, config):
    """
    LSTM ëª¨ë¸ í•™ìŠµ - ìš”ì¼ë³„ íŒ¨í„´ ê³ ë ¤ ë²„ì „
    
    ì£¼ìš” ê°œì„  ì‚¬í•­:
    - âœ… í‰ì¼/íœ´ì¼ íŒ¨í„´ ìë™ í•™ìŠµ
    - âœ… ìš”ì¼ë³„ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì ìš©
    - âœ… í‰ì¼/íœ´ì¼ë³„ ì„±ëŠ¥ ë¶„ì„
    - âœ… ì‹œê°„ëŒ€ë³„ íŠ¹ì„± ì œê±° (ìš”ì¼ë§Œ ê³ ë ¤)
    """
    
    if not tf.executing_eagerly():
        tf.config.run_functions_eagerly(True)

    # ====================================================================
    # 1ë‹¨ê³„: ì„¤ì • íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    # ====================================================================
    modelName = config['modelName']
    dateColumn = config['dateColumn']
    studyColumns = config['studyColumns']
    targetColumn = config['targetColumn']
    r_epochs = config['r_epochs']
    r_batchSize = config['r_batchSize']
    r_validationSplit = config['r_validationSplit']
    r_seqLen = config['r_seqLen']
    r_predDays = config['r_predDays']

    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    training_loss_path = os.path.join(graph_path, f"{modelName}_trainingLoss.png")
    total_graph_path = os.path.join(graph_path, f"{modelName}_totalgraph.png")
    diff_graph_path = os.path.join(graph_path, f"{modelName}_diffgraph.png")
    weekday_comparison_path = os.path.join(graph_path, f"{modelName}_weekday_comparison.png")
    model_file_path = os.path.join(model_path, f"{modelName}.h5")

    stock_data = lstmData
    
    # ====================================================================
    # 2ë‹¨ê³„: ë°ì´í„° ê²€ì¦
    # ====================================================================
    if stock_data.empty:
        return {"status": "error", "message": "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
    
    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ì •ë³´:")
    print(f"   - ì´ ë°ì´í„° ìˆ˜: {len(stock_data)}ê°œ")
    
    study_columns_list = [col.strip() for col in studyColumns.split(',')]
    if targetColumn not in study_columns_list:
        return {"status": "error", "message": f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{targetColumn}'ì´ í•™ìŠµ ì»¬ëŸ¼ì— ì—†ìŠµë‹ˆë‹¤."}

    # ====================================================================
    # 3ë‹¨ê³„: ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
    # ====================================================================
    if dateColumn in stock_data.columns:
        dates = pd.to_datetime(stock_data[dateColumn], errors='coerce')
    else:
        dates = pd.date_range(start='2023-01-01', periods=len(stock_data), freq='15T')
        print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ '{dateColumn}'ì´ ì—†ì–´ì„œ ê°€ìƒ ë‚ ì§œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    
    # ====================================================================
    # 4ë‹¨ê³„: í‰ì¼/íœ´ì¼ íŒ¨í„´ ë¶„ì„
    # ====================================================================
    print(f"\nğŸ” ìš”ì¼ë³„ ë°ì´í„° íŒ¨í„´ ë¶„ì„...")
    
    target_values = stock_data[targetColumn].values
    weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
    
    # ìš”ì¼ë³„ í†µê³„ ê³„ì‚°
    weekday_stats = {}
    for day_idx in range(7):
        day_mask = dates.dt.weekday == day_idx
        day_values = target_values[day_mask]
        
        if len(day_values) > 0:
            weekday_stats[day_idx] = {
                "name": weekday_names[day_idx],
                "mean": float(np.mean(day_values)),
                "std": float(np.std(day_values)),
                "median": float(np.median(day_values)),
                "count": len(day_values),
                "zero_ratio": float(np.sum(day_values == 0) / len(day_values)),
                "is_workday": day_idx < 5  # ì›”~ê¸ˆ = í‰ì¼
            }
    
    # í‰ì¼/íœ´ì¼ ê·¸ë£¹ í†µê³„
    workday_mask = dates.dt.weekday < 5
    holiday_mask = dates.dt.weekday >= 5
    
    workday_values = target_values[workday_mask]
    holiday_values = target_values[holiday_mask]
    
    workday_mean = np.mean(workday_values)
    workday_std = np.std(workday_values)
    holiday_mean = np.mean(holiday_values)
    holiday_std = np.std(holiday_values)
    
    print(f"\n   ğŸ“Š í‰ì¼/íœ´ì¼ íŒ¨í„´:")
    print(f"      ğŸ¢ í‰ì¼ (ì›”~ê¸ˆ): í‰ê·  {workday_mean:.2f} (Â±{workday_std:.2f}), "
          f"ë°ì´í„° {len(workday_values):,}ê°œ")
    print(f"      ğŸ–ï¸ íœ´ì¼ (í† , ì¼): í‰ê·  {holiday_mean:.2f} (Â±{holiday_std:.2f}), "
          f"ë°ì´í„° {len(holiday_values):,}ê°œ")
    
    print(f"\n   ğŸ“… ìš”ì¼ë³„ ìƒì„¸:")
    for day_idx in range(7):
        if day_idx in weekday_stats:
            stats = weekday_stats[day_idx]
            icon = "ğŸ¢" if stats["is_workday"] else "ğŸ–ï¸"
            print(f"      {icon} {stats['name']}ìš”ì¼: {stats['mean']:6.2f} kWh "
                  f"(Â±{stats['std']:5.2f}) | 0ê°’: {stats['zero_ratio']*100:4.1f}% | "
                  f"ë°ì´í„°: {stats['count']:,}ê°œ")
    
    # ì„¤ì •ì— íŒ¨í„´ ì •ë³´ ì¶”ê°€ (ë‚˜ì¤‘ì— ì˜ˆì¸¡ ì‹œ ì‚¬ìš©)
    config['weekday_patterns'] = {
        'workday': {'mean': workday_mean, 'std': workday_std},
        'holiday': {'mean': holiday_mean, 'std': holiday_std},
        'details': weekday_stats
    }
    
    # ====================================================================
    # 5ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§
    # ====================================================================
    original_open = stock_data[targetColumn].values
    stock_data_for_training = stock_data[study_columns_list].astype(float)

    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    stock_data_scaled = scaler.fit_transform(stock_data_for_training)

    # 80/20 split
    split_index = int(len(stock_data_scaled) * 0.8)
    train_data_scaled = stock_data_scaled[:split_index]
    test_data_scaled = stock_data_scaled[split_index:]
    train_dates = dates[:split_index]
    test_dates = dates[split_index:]

    pred_days = int(r_predDays)
    seq_len = int(r_seqLen)
    input_dim = stock_data_for_training.shape[1]
    target_idx = study_columns_list.index(targetColumn)

    # ====================================================================
    # 6ë‹¨ê³„: ë°ì´í„° ì¶©ë¶„ì„± ê²€ì¦
    # ====================================================================
    print(f"\nğŸ” ì‹œí€€ìŠ¤ ìƒì„± ê²€ì¦:")
    print(f"   - ì „ì²´ ë°ì´í„°: {len(stock_data_scaled)}ê°œ")
    print(f"   - í•™ìŠµ ë°ì´í„°: {len(train_data_scaled)}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data_scaled)}ê°œ")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´(seq_len): {seq_len}")
    print(f"   - ì˜ˆì¸¡ ì¼ìˆ˜(pred_days): {pred_days}")
    
    min_required = seq_len + pred_days
    print(f"   - í•„ìš”í•œ ìµœì†Œ ë°ì´í„°: {min_required}ê°œ")
    
    if len(train_data_scaled) < min_required:
        error_msg = f"í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(train_data_scaled)}ê°œ (ìµœì†Œ {min_required}ê°œ í•„ìš”)"
        print(f"âŒ {error_msg}")
        return {"status": "error", "message": error_msg}
    
    if len(test_data_scaled) < min_required:
        error_msg = f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ì¡±: {len(test_data_scaled)}ê°œ (ìµœì†Œ {min_required}ê°œ í•„ìš”)"
        print(f"âŒ {error_msg}")
        return {"status": "error", "message": error_msg}

    # ====================================================================
    # 7ë‹¨ê³„: ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (ìƒ˜í”Œ ê°€ì¤‘ì¹˜ í¬í•¨)
    # ====================================================================
    trainX, trainY, train_sample_weights = [], [], []
    testX, testY, test_sample_dates = [], [], []
    
    train_range = range(seq_len, len(train_data_scaled) - pred_days + 1)
    test_range = range(seq_len, len(test_data_scaled) - pred_days + 1)
    
    print(f"\nğŸ“Š ì‹œí€€ìŠ¤ ìƒì„± ë²”ìœ„:")
    print(f"   - í•™ìŠµ ì‹œí€€ìŠ¤: {len(train_range)}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤: {len(test_range)}ê°œ")
    
    if len(train_range) == 0:
        return {"status": "error", "message": "í•™ìŠµ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    if len(test_range) == 0:
        return {"status": "error", "message": "í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    # ====================================================================
    # í•™ìŠµ ë°ì´í„° ìƒì„± + ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚°
    # ====================================================================
    print(f"\nâš–ï¸ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
    
    # í‰ì¼/íœ´ì¼ ë¹„ìœ¨ ê³„ì‚°
    workday_count = np.sum(train_dates.dt.weekday < 5)
    holiday_count = len(train_dates) - workday_count
    total_count = len(train_dates)
    
    # ì—­ë¹„ìœ¨ ê°€ì¤‘ì¹˜ (ë°ì´í„°ê°€ ì ì€ ê·¸ë£¹ì— ë†’ì€ ê°€ì¤‘ì¹˜)
    workday_weight = total_count / (2 * workday_count) if workday_count > 0 else 1.0
    holiday_weight = total_count / (2 * holiday_count) if holiday_count > 0 else 1.0
    
    print(f"   - í‰ì¼ ë°ì´í„°: {workday_count:,}ê°œ (ê°€ì¤‘ì¹˜: {workday_weight:.3f})")
    print(f"   - íœ´ì¼ ë°ì´í„°: {holiday_count:,}ê°œ (ê°€ì¤‘ì¹˜: {holiday_weight:.3f})")
    
    for i in train_range:
        trainX.append(train_data_scaled[i - seq_len:i, 0:input_dim])
        trainY.append(train_data_scaled[i + pred_days - 1:i + pred_days, target_idx])
        
        # íƒ€ê²Ÿ ë‚ ì§œì˜ ìš”ì¼ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        target_date_idx = i + pred_days - 1
        if target_date_idx < len(train_dates):
            target_weekday = train_dates.iloc[target_date_idx].weekday()
            # í‰ì¼(0~4) vs íœ´ì¼(5~6)
            if target_weekday < 5:  # í‰ì¼
                train_sample_weights.append(workday_weight)
            else:  # íœ´ì¼
                train_sample_weights.append(holiday_weight)
        else:
            train_sample_weights.append(1.0)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    for i in test_range:
        testX.append(test_data_scaled[i - seq_len:i, 0:input_dim])
        testY.append(test_data_scaled[i + pred_days - 1:i + pred_days, target_idx])
        
        target_date_idx = i + pred_days - 1
        if target_date_idx < len(test_dates):
            test_sample_dates.append(test_dates.iloc[target_date_idx])
        else:
            test_sample_dates.append(test_dates.iloc[-1])

    trainX, trainY = np.array(trainX), np.array(trainY)
    train_sample_weights = np.array(train_sample_weights)
    testX, testY = np.array(testX), np.array(testY)

    print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
    print(f"   - trainX: {trainX.shape}, trainY: {trainY.shape}")
    print(f"   - testX: {testX.shape}, testY: {testY.shape}")
    print(f"   - ìƒ˜í”Œ ê°€ì¤‘ì¹˜: {train_sample_weights.shape}")

    # ====================================================================
    # 8ë‹¨ê³„: ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    # ====================================================================
    print(f"\nğŸ”„ {modelName} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    is_new_model = False

    try:
        model = load_model(model_file_path, compile=False)
        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
        print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œë¨")
        is_new_model = False
    except (OSError, IOError):
        print("ğŸ”„ ìƒˆ ëª¨ë¸ ìƒì„± ì¤‘...")
        is_new_model = True

        model = Sequential([
            Input(shape=(trainX.shape[1], trainX.shape[2])),
            LSTM(64, return_sequences=True),
            LSTM(32, return_sequences=False),
            Dense(trainY.shape[1])
        ])

        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')

        class TrainingCallback(Callback):
            """í•™ìŠµ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±"""
            def __init__(self, total_epochs, batch_size):
                super().__init__()
                self.total_epochs = total_epochs
                self.batch_size = batch_size
                self.prev_val_loss = None
                
            def on_train_begin(self, logs=None):
                print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - ì´ {self.total_epochs} ì—í¬í¬")
                print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
                print(f"âš–ï¸ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì ìš©: í‰ì¼/íœ´ì¼ ê· í˜• í•™ìŠµ")
                
            def on_epoch_begin(self, epoch, logs=None):
                print(f"\nâ³ Epoch {epoch + 1}/{self.total_epochs} ì‹œì‘...")
                
            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                loss = logs.get('loss', 0)
                val_loss = logs.get('val_loss', 0)
                
                progress = (epoch + 1) / self.total_epochs * 100
                bar_length = 30
                filled_length = int(bar_length * (epoch + 1) // self.total_epochs)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                
                print(f"âœ… Epoch {epoch + 1}/{self.total_epochs} [{bar}] {progress:.1f}%")
                print(f"   ğŸ“‰ Loss: {loss:.6f} | Val_Loss: {val_loss:.6f}")
                
                if epoch > 0 and self.prev_val_loss is not None:
                    if val_loss < self.prev_val_loss:
                        print(f"   ğŸ“ˆ ê²€ì¦ ì†ì‹¤ ê°œì„ : {self.prev_val_loss:.6f} â†’ {val_loss:.6f}")
                    elif val_loss > self.prev_val_loss * 1.1:
                        print(f"   âš ï¸  ê²€ì¦ ì†ì‹¤ ì¦ê°€: {self.prev_val_loss:.6f} â†’ {val_loss:.6f}")
                
                self.prev_val_loss = val_loss
                
            def on_train_end(self, logs=None):
                print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")

        # âœ… ìƒ˜í”Œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í•™ìŠµ
        print(f"\nâš–ï¸ í‰ì¼/íœ´ì¼ ê· í˜• í•™ìŠµ ì‹œì‘...")
        history = model.fit(
            trainX, trainY,
            epochs=int(r_epochs),
            batch_size=int(r_batchSize),
            validation_split=float(r_validationSplit),
            sample_weight=train_sample_weights,  # âœ… ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì ìš©!
            verbose=1,
            callbacks=[TrainingCallback(int(r_epochs), int(r_batchSize))]
        )

        model.save(model_file_path)
        print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

        # í•™ìŠµ ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥
        plt.figure(figsize=(12, 4))
        plt.plot(history.history['loss'], label='Training loss')
        plt.plot(history.history['val_loss'], label='Validation loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(f'{modelName} - Training Loss (With Sample Weights)')
        plt.legend()
        plt.savefig(training_loss_path)
        plt.close()

    # ====================================================================
    # 9ë‹¨ê³„: ì˜ˆì¸¡ ìˆ˜í–‰
    # ====================================================================
    print(f"\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    print(f"ğŸ“Š ì˜ˆì¸¡í•  ìƒ˜í”Œ ìˆ˜: {len(testX)}")
    
    batch_size_pred = 32
    predictions = []
    total_batches = (len(testX) + batch_size_pred - 1) // batch_size_pred
    
    for i in range(0, len(testX), batch_size_pred):
        batch_end = min(i + batch_size_pred, len(testX))
        batch_data = testX[i:batch_end]
        
        batch_pred = model.predict(batch_data, verbose=0)
        predictions.append(batch_pred)
        
        current_batch = (i // batch_size_pred) + 1
        progress = current_batch / total_batches * 100
        bar_length = 25
        filled_length = int(bar_length * current_batch // total_batches)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        print(f"\râ³ ì˜ˆì¸¡ ì§„í–‰: [{bar}] {progress:.1f}% ({current_batch}/{total_batches} ë°°ì¹˜)", end='', flush=True)
    
    prediction = np.vstack(predictions)
    print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ! ì´ {len(prediction)}ê°œ ìƒ˜í”Œ ì˜ˆì¸¡ë¨")

    # ====================================================================
    # 10ë‹¨ê³„: ì˜ˆì¸¡ ê²°ê³¼ ì—­ë³€í™˜
    # ====================================================================
    mean_values_pred = np.repeat(scaler.mean_[np.newaxis, :], prediction.shape[0], axis=0)
    mean_values_pred[:, target_idx] = np.squeeze(prediction)
    y_pred = scaler.inverse_transform(mean_values_pred)[:, target_idx]

    mean_values_testY = np.repeat(scaler.mean_[np.newaxis, :], testY.shape[0], axis=0)
    mean_values_testY[:, target_idx] = np.squeeze(testY)
    testY_original = scaler.inverse_transform(mean_values_testY)[:, target_idx]
    
    valid_test_dates = test_dates[seq_len : seq_len + len(testY_original)]

    # ====================================================================
    # 11ë‹¨ê³„: í‰ì¼/íœ´ì¼ë³„ ì„±ëŠ¥ ë¶„ì„
    # ====================================================================
    print(f"\nğŸ“Š í‰ì¼/íœ´ì¼ë³„ ì„±ëŠ¥ ë¶„ì„...")
    
    # ì˜ˆì¸¡ ë‚ ì§œì˜ ìš”ì¼ í™•ì¸
    test_weekdays = pd.Series([d.weekday() for d in test_sample_dates[:len(testY_original)]])
    workday_mask_test = test_weekdays < 5
    holiday_mask_test = test_weekdays >= 5
    
    # í‰ì¼ ì„±ëŠ¥
    workday_actual = testY_original[workday_mask_test]
    workday_pred = y_pred[workday_mask_test]
    
    # íœ´ì¼ ì„±ëŠ¥
    holiday_actual = testY_original[holiday_mask_test]
    holiday_pred = y_pred[holiday_mask_test]
    
    print(f"\n   ğŸ¢ í‰ì¼ ì˜ˆì¸¡ ì„±ëŠ¥:")
    if len(workday_actual) > 0:
        workday_mae = np.mean(np.abs(workday_pred - workday_actual))
        workday_rmse = np.sqrt(np.mean((workday_pred - workday_actual) ** 2))
        workday_mape = np.mean(np.abs((workday_pred - workday_actual) / workday_actual)) * 100
        print(f"      - ìƒ˜í”Œ ìˆ˜: {len(workday_actual)}ê°œ")
        print(f"      - MAE: {workday_mae:.4f}")
        print(f"      - RMSE: {workday_rmse:.4f}")
        print(f"      - MAPE: {workday_mape:.2f}%")
        print(f"      - ì‹¤ì œ í‰ê· : {np.mean(workday_actual):.2f}")
        print(f"      - ì˜ˆì¸¡ í‰ê· : {np.mean(workday_pred):.2f}")
    
    print(f"\n   ğŸ–ï¸ íœ´ì¼ ì˜ˆì¸¡ ì„±ëŠ¥:")
    if len(holiday_actual) > 0:
        holiday_mae = np.mean(np.abs(holiday_pred - holiday_actual))
        holiday_rmse = np.sqrt(np.mean((holiday_pred - holiday_actual) ** 2))
        # 0 ë°©ì§€
        valid_holiday_mask = holiday_actual > 1
        if np.sum(valid_holiday_mask) > 0:
            holiday_mape = np.mean(np.abs((holiday_pred[valid_holiday_mask] - holiday_actual[valid_holiday_mask]) / holiday_actual[valid_holiday_mask])) * 100
        else:
            holiday_mape = 999.0
        
        print(f"      - ìƒ˜í”Œ ìˆ˜: {len(holiday_actual)}ê°œ")
        print(f"      - MAE: {holiday_mae:.4f}")
        print(f"      - RMSE: {holiday_rmse:.4f}")
        print(f"      - MAPE: {holiday_mape:.2f}%")
        print(f"      - ì‹¤ì œ í‰ê· : {np.mean(holiday_actual):.2f}")
        print(f"      - ì˜ˆì¸¡ í‰ê· : {np.mean(holiday_pred):.2f}")

    # ====================================================================
    # 12ë‹¨ê³„: ìš”ì¼ë³„ ë¹„êµ ê·¸ë˜í”„ ìƒì„±
    # ====================================================================
    print(f"\nğŸ“Š ìš”ì¼ë³„ ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    
    weekday_actual_means = []
    weekday_pred_means = []
    weekday_labels = []
    
    for day_idx in range(7):
        day_mask = test_weekdays == day_idx
        if np.sum(day_mask) > 0:
            day_actual = testY_original[day_mask]
            day_pred = y_pred[day_mask]
            weekday_actual_means.append(np.mean(day_actual))
            weekday_pred_means.append(np.mean(day_pred))
            weekday_labels.append(weekday_names[day_idx])
    
    if len(weekday_labels) > 0:
        plt.figure(figsize=(12, 6))
        x = np.arange(len(weekday_labels))
        width = 0.35
        
        plt.bar(x - width/2, weekday_actual_means, width, label='ì‹¤ì œ í‰ê· ', alpha=0.8)
        plt.bar(x + width/2, weekday_pred_means, width, label='ì˜ˆì¸¡ í‰ê· ', alpha=0.8)
        
        plt.xlabel('ìš”ì¼')
        plt.ylabel(f'{targetColumn} í‰ê· ê°’')
        plt.title(f'{modelName} - ìš”ì¼ë³„ ì‹¤ì œ vs ì˜ˆì¸¡ ë¹„êµ')
        plt.xticks(x, weekday_labels)
        plt.legend()
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        plt.savefig(weekday_comparison_path)
        plt.close()
        print(f"âœ… ìš”ì¼ë³„ ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {weekday_comparison_path}")

    # ====================================================================
    # 13ë‹¨ê³„: ì˜ˆì¸¡ ê²°ê³¼ JSON ì €ì¥
    # ====================================================================
    print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
    prediction_summary = save_predictions_to_json(
        modelName, 
        valid_test_dates, 
        testY_original, 
        y_pred, 
        targetColumn
    )

    # ====================================================================
    # 14ë‹¨ê³„: ì‹œê°í™” ê·¸ë˜í”„ ìƒì„±
    # ====================================================================
    # ì „ì²´ ê·¸ë˜í”„
    plt.figure(figsize=(15, 5))
    plt.plot(dates, original_open, color='green', label=f'Original {targetColumn}', alpha=0.7)
    plt.plot(valid_test_dates, testY_original, color='blue', label=f'Actual {targetColumn}')
    plt.plot(valid_test_dates, y_pred, color='red', linestyle='--', label=f'Predicted {targetColumn}')
    plt.xlabel(dateColumn)
    plt.ylabel(f'{targetColumn} Value')
    plt.title(f'{modelName} - Prediction Results (Weekday-aware)')
    plt.legend()
    plt.savefig(total_graph_path)
    plt.close()

    # í™•ëŒ€ ê·¸ë˜í”„
    zoom_start = max(0, len(valid_test_dates) - 50)
    plt.figure(figsize=(15, 5))
    plt.plot(valid_test_dates[zoom_start:], testY_original[zoom_start:], color='blue', label=f'Actual {targetColumn}')
    plt.plot(valid_test_dates[zoom_start:], y_pred[zoom_start:], color='red', linestyle='--', label=f'Predicted {targetColumn}')
    plt.xlabel(dateColumn)
    plt.ylabel(f'{targetColumn} Value')
    plt.title(f'{modelName} - Recent Predictions (Last 50 points)')
    plt.legend()
    plt.savefig(diff_graph_path)
    plt.close()

    # ====================================================================
    # 15ë‹¨ê³„: ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    # ====================================================================
    print(f"\nğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    
    def mean_absolute_percentage_error(y_true, y_pred, valid_test_dates, eps=9):
        """MAPE ê³„ì‚° í•¨ìˆ˜"""
        mask = y_true > eps
        
        print(f"\nğŸ“Š MAPE ê³„ì‚° ì •ë³´:")
        print(f"   - ì„ê³„ê°’(eps): {eps}")
        print(f"   - ì „ì²´ ë°ì´í„°: {len(y_true)}ê°œ")
        print(f"   - ì„ê³„ê°’ ì´ˆê³¼ ë°ì´í„°: {np.sum(mask)}ê°œ")
        
        if np.sum(mask) == 0:
            print("   âš ï¸ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 999.0
        
        mape_value = np.mean(np.abs((y_pred[mask] - y_true[mask]) / y_true[mask])) * 100
        print(f"   âœ… ê³„ì‚°ëœ MAPE: {mape_value:.2f}%")
        
        return mape_value

    try:
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        sklearn_available = True
    except ImportError:
        print("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì§€í‘œë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.")
        sklearn_available = False
    
    mape = mean_absolute_percentage_error(testY_original, y_pred, valid_test_dates)
    accuracy = 100 - mape if not np.isnan(mape) else 0
    
    if sklearn_available:
        mse = mean_squared_error(testY_original, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(testY_original, y_pred)
        r2 = r2_score(testY_original, y_pred)
    else:
        mse = np.mean((testY_original - y_pred) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(testY_original - y_pred))
        ss_res = np.sum((testY_original - y_pred) ** 2)
        ss_tot = np.sum((testY_original - np.mean(testY_original)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
    
    # ë°©í–¥ì„± ì •í™•ë„
    if len(testY_original) > 1:
        actual_direction = np.diff(testY_original) > 0
        pred_direction = np.diff(y_pred) > 0
        direction_accuracy = np.mean(actual_direction == pred_direction) * 100
    else:
        direction_accuracy = 0
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ì „ì²´ ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   ğŸ¯ MAPE: {mape:.2f}%")
    print(f"   ğŸ“ˆ ì •í™•ë„: {accuracy:.2f}%")
    print(f"   ğŸ“ MAE: {mae:.4f}")
    print(f"   ğŸ“ RMSE: {rmse:.4f}")
    print(f"   ğŸ” RÂ² Score: {r2:.4f}")
    print(f"   ğŸ§­ ë°©í–¥ì„± ì •í™•ë„: {direction_accuracy:.2f}%")
    
    # ì„±ëŠ¥ ë“±ê¸‰
    if accuracy >= 90:
        grade = "ğŸ† ìš°ìˆ˜"
    elif accuracy >= 80:
        grade = "ğŸ¥‡ ì–‘í˜¸"
    elif accuracy >= 70:
        grade = "ğŸ¥ˆ ë³´í†µ"
    elif accuracy >= 60:
        grade = "ğŸ¥‰ ê°œì„ í•„ìš”"
    else:
        grade = "âŒ ë¶ˆëŸ‰"
    
    print(f"   ğŸ“Š ì„±ëŠ¥ ë“±ê¸‰: {grade}")
    
    # ì˜ˆì¸¡ ë²”ìœ„ ë¶„ì„
    pred_min, pred_max = np.min(y_pred), np.max(y_pred)
    actual_min, actual_max = np.min(testY_original), np.max(testY_original)
    print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ ë²”ìœ„ ë¶„ì„:")
    print(f"   ì‹¤ì œê°’ ë²”ìœ„: {actual_min:.3f} ~ {actual_max:.3f}")
    print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {pred_min:.3f} ~ {pred_max:.3f}")
    
    over_predict = np.sum(y_pred > testY_original) / len(y_pred) * 100
    under_predict = 100 - over_predict
    print(f"   ê³¼ì˜ˆì¸¡ ë¹„ìœ¨: {over_predict:.1f}%")
    print(f"   ì†Œì˜ˆì¸¡ ë¹„ìœ¨: {under_predict:.1f}%")

    # ====================================================================
    # 16ë‹¨ê³„: ì„¤ì • ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    # ====================================================================
    with open(os.path.join(model_path, f"{modelName}_config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4, ensure_ascii=False)

    joblib.dump(scaler, os.path.join(model_path, f"{modelName}_scaler.pkl"))

    # ====================================================================
    # 17ë‹¨ê³„: ê²°ê³¼ ë°˜í™˜
    # ====================================================================
    result = {
        "status": "success",
        "modelName": modelName,
        "training_loss_img": f"graphImage/{modelName}_trainingLoss.png",
        "total_graph_img": f"graphImage/{modelName}_totalgraph.png",
        "diff_graph_img": f"graphImage/{modelName}_diffgraph.png",
        "weekday_comparison_img": f"graphImage/{modelName}_weekday_comparison.png",
        "mape": round(mape, 2),
        "accuracy": round(accuracy, 2),
        "mae": round(mae, 4),
        "rmse": round(rmse, 4),
        "r2_score": round(r2, 4),
        "direction_accuracy": round(direction_accuracy, 2),
        "prediction_file": f"predictions/{modelName}_predictions.json",
        "weekday_performance": {
            "workday": {
                "count": int(len(workday_actual)) if len(workday_actual) > 0 else 0,
                "mae": round(float(workday_mae), 4) if len(workday_actual) > 0 else 0,
                "rmse": round(float(workday_rmse), 4) if len(workday_actual) > 0 else 0,
                "mape": round(float(workday_mape), 2) if len(workday_actual) > 0 else 999
            },
            "holiday": {
                "count": int(len(holiday_actual)) if len(holiday_actual) > 0 else 0,
                "mae": round(float(holiday_mae), 4) if len(holiday_actual) > 0 else 0,
                "rmse": round(float(holiday_rmse), 4) if len(holiday_actual) > 0 else 0,
                "mape": round(float(holiday_mape), 2) if len(holiday_actual) > 0 else 999
            }
        },
        "prediction_summary": {
            "total_predictions": len(y_pred),
            "prediction_period": {
                "start_date": convert_numpy_to_json_serializable(valid_test_dates.iloc[0]) if len(valid_test_dates) > 0 else None,
                "end_date": convert_numpy_to_json_serializable(valid_test_dates.iloc[-1]) if len(valid_test_dates) > 0 else None
            },
            "value_statistics": {
                "actual_min": convert_numpy_to_json_serializable(np.min(testY_original)),
                "actual_max": convert_numpy_to_json_serializable(np.max(testY_original)),
                "actual_mean": convert_numpy_to_json_serializable(np.mean(testY_original)),
                "predicted_min": convert_numpy_to_json_serializable(np.min(y_pred)),
                "predicted_max": convert_numpy_to_json_serializable(np.max(y_pred)),
                "predicted_mean": convert_numpy_to_json_serializable(np.mean(y_pred))
            }
        }
    }
    
    # ìµœê·¼ ì˜ˆì¸¡ê°’ ì¶”ê°€
    recent_predictions_count = min(10, len(y_pred))
    if recent_predictions_count > 0:
        result["recent_predictions"] = []
        for i in range(-recent_predictions_count, 0):
            result["recent_predictions"].append({
                "date": convert_numpy_to_json_serializable(valid_test_dates.iloc[i]),
                "actual": convert_numpy_to_json_serializable(testY_original[i]),
                "predicted": convert_numpy_to_json_serializable(y_pred[i]),
                "error": convert_numpy_to_json_serializable(abs(y_pred[i] - testY_original[i]))
            })

    result['is_new_model'] = is_new_model
    
    print(f"\nâœ… í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“Š í‰ì¼/íœ´ì¼ íŒ¨í„´ ì •ë³´ê°€ configì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return result

# âœ… ë©€í‹° ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
def run_multiple_experiments(config_file="experiments.json"):
    """ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (ì˜ˆì¸¡ê°’ í¬í•¨)"""
    experiments = load_experiments_config(config_file)
    
    if not experiments:
        print("âŒ ì‹¤í–‰í•  ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ”¬ ì´ {len(experiments)}ê°œì˜ ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = []
    total_start_time = time.time()
    
    for i, experiment in enumerate(experiments):
        try:
            result = run_single_experiment(experiment, i)
            results.append(result)
            
            if result['status'] == 'success':
                print(f"âœ… {experiment['name']} ì™„ë£Œ - ì •í™•ë„: {result['accuracy']:.2f}%")
                print(f"   ğŸ“Š ì˜ˆì¸¡ ë°ì´í„° ìˆ˜: {result['prediction_summary']['total_predictions']}ê°œ")
            else:
                print(f"âŒ {experiment['name']} ì‹¤íŒ¨: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
        except Exception as e:
            print(f"âŒ {experiment['name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            results.append({"status": "error", "message": str(e), "experiment_name": experiment['name']})
    
    total_end_time = time.time()
    total_time = round(total_end_time - total_start_time, 2)
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_time}ì´ˆ")
    print(f"âœ… ì„±ê³µ: {len([r for r in results if r['status'] == 'success'])}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {len([r for r in results if r['status'] == 'error'])}ê°œ")
    
    # ì„±ê³µí•œ ì‹¤í—˜ë“¤ì˜ ì •í™•ë„ ìˆœìœ„
    successful_results = [r for r in results if r['status'] == 'success']
    if successful_results:
        successful_results.sort(key=lambda x: x['accuracy'], reverse=True)
        print(f"\nğŸ† ì •í™•ë„ ìˆœìœ„:")
        for i, result in enumerate(successful_results, 1):
            print(f"{i}. {result['experiment_name']}: {result['accuracy']:.2f}% (MAPE: {result['mape']:.2f}%)")
            print(f"   ğŸ“ˆ RÂ² Score: {result.get('r2_score', 'N/A')}, ë°©í–¥ì„± ì •í™•ë„: {result.get('direction_accuracy', 'N/A'):.1f}%")
    
    # ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ë° ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í¬í•¨í•œ ì¢…í•© JSON ì €ì¥
    comprehensive_results = {
        "experiment_summary": {
            "total_experiments": len(experiments),
            "successful_experiments": len(successful_results),
            "failed_experiments": len(results) - len(successful_results),
            "total_execution_time_seconds": total_time,
            "start_timestamp": datetime.now().isoformat(),
            "completion_timestamp": datetime.now().isoformat()
        },
        "performance_ranking": [
            {
                "rank": i + 1,
                "experiment_name": result['experiment_name'],
                "model_name": result['modelName'],
                "accuracy": result['accuracy'],
                "mape": result['mape'],
                "mae": result.get('mae', None),
                "rmse": result.get('rmse', None),
                "r2_score": result.get('r2_score', None),
                "direction_accuracy": result.get('direction_accuracy', None),
                "total_predictions": result['prediction_summary']['total_predictions'] if 'prediction_summary' in result else 0
            }
            for i, result in enumerate(successful_results)
        ],
        "detailed_results": results,
        "prediction_files": [
            {
                "experiment_name": result['experiment_name'],
                "model_name": result['modelName'],
                "prediction_file_path": result.get('prediction_file', 'N/A'),
                "prediction_count": result['prediction_summary']['total_predictions'] if 'prediction_summary' in result else 0,
                "recent_predictions": result.get('recent_predictions', [])
            }
            for result in successful_results
        ]
    }
    
    # ì¢…í•© ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    comprehensive_results_file = "comprehensive_experiment_results.json"
    with open(comprehensive_results_file, "w", encoding="utf-8") as f:
        json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=convert_numpy_to_json_serializable)
    
    print(f"\nğŸ’¾ ì¢…í•© ê²°ê³¼ê°€ '{comprehensive_results_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“ ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” 'predictions/' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì˜ˆì¸¡ íŒŒì¼ ëª©ë¡ ì¶œë ¥
    if successful_results:
        print(f"\nğŸ“„ ìƒì„±ëœ ì˜ˆì¸¡ íŒŒì¼ ëª©ë¡:")
        for result in successful_results:
            if 'prediction_file' in result:
                print(f"   - {result['prediction_file']}")
    
    return results


# âœ… ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    print("\nğŸ“– ë©€í‹° ì‹¤í—˜ ëª¨ë“œ ì„¤ëª…:")
    print("   - experiments.json íŒŒì¼ì˜ ì„¤ì •ì— ë”°ë¼ ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ ì‹¤í–‰")
    print("   - ê° ì‹¤í—˜ë³„ë¡œ ëª¨ë¸ í•™ìŠµ, ì˜ˆì¸¡, ì„±ëŠ¥ í‰ê°€ë¥¼ ìë™í™”")
    print("   - ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì„±ëŠ¥ ìˆœìœ„í‘œ ìë™ ìƒì„±")
    
    config_file = input("ì„¤ì • íŒŒì¼ëª… (ê¸°ë³¸ê°’: experiments.json): ").strip() or "experiments.json"
    results = run_multiple_experiments(config_file)
    
    if results and any(r['status'] == 'success' for r in results):
        print(f"\nğŸ‰ ëª¨ë“  ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"   - comprehensive_experiment_results.json (ì¢…í•© ê²°ê³¼)")
        print(f"   - predictions/ í´ë” (ê°œë³„ ì˜ˆì¸¡ íŒŒì¼ë“¤)")
        print(f"   - graphImage/ í´ë” (ì‹œê°í™” ê·¸ë˜í”„ë“¤)")
        print(f"   - saved_models/ í´ë” (í•™ìŠµëœ ëª¨ë¸ë“¤)")
        
    