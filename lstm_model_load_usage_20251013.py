# -*- coding: utf-8 -*-
"""
Title   : EPS ì„ê³„ê°’ í•„í„°ë§ì´ ì ìš©ëœ LSTM ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
Description: 
    - í•™ìŠµëœ LSTM ëª¨ë¸ë¡œ ì‹ ê·œ ë°ì´í„° ì˜ˆì¸¡ ìˆ˜í–‰
    - EPS ì„ê³„ê°’ ê¸°ë°˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ í•„í„°ë§ ì¶”ê°€
    - ë¯¸ë˜ê°’ ì˜ˆì¸¡ ê¸°ëŠ¥ í¬í•¨
    - PostgreSQL DB ì €ì¥ ê¸°ëŠ¥
Version : 2.4
Date    : 2025-10-22
"""

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
import json
import joblib
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta

# -----------------------------------------------------------------------------
# í™˜ê²½ ì„¤ì • ë¸”ë¡
# -----------------------------------------------------------------------------
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    # ê°œë°œ(ë¡œì»¬) í™˜ê²½ì¼ ë•Œì˜ ë£¨íŠ¸ ê²½ë¡œ
    root = "D:/work/lstm"
else:
    # ë°°í¬(ì»¨í…Œì´ë„ˆ ë“±) í™˜ê²½ì¼ ë•Œì˜ ë£¨íŠ¸ ê²½ë¡œ
    root = "/app/webfiles/lstm"

# ëª¨ë¸ê³¼ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥/ë¶ˆëŸ¬ì˜¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ
model_path = os.path.abspath(root + "/saved_models")
prediction_path = os.path.abspath(root + "/predictions")
os.makedirs(prediction_path, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±

# -----------------------------------------------------------------------------
# ğŸ”¥ EPS ì„ê³„ê°’ ì„¤ì • (ì „ì—­ ë³€ìˆ˜)
# -----------------------------------------------------------------------------
# EPS: Very small energy outputsë¥¼ ë¬´ì‹œí•˜ê¸° ìœ„í•œ ì„ê³„ê°’ (kWh ë‹¨ìœ„ ì˜ˆì‹œ)
# í˜„ì¬ëŠ” ë°ì´í„°ì— ì„ê³„ê°’ì„ ì£¼ì§€ ì•Šê³  í•™ìŠµ
# ì„ê³„ê°’ì„ ì£¼ê³ ì‹¶ì€ê²½ìš° PREDICTION_EPS_THRESHOLD ê°’ì„ ì¡°ì ˆ
PREDICTION_EPS_THRESHOLD = 0

# -----------------------------------------------------------------------------
# DB ì—°ê²° í•¨ìˆ˜
# -----------------------------------------------------------------------------
def get_db_engine():
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—”ì§„ ìƒì„±

    ë°˜í™˜:
        sqlalchemy Engine ê°ì²´
    ì£¼ì˜:
        - connection_stringì€ í™˜ê²½ë³„ ë¹„ë°€ë²ˆí˜¸/í˜¸ìŠ¤íŠ¸ì— ë”°ë¼ ìˆ˜ì • í•„ìš”
        - ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ë¹„ë°€ë²ˆí˜¸ë¥¼ ì½”ë“œì— ì§ì ‘ ë‘ì§€ ë§ê³  í™˜ê²½ë³€ìˆ˜/ì‹œí¬ë¦¿ ë§¤ë‹ˆì € ì‚¬ìš© ê¶Œì¥
    """
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    return create_engine(connection_string)

def convert_to_serializable(obj):
    """NumPy ë° Pandasì˜ íŠ¹ìˆ˜ íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜

    ì‚¬ìš©ì²˜:
        - ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•˜ê±°ë‚˜ DBì— ì €ì¥í•  ë•Œ ì§ë ¬í™” ë¬¸ì œ ë°©ì§€
    ì§€ì› íƒ€ì…:
        - np.ndarray -> list
        - np.integer / np.floating -> int / float
        - pandas Timestamp / datetime -> ISO 8601 ë¬¸ìì—´
    """
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    return obj

# -----------------------------------------------------------------------------
# ì‹ ê·œ ë°ì´í„° ë¡œë“œ
# -----------------------------------------------------------------------------
def load_new_data(tablename, dateColumn, studyColumns, start_date=None, end_date=None, days_limit=7):
    """PostgreSQL DBì—ì„œ ì˜ˆì¸¡í•  ì‹ ê·œ ë°ì´í„°ë¥¼ ë¡œë“œ

    íŒŒë¼ë¯¸í„°:
        tablename: DB í…Œì´ë¸” ì´ë¦„ (ì¹´íƒ€ë¡œê·¸ ì—†ì´ í…Œì´ë¸”ëª…ë§Œ)
        dateColumn: ì‹œê°„ ì»¬ëŸ¼ëª… (ì˜ˆ: 'time_point')
        studyColumns: ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ë“¤ì˜ ë¬¸ìì—´ (ì½¤ë§ˆ êµ¬ë¶„)
        start_date, end_date: ê¸°ê°„ í•„í„° (ë¬¸ìì—´ ë˜ëŠ” None)
        days_limit: ì‚¬ìš©ë˜ì§„ ì•Šì§€ë§Œ ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ë‚¨ê²¨ë‘  (í˜¸ì¶œë¶€ í˜¸í™˜ì„± ìœ ì§€)

    ë°˜í™˜ê°’:
        pandas DataFrame (ì„±ê³µ) ë˜ëŠ” None (ì‹¤íŒ¨)
    ì˜ˆì™¸/ì£¼ì˜:
        - ì¿¼ë¦¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ëª…ì€ SQL ì¸ì ì…˜ì— ì·¨ì•½í•  ìˆ˜ ìˆìœ¼ë‹ˆ
          ì™¸ë¶€ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ë„£ëŠ” ê²½ìš° ê²€ì¦ í•„ìš”
        - ë„¤íŠ¸ì›Œí¬/DB ì—°ê²° ì‹¤íŒ¨ ì‹œ None ë¦¬í„´
    """
    try:
        engine = get_db_engine()
        
        if start_date is None and end_date is None:
            # ë‚ ì§œ í•„í„°ê°€ ì—†ì„ ë•Œ: ì „ì²´ ë°ì´í„°(ì •ë ¬ í¬í•¨) ì¡°íšŒ
            query = f"""
            SELECT {studyColumns},{dateColumn}
            FROM carbontwin.{tablename}
            WHERE {dateColumn} IS NOT NULL
            ORDER BY {dateColumn} ASC
            """
        else:
            # start/endê°€ ì§€ì •ëœ ê²½ìš° ì¡°ê±´ ìƒì„±
            where_conditions = [f"{dateColumn} IS NOT NULL"]
            if start_date:
                where_conditions.append(f"{dateColumn} >= '{start_date}'")
            if end_date:
                where_conditions.append(f"{dateColumn} <= '{end_date}'")
            
            query = f"""
            SELECT {studyColumns},{dateColumn}
            FROM carbontwin.{tablename}
            WHERE {' AND '.join(where_conditions)}
            ORDER BY {dateColumn} ASC
            """
        
        # pandasì˜ read_sql_queryë¡œ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜
        data = pd.read_sql_query(query, engine)
        print(f"âœ… ì‹ ê·œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}í–‰")
        
        # ë¡œë“œëœ ë°ì´í„°ì˜ ê¸°ê°„ ì •ë³´ ì¶œë ¥(ë””ë²„ê·¸ ëª©ì )
        if len(data) > 0 and dateColumn in data.columns:
            min_date = pd.to_datetime(data[dateColumn]).min()
            max_date = pd.to_datetime(data[dateColumn]).max()
            print(f"   ğŸ“… ë°ì´í„° ê¸°ê°„: {min_date} ~ {max_date}")
        
        return data
        
    except Exception as e:
        # DB/ì¿¼ë¦¬ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ Noneì„ ë°˜í™˜
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None

# -----------------------------------------------------------------------------
# ëª¨ë¸ ë¡œë“œ
# -----------------------------------------------------------------------------
def load_trained_model(model_name):
    """ì €ì¥ëœ LSTM ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì„¤ì • íŒŒì¼ì„ ë¡œë“œ

    íŒŒì¼ êµ¬ì„±(ê´€ë¡€):
        - ëª¨ë¸: {model_name}.h5
        - ìŠ¤ì¼€ì¼ëŸ¬: {model_name}_scaler.pkl (joblibìœ¼ë¡œ ì €ì¥ëœ sklearn ìŠ¤ì¼€ì¼ëŸ¬)
        - ì„¤ì •: {model_name}_config.json (json í¬ë§·, í•„ìˆ˜ í‚¤: studyColumns, targetColumn, dateColumn, r_seqLen ë“±)

    ë°˜í™˜ê°’:
        (model, scaler, config) ë˜ëŠ” (None, None, None) on error

    ì£¼ì˜:
        - load_modelì—ì„œ compile=Falseë¡œ ë¡œë“œí•œ ë’¤ compile í˜¸ì¶œí•¨(í˜¸í™˜ì„± ë³´ì¥)
        - ìŠ¤ì¼€ì¼ëŸ¬/ì„¤ì • íŒŒì¼ì´ ì—†ìœ¼ë©´ None ë¦¬í„´
    """
    try:
        model_file = os.path.join(model_path, f"{model_name}.h5")
        scaler_file = os.path.join(model_path, f"{model_name}_scaler.pkl")
        config_file = os.path.join(model_path, f"{model_name}_config.json")
        
        if not all(os.path.exists(f) for f in [model_file, scaler_file, config_file]):
            print(f"âŒ í•„ìš”í•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None
        
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        
        # Keras ëª¨ë¸ ë¡œë“œ (ì»´íŒŒì¼ ì˜µì…˜ì€ ë‚˜ì¤‘ì— ì„¤ì •)
        model = load_model(model_file, compile=False)
        model.compile(optimizer='adam', loss='mse')  # ì˜ˆì¸¡ìš©ìœ¼ë¡œ ê¸°ë³¸ ì»´íŒŒì¼
        
        # ìŠ¤ì¼€ì¼ëŸ¬ì™€ config ë¡œë“œ
        scaler = joblib.load(scaler_file)
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼: {config['targetColumn']}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {config['r_seqLen']}")
        print(f"   - EPS ì„ê³„ê°’: {PREDICTION_EPS_THRESHOLD}")
        
        return model, scaler, config
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None, None, None

# -----------------------------------------------------------------------------
# ğŸ”¥ EPS ê¸°ë°˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def analyze_prediction_reliability(predictions, eps_threshold=PREDICTION_EPS_THRESHOLD):
    """
    ì˜ˆì¸¡ê°’ì˜ ì‹ ë¢°ë„ë¥¼ EPS ì„ê³„ê°’ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ 
    â€» í˜„ì¬ëŠ” ì„ê³„ê°’ì„ 0ìœ¼ë¡œ ì„¤ì • ìƒíƒœ

    ì„¤ëª…:
        - predictions ë°°ì—´ì„ eps_thresholdì™€ ë¹„êµí•˜ì—¬ ì‹ ë¢° ê°€ëŠ¥í•œ ì˜ˆì¸¡/ì‹ ë¢° ë¶ˆê°€ ì˜ˆì¸¡ìœ¼ë¡œ ë¶„ë¥˜
        - ì‹ ë¢° ê°€ëŠ¥í•œ ì˜ˆì¸¡ë“¤ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„(min/max/mean/median/std) ê³„ì‚°
        - ì‹ ë¢° ë¶ˆê°€ ì˜ˆì¸¡ë“¤ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„(min/max/mean/median) ê³„ì‚°

    ë°˜í™˜ê°’:
        dict í˜•íƒœì˜ ë¶„ì„ ê²°ê³¼:
            {
                "eps_threshold": eps_threshold,
                "total_predictions": total_count,
                "reliable_predictions": reliable_count,
                "unreliable_predictions": unreliable_count,
                "reliability_ratio": ratio,
                "reliable_indices": [...],
                "unreliable_indices": [...],
                "reliable_statistics": {...} or None,
                "unreliable_statistics": {...} or None
            }

    ì£¼ì˜:
        - total_countê°€ 0ì¼ ê²½ìš° ratioëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬
        - í†µê³„ê°’ì€ floatë¡œ ë³€í™˜í•˜ì—¬ JSON ì‹œë¦¬ì–¼ë¼ì´ì¦ˆê°€ ê°€ëŠ¥í•˜ë„ë¡ í•¨
    """
    predictions = np.array(predictions)
    
    # EPS ì„ê³„ê°’ ê¸°ë°˜ ë¶„ë¥˜ (ì—´ ê¸°ì¤€)
    reliable_mask = predictions >= eps_threshold
    unreliable_mask = predictions < eps_threshold
    
    # ì¸ë±ìŠ¤ ì €ì¥
    reliable_indices = np.where(reliable_mask)[0].tolist()
    unreliable_indices = np.where(unreliable_mask)[0].tolist()
    
    # ê°œìˆ˜ ê³„ì‚°
    reliable_count = len(reliable_indices)
    unreliable_count = len(unreliable_indices)
    total_count = len(predictions)
    
    # ë¹„ìœ¨ ê³„ì‚°
    reliability_ratio = reliable_count / total_count if total_count > 0 else 0
    
    # í†µê³„ ê³„ì‚°
    reliable_stats = None
    if reliable_count > 0:
        reliable_values = predictions[reliable_mask]
        reliable_stats = {
            "min": float(np.min(reliable_values)),
            "max": float(np.max(reliable_values)),
            "mean": float(np.mean(reliable_values)),
            "median": float(np.median(reliable_values)),
            "std": float(np.std(reliable_values))
        }
    
    unreliable_stats = None
    if unreliable_count > 0:
        unreliable_values = predictions[unreliable_mask]
        unreliable_stats = {
            "min": float(np.min(unreliable_values)),
            "max": float(np.max(unreliable_values)),
            "mean": float(np.mean(unreliable_values)),
            "median": float(np.median(unreliable_values))
        }
    
    return {
        "eps_threshold": eps_threshold,
        "total_predictions": int(total_count),
        "reliable_predictions": int(reliable_count),
        "unreliable_predictions": int(unreliable_count),
        "reliability_ratio": float(reliability_ratio),
        "reliable_indices": reliable_indices,
        "unreliable_indices": unreliable_indices,
        "reliable_statistics": reliable_stats,
        "unreliable_statistics": unreliable_stats
    }

# -----------------------------------------------------------------------------
# ë¯¸ë˜ ì˜ˆì¸¡ ìˆ˜í–‰ í•¨ìˆ˜ (EPS ê¸°ë°˜)
# -----------------------------------------------------------------------------
def predict_future_with_eps(model, scaler, config, data, future_steps=96, 
                            eps_threshold=PREDICTION_EPS_THRESHOLD, apply_filter=True):
    """
    EPS ê¸°ë°˜ í•„í„°ë§ì„ ì ìš©í•œ ë¯¸ë˜ê°’ ì˜ˆì¸¡

    ë™ì‘:
        1. ë°ì´í„° ì „ì²˜ë¦¬ (ì‹œê°„ ì»¬ëŸ¼ ì œê±°, ìŠ¤ì¼€ì¼ë§)
        2. rolling window ë°©ì‹ìœ¼ë¡œ future_stepsë§Œí¼ ë°˜ë³µ ì˜ˆì¸¡
        3. analyze_prediction_reliabilityë¡œ EPS ë¶„ì„ ìˆ˜í–‰
        4. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ í¬ë§·í•˜ì—¬ ë°˜í™˜

    íŒŒë¼ë¯¸í„°:
        model: í•™ìŠµëœ LSTM ëª¨ë¸
        scaler: MinMaxScaler ë˜ëŠ” StandardScaler
        config: ëª¨ë¸ ì„¤ì • dict (targetColumn, dateColumn, r_seqLen ë“±)
        data: ì…ë ¥ DataFrame (DBì—ì„œ ë¡œë“œí•œ ê²ƒ)
        future_steps: ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜
        eps_threshold: EPS ì„ê³„ê°’
        apply_filter: ì‹ ë¢°ë„ í•„í„° ì ìš© ì—¬ë¶€

    ë°˜í™˜ê°’:
        dict {
            "metadata": {...},
            "reliability_analysis": {...},
            "predictions": [{"date": ..., "predicted_value": ..., "is_reliable": ...}, ...],
            "statistics": {...}
        } ë˜ëŠ” None (ì˜¤ë¥˜ ë°œìƒ ì‹œ)
    """
    try:
        print(f"\nğŸ”® ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì‹œì‘")
        print(f"   - ì˜ˆì¸¡ ìŠ¤í… ìˆ˜: {future_steps}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {config['r_seqLen']}")
        print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼: {config['targetColumn']}")
        print(f"   - EPS ì„ê³„ê°’: {eps_threshold}")
        print(f"   - í•„í„°ë§ ì ìš©: {'ì˜ˆ' if apply_filter else 'ì•„ë‹ˆì˜¤'}")
        
        # ì‹œê°„ ì»¬ëŸ¼ ì œê±°
        dateColumn = config['dateColumn']
        feature_columns = [col for col in data.columns if col != dateColumn]
        
        # íƒ€ê²Ÿ ì»¬ëŸ¼ ì¸ë±ìŠ¤ í™•ì¸
        target_col = config['targetColumn']
        if target_col not in feature_columns:
            print(f"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        target_idx = feature_columns.index(target_col)
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (ìŠ¤ì¼€ì¼ë§)
        feature_data = data[feature_columns]
        scaled_data = scaler.transform(feature_data)
        
        # ì´ˆê¸° ì‹œí€€ìŠ¤
        seq_len = config['r_seqLen']
        if len(scaled_data) < seq_len:
            print(f"âŒ ë°ì´í„°ê°€ ì‹œí€€ìŠ¤ ê¸¸ì´({seq_len})ë³´ë‹¤ ì§§ìŠµë‹ˆë‹¤.")
            return None
        
        # í˜„ì¬ ì‹œí€€ìŠ¤(rolling window)
        current_sequence = scaled_data[-seq_len:].copy()
        
        # ë¯¸ë˜ ì˜ˆì¸¡ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        future_predictions = []
        
        # ë§ˆì§€ë§‰ ë‚ ì§œ íŒŒì‹±(ì‹œì‘ì )
        last_date = pd.to_datetime(data[dateColumn].iloc[-1])
        time_interval = timedelta(minutes=15)  # 15ë¶„ ê°„ê²© (LSTM ë°ì´í„°ì— ë§ì¶¤)
        
        # rolling windowë¡œ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ë°˜ë³µ
        print(f"   ğŸ”„ ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
        for step in range(future_steps):
            # ëª¨ë¸ ì…ë ¥ í˜•íƒœ: [1, seq_len, features]
            X = current_sequence.reshape(1, seq_len, -1)
            
            # í•œ ìŠ¤í… ì˜ˆì¸¡
            pred_scaled = model.predict(X, verbose=0)
            pred_value = pred_scaled[0, 0]
            
            # ì—­ìŠ¤ì¼€ì¼ë§ (íƒ€ê²Ÿ ì»¬ëŸ¼ë§Œ)
            # ìŠ¤ì¼€ì¼ëŸ¬ê°€ ë‹¤ë³€ëŸ‰ì´ë©´ ê°™ì€ ê¸¸ì´ì˜ dummy ë°°ì—´ ë§Œë“¤ì–´ì„œ ì—­ë³€í™˜
            dummy = np.zeros((1, len(feature_columns)))
            dummy[0, target_idx] = pred_value
            pred_original = scaler.inverse_transform(dummy)[0, target_idx]
            
            future_predictions.append(pred_original)
            
            # ë‹¤ìŒ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
            # í˜„ì¬ ì‹œí€€ìŠ¤ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ í–‰ ì œê±°, ìƒˆë¡œìš´ ì˜ˆì¸¡ì„ ëì— ì¶”ê°€
            new_row = current_sequence[-1].copy()
            new_row[target_idx] = pred_value
            
            current_sequence = np.vstack([current_sequence[1:], new_row])
            
            # ì§„í–‰ ë¡œê·¸
            if (step + 1) % 100 == 0:
                print(f"      â³ {step+1}/{future_steps} ì™„ë£Œ")
        
        print(f"   âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        
        # EPS ê¸°ë°˜ ì‹ ë¢°ë„ ë¶„ì„
        print(f"\nğŸ“Š ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„ ì¤‘...")
        reliability = analyze_prediction_reliability(future_predictions, eps_threshold)
        
        print(f"   - ì „ì²´ ì˜ˆì¸¡: {reliability['total_predictions']}ê±´")
        print(f"   - ì‹ ë¢° ê°€ëŠ¥: {reliability['reliable_predictions']}ê±´")
        print(f"   - ì‹ ë¢° ë¶ˆê°€: {reliability['unreliable_predictions']}ê±´")
        print(f"   - ì‹ ë¢°ìœ¨: {reliability['reliability_ratio']*100:.2f}%")
        
        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ í¬ë§·
        predictions_list = []
        for i, pred_val in enumerate(future_predictions):
            future_date = last_date + time_interval * (i + 1)
            is_reliable = i in reliability['reliable_indices']
            
            predictions_list.append({
                "date": convert_to_serializable(future_date),
                "predicted_value": convert_to_serializable(pred_val),
                "is_reliable": is_reliable
            })
        
        # ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        future_result = {
            "metadata": {
                "model_name": config.get('model_name', 'unknown'),
                "target_column": target_col,
                "sequence_length": seq_len,
                "prediction_steps": future_steps,
                "eps_threshold": eps_threshold,
                "filter_applied": apply_filter,
                "last_known_date": convert_to_serializable(last_date),
                "first_prediction_date": convert_to_serializable(last_date + time_interval),
                "last_prediction_date": convert_to_serializable(last_date + time_interval * future_steps)
            },
            "reliability_analysis": reliability,
            "predictions": predictions_list,
            "statistics": {
                "min_predicted": convert_to_serializable(np.min(future_predictions)),
                "max_predicted": convert_to_serializable(np.max(future_predictions)),
                "mean_predicted": convert_to_serializable(np.mean(future_predictions)),
                "median_predicted": convert_to_serializable(np.median(future_predictions)),
                "std_predicted": convert_to_serializable(np.std(future_predictions))
            }
        }
        
        return future_result
        
    except Exception as e:
        # ì˜ˆì¸¡ ì¤‘ ì˜ˆì™¸ ë°œìƒ ì‹œ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥ í›„ None ë°˜í™˜
        print(f"âŒ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# -----------------------------------------------------------------------------
# ğŸ”¥ EPS í•„í„°ë§ ì ìš©í•œ DB ì €ì¥ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def save_predictions_to_db_with_eps(prediction_result, target_table="usage_generation_forecast", 
                                    only_reliable=False):
    """
    ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ PostgreSQL DBì— ì €ì¥ (EPS í•„í„°ë§ ì˜µì…˜)

    íŒŒë¼ë¯¸í„°:
        prediction_result: predict_future_with_epsì˜ ë°˜í™˜ dict
        target_table: ì €ì¥ ëŒ€ìƒ í…Œì´ë¸”ëª… (carbontwin.<target_table> ì‚¬ìš©)
        only_reliable: Trueì´ë©´ is_reliable == Trueì¸ ì˜ˆì¸¡ë§Œ ì €ì¥

    ë™ì‘:
        - ê¸°ì¡´ ë™ì¼ time_point ë ˆì½”ë“œëŠ” DELETEë¡œ ì œê±°(ì¤‘ë³µ ë°©ì§€)
        - INSERTë¡œ ìƒˆ ë ˆì½”ë“œ ì¶”ê°€ (time_point, forecast_usage_kwh, reg_dt)
        - íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë¬¶ì–´ ì¤‘ê°„ ì˜¤ë¥˜ ì‹œ ë¡¤ë°±

    ë°˜í™˜ê°’:
        (success_count, fail_count)
    ì£¼ì˜:
        - ì‹¤ì œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ(ì¹¼ëŸ¼ëª…)ê°€ ë‹¤ë¥´ë©´ INSERTë¬¸ ìˆ˜ì • í•„ìš”
        - ì‹œê°„ í¬ë§·ì€ ISO8601 ë¬¸ìì—´ë¡œ ì „ë‹¬ë˜ë¯€ë¡œ DBì˜ time_point ì¹¼ëŸ¼ íƒ€ì…ì— ë§ê²Œ ë³€í™˜ë  ê²ƒ
    """
    if prediction_result is None:
        print("âŒ ì €ì¥í•  ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0
    
    try:
        engine = get_db_engine()
        predictions = prediction_result.get('predictions', [])
        
        if not predictions:
            print("âŒ ì˜ˆì¸¡ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return 0, 0
        
        # only_reliable ì˜µì…˜ì— ë”°ë¼ í•„í„°ë§
        if only_reliable:
            predictions = [p for p in predictions if p.get('is_reliable', False)]
            print(f"\nğŸ“Š ì‹ ë¢° ê°€ëŠ¥í•œ ì˜ˆì¸¡ë§Œ ì €ì¥: {len(predictions)}ê±´")
        
        print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ DB ì €ì¥ ì‹œì‘...")
        print(f"   - ëŒ€ìƒ í…Œì´ë¸”: carbontwin.{target_table}")
        print(f"   - ì €ì¥í•  ë°ì´í„°: {len(predictions)}ê±´")
        
        success_count = 0
        fail_count = 0
        
        # DB ì»¤ë„¥ì…˜ê³¼ íŠ¸ëœì­ì…˜ ì²˜ë¦¬
        with engine.connect() as conn:
            trans = conn.begin()
            
            try:
                for pred in predictions:
                    time_point = pred['date']
                    forecast_value = pred['predicted_value']
                    
                    # ì¤‘ë³µ ì œê±°: ë™ì¼ time_pointì¸ ê²½ìš° ì‚­ì œ(ì •ì±…)
                    delete_query = text(f"""
                    DELETE FROM carbontwin.{target_table}
                    WHERE time_point = :time_point
                    """)
                    
                    conn.execute(delete_query, {"time_point": time_point})
                    
                    # ì‚½ì…: forecast_usage_kwh ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
                    insert_query = text(f"""
                    INSERT INTO carbontwin.{target_table} 
                        (time_point, forecast_usage_kwh, reg_dt)
                    VALUES 
                        (:time_point, :forecast_value, CURRENT_TIMESTAMP)
                    """)
                    
                    conn.execute(
                        insert_query,
                        {
                            "time_point": time_point,
                            "forecast_value": forecast_value
                        }
                    )
                    
                    success_count += 1
                    
                    # ëŒ€ëŸ‰ ì‚½ì…ì‹œ ì§„í–‰ ë¡œê·¸ ì¶œë ¥(ë””ë²„ê·¸/ëª¨ë‹ˆí„°ë§)
                    if success_count % 100 == 0:
                        print(f"   â³ ì§„í–‰: {success_count}/{len(predictions)} ê±´")
                
                trans.commit()
                
                print(f"âœ… DB ì €ì¥ ì™„ë£Œ!")
                print(f"   - ì„±ê³µ: {success_count}ê±´")
                
            except Exception as e:
                trans.rollback()
                print(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ (ë¡¤ë°±ë¨): {str(e)}")
                return success_count, len(predictions) - success_count
        
        return success_count, fail_count
        
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        return 0, len(predictions) if predictions else 0

# -----------------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def main(model_name=None, tablename=None, save_to_db=True, only_reliable=False, 
         eps_threshold=PREDICTION_EPS_THRESHOLD, apply_filter=True):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜

    ë™ì‘ ìš”ì•½:
        1. ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/ì„¤ì • ë¡œë“œ
        2. DBì—ì„œ ì‹ ê·œ ë°ì´í„° ë¡œë“œ
        3. predict_future_with_epsë¡œ ë¯¸ë˜ ì˜ˆì¸¡ ìˆ˜í–‰
        4. save_predictions_to_db_with_epsë¡œ DBì— ì €ì¥ (ì˜µì…˜)
        5. ì˜ˆì™¸/ì˜¤ë¥˜ ë°œìƒ ì‹œ ì ì ˆíˆ ë©”ì‹œì§€ ì¶œë ¥

    ë°˜í™˜ê°’:
        predict_future_with_epsê°€ ë°˜í™˜í•œ ê²°ê³¼ dict ë˜ëŠ” None
    """
    print("=" * 70)
    print("ğŸ”® EPS í•„í„°ë§ ì ìš© LSTM ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("=" * 70)
    
    # ëª¨ë¸ ë¡œë“œ
    model, scaler, config = load_trained_model(model_name)
    
    if model is None:
        return None
    
    print(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    # load_new_dataì˜ days_limit íŒŒë¼ë¯¸í„°ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ í˜¸ì¶œ
    new_data = load_new_data(tablename, config['dateColumn'], config['studyColumns'], days_limit=7)
    
    if new_data is None or new_data.empty:
        print("âŒ ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì˜ˆì¸¡ ìŠ¤í… ê¸°ë³¸: 7ì¼
    future_steps = 672  # 7ì¼
    
    print(f"\nğŸ”® ë¯¸ë˜ê°’ ì˜ˆì¸¡ ìˆ˜í–‰")
    print(f"   - ì˜ˆì¸¡ ìŠ¤í…: {future_steps}ê°œ")
    print(f"   - EPS ì„ê³„ê°’: {eps_threshold}")
    print(f"   - í•„í„°ë§ ì ìš©: {'ì˜ˆ' if apply_filter else 'ì•„ë‹ˆì˜¤'}")
    
    # ì‹¤ì œ ì˜ˆì¸¡ í˜¸ì¶œ
    future_result = predict_future_with_eps(
        model, scaler, config, new_data, future_steps,
        eps_threshold, apply_filter
    )
    
    # ì˜ˆì¸¡ ê²°ê³¼ê°€ ìˆê³  DB ì €ì¥ ì˜µì…˜ì´ ì¼œì ¸ ìˆìœ¼ë©´ ì €ì¥ ìˆ˜í–‰
    if future_result and save_to_db:
        success, fail = save_predictions_to_db_with_eps(
            future_result, 
            only_reliable=only_reliable
        )
        
        if success > 0:
            print(f"\nâœ… ì´ {success}ê±´ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if only_reliable:
                print(f"   ğŸ’¡ ì‹ ë¢° ê°€ëŠ¥í•œ ì˜ˆì¸¡ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if fail > 0:
            print(f"âš ï¸  {fail}ê±´ì˜ ì €ì¥ ì‹¤íŒ¨")
    
    print(f"\n{'='*70}")
    print("ğŸ‰ ì˜ˆì¸¡ ì™„ë£Œ!")
    print("="*70)
    
    return future_result

# -----------------------------------------------------------------------------
# í”„ë¡œê·¸ë¨ ì‹œì‘ì 
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    """
    EPS í•„í„°ë§ ì ìš© ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

    ì‚¬ìš©ë²•:
        python lstm_model_load.py

    ì‹¤í–‰ì‹œ ì œê³µë˜ëŠ” ì˜µì…˜:
        - ì‚¬ìš©ìê°€ ì½˜ì†”ì—ì„œ ëª¨ë“œë¥¼ ì„ íƒí•˜ê³  EPS ê°’ ì…ë ¥ ê°€ëŠ¥
        - ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ëª…ê³¼ í…Œì´ë¸”ëª…ì€ ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ì˜ ê¸°ë³¸ê°’ì„ ì‚¬ìš©
    """
    try:
        model_name = "solar-hybrid-seq-2-test-20251017-test-no-add-usage_kwh"
        tablename = "lstm_input_15m_new"
        
        print("\n" + "=" * 80)
        print("ğŸ” ì‹¤í–‰ ëª¨ë“œ ì„ íƒ")
        print("=" * 80)
        print("\n1. EPS í•„í„°ë§ ì ìš© ì˜ˆì¸¡ (ê¶Œì¥)")
        # ì‚¬ìš©ìê°€ ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ "1" ì‚¬ìš©
        
        # EPS ì„ê³„ê°’ ì„¤ì •: ì…ë ¥ì´ ì—†ìœ¼ë©´ ì „ì—­ê°’ ì‚¬ìš©
        eps_threshold = PREDICTION_EPS_THRESHOLD;
        
        print(f"\nâš™ï¸  ì„¤ì •:")
        print(f"   - EPS ì„ê³„ê°’: {eps_threshold}")
        
        # EPS í•„í„°ë§ ì ìš©, ì „ì²´ ì €ì¥
        print(f"   - í•„í„°ë§: ì ìš©")
        print(f"   - DB ì €ì¥: ì „ì²´")

        main(
                model_name=model_name,
                tablename=tablename,
                save_to_db=True,
                only_reliable=False,
                eps_threshold=eps_threshold,
                apply_filter=True
            )
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()