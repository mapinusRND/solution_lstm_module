# -*- coding: utf-8 -*-
"""
Title   : ê°œì„ ëœ LSTM ëª¨ë¸ ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
Description: 
    - í•™ìŠµëœ LSTM ëª¨ë¸ë¡œ ì‹ ê·œ ë°ì´í„° ì˜ˆì¸¡ ìˆ˜í–‰
    - ì¤‘ë³µ ì˜ˆì¸¡ê°’ ë¬¸ì œ í•´ê²°
    - ë¯¸ë˜ê°’ ì˜ˆì¸¡ ê¸°ëŠ¥ í¬í•¨
Version : 2.0
Date    : 2025-10-14
"""

import os
# TensorFlow ì„¤ì •: ìµœì í™” ê²½ê³  ë° ë¡œê·¸ ë ˆë²¨ ì¡°ì •
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # OneDNN ìµœì í™” ë¹„í™œì„±í™”
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'   # ì—ëŸ¬ë§Œ ì¶œë ¥ (0=ëª¨ë“ ë¡œê·¸, 1=INFOì œì™¸, 2=WARNINGì œì™¸, 3=ERRORë§Œ)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
import json
import joblib
from sqlalchemy import create_engine
from datetime import datetime, timedelta

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================
# ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ê²½ë¡œ ìë™ ì„¤ì • (ë¡œì»¬ ê°œë°œ í™˜ê²½ vs ì„œë²„ ë°°í¬ í™˜ê²½)
ENV = os.getenv('FLASK_ENV', 'local')
if ENV == 'local':
    root = "D:/work/lstm"  # ë¡œì»¬ ê°œë°œ í™˜ê²½ ê²½ë¡œ
else:
    root = "/app/webfiles/lstm"  # ì„œë²„ ë°°í¬ í™˜ê²½ ê²½ë¡œ

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
model_path = os.path.abspath(root + "/saved_models")
prediction_path = os.path.abspath(root + "/predictions")
os.makedirs(prediction_path, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±

# ============================================================================
# DB ì—°ê²° í•¨ìˆ˜
# ============================================================================
def get_db_engine():
    """
    PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—”ì§„ ìƒì„±
    
    Returns:
        sqlalchemy.engine.Engine: DB ì—°ê²° ì—”ì§„
    """
    # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” í™˜ê²½ ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ë¡œ ê´€ë¦¬ ê¶Œì¥
    connection_string = "postgresql://postgres:mapinus@10.10.10.201:5432/postgres"
    return create_engine(connection_string)

# ============================================================================
# ì‹ ê·œ ë°ì´í„° ë¡œë“œ
# ============================================================================
def load_new_data(tablename, dateColumn, studyColumns, start_date=None, end_date=None):
    """
    PostgreSQL DBì—ì„œ ì˜ˆì¸¡í•  ì‹ ê·œ ë°ì´í„°ë¥¼ ë¡œë“œ
    
    Parameters:
    -----------
    tablename : str
        ì¡°íšŒí•  í…Œì´ë¸”ëª… (ì˜ˆ: 'lstm_input_15m')
    dateColumn : str
        ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ëª… (ì˜ˆ: 'timestamp')
    studyColumns : str
        ì‚¬ìš©í•  ì»¬ëŸ¼ë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•œ ë¬¸ìì—´ (ì˜ˆ: 'temp,humidity,solar_kwh')
    start_date : str, optional
        ì¡°íšŒ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹), Noneì´ë©´ ì „ì²´ ì¡°íšŒ
    end_date : str, optional
        ì¡°íšŒ ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹), Noneì´ë©´ ì „ì²´ ì¡°íšŒ
        
    Returns:
    --------
    pandas.DataFrame : ë¡œë“œëœ ë°ì´í„° (ì‹¤íŒ¨ì‹œ None)
    """
    try:
        engine = get_db_engine()
        
        # ê¸°ë³¸ ì¿¼ë¦¬: ì „ì²´ ë°ì´í„° ì¡°íšŒ
        query = f"""
        SELECT {studyColumns},{dateColumn}
        FROM carbontwin.{tablename}
        WHERE {dateColumn} IS NOT NULL
        ORDER BY {dateColumn} ASC
        """
        
        # ë‚ ì§œ ë²”ìœ„ê°€ ì§€ì •ëœ ê²½ìš° WHERE ì¡°ê±´ ì¶”ê°€
        if start_date or end_date:
            where_conditions = [f"{dateColumn} IS NOT NULL"]
            if start_date:
                where_conditions.append(f"{dateColumn} >= '{start_date}'")
            if end_date:
                where_conditions.append(f"{dateColumn} <= '{end_date}'")
            
            query = f"""
            SELECT {studyColumns},{dateColumn}
            FROM carbontwin.{tablename}
            WHERE {' AND '.join(where_conditions)}
            ORDER BY {dateColumn} ASC
            """
        
        # SQL ì¿¼ë¦¬ ì‹¤í–‰ ë° DataFrameìœ¼ë¡œ ë³€í™˜
        data = pd.read_sql_query(query, engine)
        print(f"âœ… ì‹ ê·œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}í–‰ (í…Œì´ë¸”: {tablename})")
        return data
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None

# ============================================================================
# NumPy/Pandas íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
# ============================================================================
def convert_to_serializable(obj):
    """
    NumPy ë° Pandasì˜ íŠ¹ìˆ˜ íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    
    Parameters:
    -----------
    obj : any
        ë³€í™˜í•  ê°ì²´ (np.ndarray, np.int64, np.float64, pd.Timestamp ë“±)
        
    Returns:
    --------
    any : JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì… (list, int, float, str)
    
    Notes:
    ------
    JSON íŒŒì¼ ì €ì¥ ì‹œ "Object of type float32 is not JSON serializable" 
    ê°™ì€ ì—ëŸ¬ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜
    """
    if isinstance(obj, np.ndarray):
        return obj.tolist()  # NumPy ë°°ì—´ â†’ Python ë¦¬ìŠ¤íŠ¸
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)  # NumPy ì •ìˆ˜ â†’ Python int
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)  # NumPy ì‹¤ìˆ˜ â†’ Python float
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()  # Pandas Timestamp â†’ ISO ë¬¸ìì—´
    elif isinstance(obj, datetime):
        return obj.isoformat()  # datetime â†’ ISO ë¬¸ìì—´
    return obj

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
def load_trained_model(model_name):
    """
    ì €ì¥ëœ LSTM ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì„¤ì • íŒŒì¼ì„ ë¡œë“œ
    
    Parameters:
    -----------
    model_name : str
        ë¡œë“œí•  ëª¨ë¸ëª… (ì˜ˆ: 'test15m')
        
    Returns:
    --------
    tuple : (model, scaler, config)
        - model: Keras LSTM ëª¨ë¸ ê°ì²´
        - scaler: StandardScaler ê°ì²´ (ë°ì´í„° ì •ê·œí™”ìš©)
        - config: dict (ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì„¤ì • ì •ë³´)
        ì‹¤íŒ¨ ì‹œ (None, None, None) ë°˜í™˜
        
    Notes:
    ------
    ëª¨ë¸ íŒŒì¼ êµ¬ì¡°:
    - {model_name}.h5: Keras ëª¨ë¸ ê°€ì¤‘ì¹˜
    - {model_name}_scaler.pkl: StandardScaler ê°ì²´
    - {model_name}_config.json: ëª¨ë¸ ì„¤ì • (ì»¬ëŸ¼ëª…, ì‹œí€€ìŠ¤ ê¸¸ì´ ë“±)
    """
    try:
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        model_file = os.path.join(model_path, f"{model_name}.h5")
        scaler_file = os.path.join(model_path, f"{model_name}_scaler.pkl")
        config_file = os.path.join(model_path, f"{model_name}_config.json")
        
        # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(model_file):
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_file}")
            return None, None, None
        if not os.path.exists(scaler_file):
            print(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scaler_file}")
            return None, None, None
        if not os.path.exists(config_file):
            print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}")
            return None, None, None
        
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        
        # Keras ëª¨ë¸ ë¡œë“œ (compile=False: í•™ìŠµ ì‹œ ì„¤ì • ë¬´ì‹œ, ì˜ˆì¸¡ë§Œ ì‚¬ìš©)
        model = load_model(model_file, compile=False)
        model.compile(optimizer='adam', loss='mse')  # ì˜ˆì¸¡ìš© ì¬ì»´íŒŒì¼
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì •ê·œí™” íŒŒë¼ë¯¸í„°)
        scaler = joblib.load(scaler_file)
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ (JSON)
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # í•™ìŠµ ì»¬ëŸ¼ ì •ë³´ íŒŒì‹±
        study_cols_list = [col.strip() for col in config['studyColumns'].split(',')]
        
        # ë¡œë“œ ì™„ë£Œ ì •ë³´ ì¶œë ¥
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"   - íƒ€ê²Ÿ ì»¬ëŸ¼: {config['targetColumn']}")  # ì˜ˆì¸¡í•  ë³€ìˆ˜
        print(f"   - í•™ìŠµ ì»¬ëŸ¼ ({len(study_cols_list)}ê°œ): {config['studyColumns']}")  # ì…ë ¥ ë³€ìˆ˜ë“¤
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {config['r_seqLen']}")  # LSTM ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        print(f"   - ì˜ˆì¸¡ ì¼ìˆ˜: {config['r_predDays']}")  # ëª‡ ìŠ¤í… ì•ì„ ì˜ˆì¸¡í•˜ëŠ”ì§€
        
        return model, scaler, config
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None

# ============================================================================
# ğŸ”¥ ê°œì„ ëœ ë¯¸ë˜ê°’ ì˜ˆì¸¡ (ì¤‘ë³µ ì˜ˆì¸¡ ë¬¸ì œ í•´ê²°)
# ============================================================================
def predict_future_improved(model, scaler, config, new_data, future_steps=None):
    """
    ê°œì„ ëœ ë¯¸ë˜ê°’ ì˜ˆì¸¡ - ì¬ê·€ì  ì˜ˆì¸¡ìœ¼ë¡œ ì‹¤ì œ ë¯¸ë˜ê°’ ìƒì„±
    
    ê°œì„ ì‚¬í•­:
    1. ì‹œê°„ ì •ë³´ ì¶”ê°€ (ì‹œê°„, ë¶„) - íƒœì–‘ê´‘ ë°œì „ì€ ì‹œê°„ëŒ€ë³„ íŒ¨í„´ì´ ì¤‘ìš”
    2. ë” ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ì¶”ê°€ - ì˜ˆì¸¡ì˜ ë‹¤ì–‘ì„± í™•ë³´
    3. ì˜ˆì¸¡ê°’ ë²”ìœ„ ê²€ì¦ - ë¬¼ë¦¬ì  ì œì•½ ì¡°ê±´ ì ìš© (ì•¼ê°„=0)
    4. ì•™ìƒë¸” ì˜ˆì¸¡ - ì—¬ëŸ¬ ë²ˆ ì˜ˆì¸¡í•˜ì—¬ í‰ê·  (ì•ˆì •ì„± í–¥ìƒ)
    
    Parameters:
    -----------
    model : Keras Model
        í•™ìŠµëœ LSTM ëª¨ë¸
    scaler : StandardScaler
        í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬
    config : dict
        ëª¨ë¸ ì„¤ì • ì •ë³´
    new_data : DataFrame
        ê¸°ì¤€ì´ ë˜ëŠ” ìµœê·¼ ë°ì´í„°
    future_steps : int, optional
        ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜ (Noneì´ë©´ ìë™ ê³„ì‚°: max(10, seq_len//2))
        
    Returns:
    --------
    dict : ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼
        - predictions: ê° ìŠ¤í…ë³„ ì˜ˆì¸¡ê°’, ì‹œê°„ ì •ë³´
        - statistics: ì˜ˆì¸¡ê°’ í†µê³„ (ìµœì†Œ, ìµœëŒ€, í‰ê· , í‘œì¤€í¸ì°¨)
    """
    try:
        # ì„¤ì • ì •ë³´ ì¶”ì¶œ
        dateColumn = config['dateColumn']
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        seq_len = int(config['r_seqLen'])  # LSTM ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        pred_days = int(config['r_predDays'])  # ì˜ˆì¸¡ ê°„ê²©
        
        # ë¯¸ë˜ ìŠ¤í… ìˆ˜ ìë™ ê³„ì‚° (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
        if future_steps is None:
            future_steps = max(10, seq_len // 2)  # ìµœì†Œ 10, ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ì ˆë°˜
        
        # ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° íƒ€ê²Ÿ ì¸ë±ìŠ¤ ì°¾ê¸°
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        target_idx = study_columns_list.index(targetColumn)  # ì˜ˆì¸¡í•  ë³€ìˆ˜ì˜ ì¸ë±ìŠ¤
        
        # ë§ˆì§€ë§‰ ë‚ ì§œ ì¶”ì¶œ (ê¸°ì¤€ ì‹œì )
        if dateColumn in new_data.columns:
            last_date = pd.to_datetime(new_data[dateColumn].iloc[-1])
        else:
            last_date = datetime.now()
        
        # ë°ì´í„° ì¤€ë¹„ ë° ì •ê·œí™”
        data_for_prediction = new_data[study_columns_list].astype(float)
        data_scaled = scaler.transform(data_for_prediction)  # StandardScalerë¡œ ì •ê·œí™”
        
        print(f"\nğŸ”® ê°œì„ ëœ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì‹œì‘...")
        print(f"   - ê¸°ì¤€ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}ê°œ")
        print(f"   - ì˜ˆì¸¡ ì‹œì‘ì : {last_date}")
        print(f"   - ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í…: {future_steps}ê°œ")
        
        # ì‹œê°„ ê°„ê²© ê³„ì‚° (ë°ì´í„°ì˜ í‰ê·  ì‹œê°„ ê°„ê²©)
        if dateColumn in new_data.columns and len(new_data) > 1:
            dates = pd.to_datetime(new_data[dateColumn])
            time_delta = (dates.iloc[-1] - dates.iloc[-2])  # ë§ˆì§€ë§‰ ë‘ ë°ì´í„°ì˜ ì‹œê°„ ì°¨ì´
        else:
            time_delta = pd.Timedelta(minutes=1)  # ê¸°ë³¸ê°’: 1ë¶„
        
        # ì´ˆê¸° ì‹œí€€ìŠ¤ ì„¤ì • (ë§ˆì§€ë§‰ seq_len ê°œ ë°ì´í„°)
        current_sequence = data_scaled[-seq_len:].copy()
        
        # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        future_predictions = []  # ì˜ˆì¸¡ê°’
        future_dates = []  # ì˜ˆì¸¡ ë‚ ì§œ
        prediction_confidence = []  # ì‹ ë¢°ë„ (ë‚´ë¶€ ì‚¬ìš©ìš©)
        
        # ğŸ”¥ ì•™ìƒë¸” ì˜ˆì¸¡ ì„¤ì • (ì—¬ëŸ¬ ë²ˆ ì˜ˆì¸¡í•˜ì—¬ í‰ê· )
        n_ensemble = 5  # 5ë²ˆ ì˜ˆì¸¡í•˜ì—¬ í‰ê·  ì‚¬ìš©
        
        # ì¬ê·€ì  ì˜ˆì¸¡ ë£¨í”„ (ê° ë¯¸ë˜ ìŠ¤í…ë§ˆë‹¤ ë°˜ë³µ)
        for step in range(future_steps):
            # ë‹¤ìŒ ì˜ˆì¸¡ ì‹œì  ê³„ì‚°
            next_date = last_date + time_delta * (step + 1)
            
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ (íƒœì–‘ê´‘ ë°œì „ì€ ì‹œê°„ëŒ€ê°€ ì¤‘ìš”)
            hour = next_date.hour
            minute = next_date.minute
            
            # ğŸ”¥ ì•™ìƒë¸” ì˜ˆì¸¡: ì—¬ëŸ¬ ë²ˆ ì˜ˆì¸¡í•˜ì—¬ í‰ê·  (ì•ˆì •ì„± í–¥ìƒ)
            ensemble_predictions = []
            
            for _ in range(n_ensemble):
                # ë…¸ì´ì¦ˆ ì¶”ê°€ (ì…ë ¥ ë°ì´í„°ì— ì‘ì€ ë³€ë™ ì¶”ê°€)
                # ëª©ì : ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ ë‹¤ì–‘í•œ ì˜ˆì¸¡ê°’ ìƒì„±
                noisy_sequence = current_sequence + np.random.normal(0, 0.05, current_sequence.shape)
                
                # LSTM ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜: (batch_size=1, seq_len, features)
                input_data = noisy_sequence.reshape(1, seq_len, len(study_columns_list))
                
                # ëª¨ë¸ ì˜ˆì¸¡ (ì •ê·œí™”ëœ ê°’ ì¶œë ¥)
                pred_scaled = model.predict(input_data, verbose=0)
                ensemble_predictions.append(pred_scaled[0, 0])
            
            # ì•™ìƒë¸” í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
            avg_pred_scaled = np.mean(ensemble_predictions)  # í‰ê·  ì˜ˆì¸¡ê°’
            pred_std = np.std(ensemble_predictions)  # í‘œì¤€í¸ì°¨ (ë¶ˆí™•ì‹¤ì„± ì§€í‘œ)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ë‚´ë¶€ ì‚¬ìš©ìš©, JSONì—ë§Œ ì €ì¥)
            distance_penalty = 1.0 - (step / future_steps) * 0.3
            ensemble_uncertainty = min(pred_std / 0.1, 1.0)
            confidence = distance_penalty * (1.0 - ensemble_uncertainty)
            confidence = max(0.0, min(1.0, confidence))
            
            # ì˜ˆì¸¡ê°’ ì—­ì •ê·œí™” (ìŠ¤ì¼€ì¼ëœ ê°’ â†’ ì›ë˜ ë‹¨ìœ„)
            mean_values = scaler.mean_.copy()  # ìŠ¤ì¼€ì¼ëŸ¬ì˜ í‰ê· ê°’
            mean_values[target_idx] = avg_pred_scaled  # íƒ€ê²Ÿ ë³€ìˆ˜ë§Œ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë³€ê²½
            pred_value = scaler.inverse_transform([mean_values])[0, target_idx]  # ì—­ë³€í™˜
            
            # ğŸ”¥ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ë¬¼ë¦¬ì  ì œì•½ ì ìš©
            # ì•¼ê°„(18ì‹œ~06ì‹œ)ì—ëŠ” ë°œì „ëŸ‰ì´ ê±°ì˜ 0ì´ì–´ì•¼ í•¨
            if 18 <= hour or hour < 6:
                pred_value = max(0, pred_value * 0.1)  # ì•¼ê°„ì€ ì˜ˆì¸¡ê°’ì˜ 10%ë§Œ ì‚¬ìš©
            else:
                pred_value = max(0, pred_value)  # ì£¼ê°„ì€ ìŒìˆ˜ë§Œ ë°©ì§€
            
            # ê²°ê³¼ ì €ì¥
            future_predictions.append(pred_value)
            future_dates.append(next_date)
            prediction_confidence.append(confidence)  # ë‚´ë¶€ìš©
            
            # ğŸ”¥ ë‹¤ìŒ ì‹œí€€ìŠ¤ ì¤€ë¹„ (ì¬ê·€ì  ì˜ˆì¸¡ì˜ í•µì‹¬)
            # í˜„ì¬ ì˜ˆì¸¡ê°’ì„ ë‹¤ìŒ ì˜ˆì¸¡ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            new_point = current_sequence[-1].copy()  # ë§ˆì§€ë§‰ ë°ì´í„° í¬ì¸íŠ¸ ë³µì‚¬
            new_point[target_idx] = avg_pred_scaled  # íƒ€ê²Ÿ ë³€ìˆ˜ë§Œ ì˜ˆì¸¡ê°’ìœ¼ë¡œ êµì²´
            
            # ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì— ì‘ì€ ë³€í™” ì¶”ê°€ (ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë°˜ì˜)
            time_factor = np.sin(2 * np.pi * hour / 24)  # ì¼ì¼ ì£¼ê¸° íŒ¨í„´ (-1 ~ 1)
            for i in range(len(new_point)):
                if i != target_idx:
                    # íƒ€ê²Ÿì´ ì•„ë‹Œ ë³€ìˆ˜ë“¤ì€ ì‹œê°„ íŒ¨í„´ì— ë”°ë¼ ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€
                    new_point[i] += np.random.normal(0, 0.02) * time_factor
            
            # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸: ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„° ì œê±°, ìƒˆ ì˜ˆì¸¡ê°’ ì¶”ê°€
            current_sequence = np.vstack([current_sequence[1:], new_point])
            
            # ì§„í–‰ìƒí™© í‘œì‹œ (10ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰)
            if (step + 1) % 10 == 0 or step == future_steps - 1:
                print(f"   â³ ì§„í–‰: {step + 1}/{future_steps} ìŠ¤í… ì™„ë£Œ")
        
        print(f"âœ… ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì™„ë£Œ!")
        
        # ê²°ê³¼ êµ¬ì„± (JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœ)
        future_result = {
            "model_name": config['modelName'],
            "target_column": targetColumn,
            "prediction_type": "future_improved",
            "base_date": last_date.isoformat(),
            "sequence_length": seq_len,
            "future_steps": future_steps,
            "prediction_interval": pred_days,
            "predictions": []
        }
        
        # ê° ìŠ¤í…ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        for i, (date, pred, conf) in enumerate(zip(future_dates, future_predictions, prediction_confidence)):
            future_result["predictions"].append({
                "step": i + 1,
                "date": date.isoformat(),
                "predicted_value": convert_to_serializable(pred),
                "confidence": convert_to_serializable(conf),  # JSON íŒŒì¼ìš©
                "hour": date.hour,
                "is_daytime": 6 <= date.hour < 18  # ì£¼ê°„ ì—¬ë¶€ (06~18ì‹œ)
            })
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        future_result["statistics"] = {
            "min_predicted": convert_to_serializable(np.min(future_predictions)),
            "max_predicted": convert_to_serializable(np.max(future_predictions)),
            "mean_predicted": convert_to_serializable(np.mean(future_predictions)),
            "std_predicted": convert_to_serializable(np.std(future_predictions)),
            "avg_confidence": convert_to_serializable(np.mean(prediction_confidence))  # JSON íŒŒì¼ìš©
        }
        
        return future_result
        
    except Exception as e:
        print(f"âŒ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# ê°œì„ ëœ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
# ============================================================================
def print_future_predictions_improved(result):
    """
    ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    
    Parameters:
    -----------
    result : dict
        predict_future_improved() í•¨ìˆ˜ì˜ ë°˜í™˜ê°’
    """
    predictions = result.get('predictions', [])
    
    # í—¤ë” ì¶œë ¥
    print(f"\nğŸ”® ê°œì„ ëœ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ê²°ê³¼:")
    print(f"   ê¸°ì¤€ ì‹œì : {result['base_date'][:19]}")
    print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´: {result.get('sequence_length', 'N/A')}ê°œ")
    print(f"   ì´ ì˜ˆì¸¡ ìŠ¤í…: {result['future_steps']}ê°œ")
    print("=" * 80)
    print(f"{'ìŠ¤í…':>6} {'ì˜ˆì¸¡ ë‚ ì§œ':<20} {'ì‹œê°„':>6} {'ì˜ˆì¸¡ê°’':>12} {'ì£¼ì•¼':>10}")
    print("=" * 80)
    
    # ê° ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
    for pred in predictions:
        date_str = pred['date'][:19]
        hour = pred.get('hour', 0)
        is_day = "â˜€ï¸ ì£¼ê°„" if pred.get('is_daytime', False) else "ğŸŒ™ ì•¼ê°„"
        
        print(f"{pred['step']:>6} {date_str:<20} {hour:>6}ì‹œ "
              f"{pred['predicted_value']:>12.4f} {is_day:>10}")
    
    print("=" * 80)
    
    # í†µê³„ ì •ë³´ ì¶œë ¥
    stats = result.get('statistics', {})
    
    print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„:")
    print(f"   ìµœì†Ÿê°’: {stats.get('min_predicted', 0):.4f}")
    print(f"   ìµœëŒ“ê°’: {stats.get('max_predicted', 0):.4f}")
    print(f"   í‰ê· ê°’: {stats.get('mean_predicted', 0):.4f}")
    print(f"   í‘œì¤€í¸ì°¨: {stats.get('std_predicted', 0):.4f}")
    
    # ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì²´í¬ (ì¤‘ë³µ ì˜ˆì¸¡ê°’ ë¬¸ì œ ê°ì§€)
    # pred_values = [p['predicted_value'] for p in predictions]
    # unique_values = len(set([round(v, 4) for v in pred_values]))  # ì†Œìˆ˜ì  4ìë¦¬ ê¸°ì¤€ ê³ ìœ ê°’
    # diversity_ratio = unique_values / len(pred_values) * 100

# ============================================================================
# ì˜ˆì¸¡ ìˆ˜í–‰ (ê³¼ê±° ë°ì´í„°ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€)
# ============================================================================
def predict_with_model(model, scaler, config, new_data):
    """
    ë¡œë“œëœ ëª¨ë¸ë¡œ ì‹ ê·œ ë°ì´í„° ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
    
    ê³¼ê±° ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì‹¤ì œê°’ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    model : Keras Model
        í•™ìŠµëœ LSTM ëª¨ë¸
    scaler : StandardScaler
        í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬
    config : dict
        ëª¨ë¸ ì„¤ì • ì •ë³´
    new_data : DataFrame
        ì˜ˆì¸¡í•  ì‹ ê·œ ë°ì´í„° (ì‹¤ì œê°’ í¬í•¨)
        
    Returns:
    --------
    dict : ì˜ˆì¸¡ ê²°ê³¼ ë° ì„±ëŠ¥ ì§€í‘œ
        - predictions: ê° ì‹œì ë³„ ì‹¤ì œê°’, ì˜ˆì¸¡ê°’, ì˜¤ì°¨
        - ì„±ëŠ¥ ì§€í‘œ: MAPE, MAE, RMSE, RÂ², ë°©í–¥ì„± ì •í™•ë„
        - í†µê³„: ì‹¤ì œê°’/ì˜ˆì¸¡ê°’ì˜ min, max, mean
    """
    try:
        # ì„¤ì • ì •ë³´ ì¶”ì¶œ
        dateColumn = config['dateColumn']
        studyColumns = config['studyColumns']
        targetColumn = config['targetColumn']
        seq_len = int(config['r_seqLen'])  # LSTM ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        pred_days = int(config['r_predDays'])  # ì˜ˆì¸¡ ê°„ê²© (ëª‡ ìŠ¤í… ë’¤ë¥¼ ì˜ˆì¸¡)
        
        # ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        study_columns_list = [col.strip() for col in studyColumns.split(',')]
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        if dateColumn in new_data.columns:
            dates = pd.to_datetime(new_data[dateColumn], errors='coerce')
        else:
            # ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì„ì˜ë¡œ ìƒì„± (5ë¶„ ê°„ê²©)
            print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ '{dateColumn}'ì´ ì—†ìŠµë‹ˆë‹¤.")
            dates = pd.date_range(start='2023-01-01', periods=len(new_data), freq='5T')
        
        # ë°ì´í„° ì¤€ë¹„ (ë¬¸ìì—´ì„ ì‹¤ìˆ˜ë¡œ ë³€í™˜)
        data_for_prediction = new_data[study_columns_list].astype(float)
        
        print(f"ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        # ë°ì´í„° ì •ê·œí™” (StandardScaler: í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
        data_scaled = scaler.transform(data_for_prediction)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        target_idx = study_columns_list.index(targetColumn)
        input_dim = len(study_columns_list)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        predX = []  # ì…ë ¥ ì‹œí€€ìŠ¤ (X)
        valid_dates = []  # ì˜ˆì¸¡ ì‹œì ì˜ ë‚ ì§œ
        actual_values = []  # ì‹¤ì œê°’ (ì •ë‹µ)
        
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì‹œí€€ìŠ¤ ìƒì„±
        for i in range(seq_len, len(data_scaled) - pred_days + 1):
            # i ì‹œì ì—ì„œ ê³¼ê±° seq_len ê°œ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            predX.append(data_scaled[i - seq_len:i, 0:input_dim])
            
            # i + pred_days ì‹œì ì˜ ê°’ì„ ì˜ˆì¸¡ (pred_days ìŠ¤í… ë’¤)
            valid_dates.append(dates.iloc[i + pred_days - 1])
            actual_values.append(data_for_prediction.iloc[i + pred_days - 1][targetColumn])
        
        # ë°ì´í„° ë¶€ì¡± ì²´í¬
        if len(predX) == 0:
            print(f"âŒ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            print(f"   í•„ìš”: {seq_len + pred_days}í–‰ ì´ìƒ")
            print(f"   í˜„ì¬: {len(new_data)}í–‰")
            return None
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        predX = np.array(predX)  # Shape: (samples, seq_len, features)
        
        print(f"ğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        print(f"   - ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {len(predX)}")
        
        # ëª¨ë¸ ì˜ˆì¸¡ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ í•œ ë²ˆì— ì˜ˆì¸¡)
        predictions_scaled = model.predict(predX, verbose=0)  # ì •ê·œí™”ëœ ì˜ˆì¸¡ê°’
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì—­ì •ê·œí™” (ì›ë˜ ë‹¨ìœ„ë¡œ ë³µì›)
        mean_values = np.repeat(scaler.mean_[np.newaxis, :], predictions_scaled.shape[0], axis=0)
        mean_values[:, target_idx] = np.squeeze(predictions_scaled)
        predictions = scaler.inverse_transform(mean_values)[:, target_idx]
        
        print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        actual_values = np.array(actual_values)
        
        # ========== ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ==========
        
        # 1. MAE (Mean Absolute Error): í‰ê·  ì ˆëŒ€ ì˜¤ì°¨
        mae = np.mean(np.abs(predictions - actual_values))
        
        # 2. RMSE (Root Mean Square Error): í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨
        rmse = np.sqrt(np.mean((predictions - actual_values) ** 2))
        
        # 3. MAPE (Mean Absolute Percentage Error): í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨
        # ì‹¤ì œê°’ì´ 0ì¸ ê²½ìš° ì œì™¸ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        mask = actual_values != 0
        if np.sum(mask) == 0:
            mape = 999.0  # ëª¨ë“  ì‹¤ì œê°’ì´ 0ì¸ ê²½ìš°
        else:
            mape = np.mean(np.abs((actual_values[mask] - predictions[mask]) / actual_values[mask])) * 100
        
        # ì •í™•ë„ = 100 - MAPE
        accuracy = 100 - mape if not np.isnan(mape) else 0
        
        # ì‹¤ì œê°’ 0ì¸ ë°ì´í„° ë¹„ìœ¨ ê³„ì‚°
        zero_ratio = (len(actual_values) - np.sum(mask)) / len(actual_values) * 100
        
        # 4. RÂ² Score (ê²°ì •ê³„ìˆ˜): ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€
        ss_res = np.sum((actual_values - predictions) ** 2)  # ì”ì°¨ ì œê³±í•©
        ss_tot = np.sum((actual_values - np.mean(actual_values)) ** 2)  # ì´ ì œê³±í•©
        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
        
        # 5. ë°©í–¥ì„± ì •í™•ë„: ìƒìŠ¹/í•˜ë½ ë°©í–¥ì„ ë§ì¶˜ ë¹„ìœ¨
        if len(actual_values) > 1:
            actual_direction = np.diff(actual_values) > 0  # ì‹¤ì œ ìƒìŠ¹/í•˜ë½
            pred_direction = np.diff(predictions) > 0  # ì˜ˆì¸¡ ìƒìŠ¹/í•˜ë½
            direction_accuracy = np.mean(actual_direction == pred_direction) * 100
        else:
            direction_accuracy = 0
        
        # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼:")
        print(f"   ğŸ¯ MAPE: {mape:.2f}%")
        print(f"   ğŸ“ˆ ì •í™•ë„: {accuracy:.2f}%")
        print(f"   ğŸ“ MAE: {mae:.4f}")
        print(f"   ğŸ“ RMSE: {rmse:.4f}")
        print(f"   ğŸ” RÂ² Score: {r2:.4f}")
        print(f"   ğŸ§­ ë°©í–¥ì„± ì •í™•ë„: {direction_accuracy:.2f}%")
        print(f"   â„¹ï¸  ì‹¤ì œê°’ 0ì¸ ë°ì´í„°: {zero_ratio:.1f}% ({len(actual_values) - np.sum(mask)}/{len(actual_values)}ê°œ)")
        
        # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
        result = {
            "status": "success",
            "modelName": config['modelName'],
            "target_column": targetColumn,
            "prediction_count": len(predictions),
            "timestamp": datetime.now().isoformat(),
            
            # ì„±ëŠ¥ ì§€í‘œ
            "mape": round(mape, 2),
            "accuracy": round(accuracy, 2),
            "mae": round(mae, 4),
            "rmse": round(rmse, 4),
            "r2_score": round(r2, 4),
            "direction_accuracy": round(direction_accuracy, 2),
            "zero_ratio": round(zero_ratio, 2),
            "zero_count": int(len(actual_values) - np.sum(mask)),
            
            # í†µê³„ ì •ë³´
            "statistics": {
                "actual_min": convert_to_serializable(np.min(actual_values)),
                "actual_max": convert_to_serializable(np.max(actual_values)),
                "actual_mean": convert_to_serializable(np.mean(actual_values)),
                "predicted_min": convert_to_serializable(np.min(predictions)),
                "predicted_max": convert_to_serializable(np.max(predictions)),
                "predicted_mean": convert_to_serializable(np.mean(predictions))
            },
            
            "predictions": []
        }
        
        # ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        for i in range(len(predictions)):
            actual = actual_values[i]
            predicted = predictions[i]
            diff = predicted - actual
            
            # ğŸ”¥ ê°œì„ : ì˜¤ì°¨ìœ¨ ê³„ì‚° ë¡œì§
            # ì‹¤ì œê°’ì´ 0ì¼ ë•ŒëŠ” ì ˆëŒ€ì˜¤ì°¨ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ì°¨ìœ¨ ê³„ì‚°
            if actual == 0:
                # ì˜ˆì¸¡ê°’ì´ ì‘ìœ¼ë©´(< 0.001) ë‚®ì€ ì˜¤ì°¨ìœ¨
                # ì˜ˆì¸¡ê°’ì´ í¬ë©´ ë†’ì€ ì˜¤ì°¨ìœ¨ë¡œ í‘œì‹œ
                if abs(predicted) < 0.001:
                    pct_error = abs(predicted) * 10000  # 0.0001 â†’ 1%
                elif abs(predicted) < 0.01:
                    pct_error = abs(predicted) * 1000   # 0.001 â†’ 1%
                else:
                    pct_error = 999.0  # í° ì˜¤ì°¨
            else:
                # ì‹¤ì œê°’ì´ 0ì´ ì•„ë‹Œ ê²½ìš° ì¼ë°˜ì ì¸ ë°±ë¶„ìœ¨ ì˜¤ì°¨
                pct_error = abs(diff / actual) * 100
            
            result["predictions"].append({
                "index": i,
                "date": convert_to_serializable(valid_dates[i]),
                "actual_value": convert_to_serializable(actual),
                "predicted_value": convert_to_serializable(predicted),
                "difference": convert_to_serializable(diff),
                "percentage_error": convert_to_serializable(pct_error)
            })
        
        return result
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# ìµœê·¼ Nê°œ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
# ============================================================================
def print_recent_predictions(result, n=30):
    """
    ìµœê·¼ Nê°œì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    
    Parameters:
    -----------
    result : dict
        predict_with_model() í•¨ìˆ˜ì˜ ë°˜í™˜ê°’
    n : int, optional
        ì¶œë ¥í•  ì˜ˆì¸¡ ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 30)
    """
    predictions = result.get('predictions', [])
    # ì „ì²´ ì¤‘ì—ì„œ ë§ˆì§€ë§‰ nê°œë§Œ ì„ íƒ
    recent = predictions[-n:] if len(predictions) > n else predictions
    
    print(f"\nğŸ” ìµœê·¼ {len(recent)}ê°œ ì˜ˆì¸¡ ê²°ê³¼:")
    print("=" * 110)
    print(f"{'ë‚ ì§œ':<20} {'ì‹¤ì œê°’':>12} {'ì˜ˆì¸¡ê°’':>12} {'ì°¨ì´':>12} {'ì˜¤ì°¨ìœ¨':>12} {'ë¹„ê³ ':>10}")
    print("=" * 110)
    
    for pred in recent:
        date_str = pred['date'][:19] if len(pred['date']) > 19 else pred['date']
        actual = pred['actual_value']
        predicted = pred['predicted_value']
        diff = pred['difference']
        pct_error = pred['percentage_error']
        
        # ì‹¤ì œê°’ì´ 0ì¼ ë•Œ í‘œì‹œ ë°©ì‹ ë³€ê²½
        if actual == 0:
            # ì˜ˆì¸¡ê°’ í¬ê¸°ì— ë”°ë¼ ë¹„ê³  í‘œì‹œ
            if abs(predicted) < 0.001:
                remark = "âœ“ ë¯¸ì„¸"  # ê±°ì˜ ì •í™•
            elif abs(predicted) < 0.01:
                remark = "â–³ ì†Œì˜¤ì°¨"  # ì‘ì€ ì˜¤ì°¨
            else:
                remark = "âœ— ëŒ€ì˜¤ì°¨"  # í° ì˜¤ì°¨
            error_str = f"{pct_error:>9.2f}%*"  # *í‘œì‹œë¡œ íŠ¹ìˆ˜ ê³„ì‚° í‘œì‹œ
        else:
            remark = ""
            error_str = f"{pct_error:>10.2f}%"
        
        print(f"{date_str:<20} "
              f"{actual:>12.4f} "
              f"{predicted:>12.4f} "
              f"{diff:>12.4f} "
              f"{error_str:>12} "
              f"{remark:>10}")
    
    print("=" * 110)
    print("* ì‹¤ì œê°’ì´ 0ì¼ ë•ŒëŠ” ì ˆëŒ€ì˜¤ì°¨ ê¸°ì¤€ ì˜¤ì°¨ìœ¨ (ì°¸ê³ ìš©)")

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================
def main(model_name=None, tablename=None):
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì „ì²´ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    
    Parameters:
    -----------
    model_name : str, optional
        ì‚¬ìš©í•  ëª¨ë¸ëª… (Noneì´ë©´ ì…ë ¥ ë°›ìŒ)
    tablename : str, optional
        ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ í…Œì´ë¸”ëª… (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        
    Returns:
    --------
    dict : í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë° ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼
    """
    print("=" * 70)
    print("ğŸ”® ê°œì„ ëœ LSTM ëª¨ë¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("=" * 70)
    
    # 1. ëª¨ë¸ëª… ì…ë ¥ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
    if model_name is None:
        model_name = input("\nğŸ“ ì‚¬ìš©í•  ëª¨ë¸ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: test15m): ").strip()
        if not model_name:
            model_name = "test15m"
            print(f"âœ… ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {model_name}")
    else:
        print(f"\nâœ… ì§€ì •ëœ ëª¨ë¸ ì‚¬ìš©: {model_name}")
    
    # 2. ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì„¤ì • ë¡œë“œ
    model, scaler, config = load_trained_model(model_name)
    
    # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    if model is None:
        print("\nğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:")
        if os.path.exists(model_path):
            models = [f.replace('.h5', '') for f in os.listdir(model_path) if f.endswith('.h5')]
            if models:
                for i, m in enumerate(models, 1):
                    print(f"   {i}. {m}")
            else:
                print("   (ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤)")
        return
    
    # 3. í…Œì´ë¸”ëª… ì„¤ì •
    if tablename is None:
        tablename = "lstm_input_15m"  # ê¸°ë³¸ê°’: 1ë¶„ ë‹¨ìœ„ ë°ì´í„°
    print(f"\nğŸ“Š ì‚¬ìš©í•  í…Œì´ë¸”: {tablename}")
    
    # 4. DBì—ì„œ ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    new_data = load_new_data(
        tablename,
        config['dateColumn'],
        config['studyColumns'],
        start_date=None,  # ì „ì²´ ê¸°ê°„ ì¡°íšŒ
        end_date=None
    )
    
    if new_data is None or new_data.empty:
        print("âŒ ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 5. ê³¼ê±° ë°ì´í„°ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰ (ëª¨ë¸ ì„±ëŠ¥ í‰ê°€)
    print(f"\n{'='*70}")
    result = predict_with_model(model, scaler, config, new_data)
    
    if result is None:
        return
    
    # 6. ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ (ìµœê·¼ 30ê°œ)
    print_recent_predictions(result, n=10)
    
    # 7. ì‹¤ì œ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì‹¤í–‰
    print(f"\n{'='*70}")
    
    seq_len = int(config.get('r_seqLen', 60))
    # auto_future_steps = max(20, seq_len)  # ìµœì†Œ 20, ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì˜ˆì¸¡
    auto_future_steps = 672;
    
    print(f"ğŸ”® ê°œì„ ëœ ì‹¤ì œ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ìˆ˜í–‰")
    print(f"   - ëª¨ë¸ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
    print(f"   - ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í…: {auto_future_steps}ê°œ")
    
    future_result = None
    try:
        # ë¯¸ë˜ê°’ ì˜ˆì¸¡ ìˆ˜í–‰
        future_result = predict_future_improved(model, scaler, config, new_data, auto_future_steps)
        
        if future_result:
            # ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
            print_future_predictions_improved(future_result)
            
            # ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ JSON íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            future_filename = f"{model_name}_future_improved_{timestamp}.json"
            future_filepath = os.path.join(prediction_path, future_filename)
            
            with open(future_filepath, 'w', encoding='utf-8') as f:
                json.dump(future_result, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {future_filepath}")
    except Exception as e:
        print(f"âŒ ë¯¸ë˜ê°’ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # 8. ì™„ë£Œ ë©”ì‹œì§€
    print(f"\n{'='*70}")
    print("ğŸ‰ ì˜ˆì¸¡ ì™„ë£Œ!")
    print("="*70)
    
    # 9. ê²°ê³¼ ë°˜í™˜ (APIë‚˜ ë‹¤ë¥¸ í•¨ìˆ˜ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
    return_data = {
        "test_result": result,  # ê³¼ê±° ë°ì´í„° ì˜ˆì¸¡ ê²°ê³¼
        "future_result": future_result  # ë¯¸ë˜ê°’ ì˜ˆì¸¡ ê²°ê³¼
    }
    return return_data

# ============================================================================
# í”„ë¡œê·¸ë¨ ì‹œì‘ì 
# ============================================================================
if __name__ == "__main__":
    """
    ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ main() í•¨ìˆ˜ í˜¸ì¶œ
    
    ì‚¬ìš©ë²•:
        python lstm_predict.py
    """
    try:
        main()
    except KeyboardInterrupt:
        # Ctrl+Cë¡œ ì¤‘ë‹¨ ì‹œ
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ ë°œìƒ ì‹œ
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()  # ìƒì„¸ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥