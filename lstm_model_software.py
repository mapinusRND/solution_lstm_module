# -*- coding: utf-8 -*-
"""
Title   : ì™¸ë¶€ë°ì´í„° ê¸°ë°˜ LSTM ì˜ˆì¸¡ ë° ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë“ˆ
Author  : ì£¼ì„±ì¤‘ / (ì£¼)ë§µì¸ì–´ìŠ¤
Description: PostgreSQL ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“ˆ
"""

# ================= ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ =================
import os
# TensorFlow ìµœì í™” ì˜µì…˜ ë¹„í™œì„±í™” (ê²½ê³  ë©”ì‹œì§€ ì œê±°)
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import warnings
warnings.filterwarnings('ignore')
import absl.logging
# ABSL ë¡œê·¸ ì œê±° (Google ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ê³ )
absl.logging.set_verbosity(absl.logging.ERROR)
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import psycopg2
from sklearn.preprocessing import StandardScaler
import json
import joblib
from sqlalchemy import create_engine  # PostgreSQL ì—°ê²°ì„ ìœ„í•œ SQLAlchemy

# ================= í™˜ê²½ ì„¤ì • =================
root = "D:/work/lstm"  # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì • ë° ë””ë ‰í† ë¦¬ ìƒì„±
model_path = os.path.abspath(root + "/saved_models")
os.makedirs(model_path, exist_ok=True)

# ================= ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜ =================
def get_db_engine():
    """
    SQLAlchemy ì—”ì§„ì„ ìƒì„±í•˜ì—¬ PostgreSQL ì—°ê²°
    Returns:
        sqlalchemy.engine: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—”ì§„
    """
    connection_string = "postgresql://[ì‚¬ìš©ìëª…]:[ë¹„ë°€ë²ˆí˜¸]@[í˜¸ìŠ¤íŠ¸]:[í¬íŠ¸]/[ë°ì´í„°ë² ì´ìŠ¤ëª…]"
    return create_engine(connection_string)

def get_db_connection():
    """
    psycopg2ë¥¼ ì‚¬ìš©í•œ ì§ì ‘ PostgreSQL ì—°ê²°
    Returns:
        psycopg2.connection: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´
    """
    return psycopg2.connect(
        dbname="ë””ë¹„ì´ë¦„",
        user="ì‚¬ìš©ìëª…",
        password="ë¹„ë°€ë²ˆí˜¸",
        host="ip",
        port="port"
    )

# ================= LSTM ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜ =================
def lstmLearning():
    """
    LSTM ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰
    
    Returns:
        dict: í•™ìŠµ ìƒíƒœì™€ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    # ================= ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì„¤ì • =================
    dateColumn = "ì»¬ëŸ¼ëª…"      # ì‹œê³„ì—´ ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ëª…
    studyColumns = "ì»¬ëŸ¼ëª…1,ì»¬ëŸ¼ëª…2"  # í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜ ì»¬ëŸ¼ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)
    targetColumn = "ì»¬ëŸ¼ëª…"     # ì˜ˆì¸¡í•˜ë ¤ëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ëª…

    lstmData = None

    # ================= ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•™ìŠµ ë°ì´í„° ì¡°íšŒ =================
    try:    
        tablename = "í…Œì´ë¸”ëª…"  # ì¡°íšŒí•  í…Œì´ë¸”ëª…
        engine = get_db_engine()     # SQLAlchemy ì—”ì§„ ìƒì„±

        # LSTM í•™ìŠµìš© ë°ì´í„° ì¡°íšŒ ì¿¼ë¦¬
        query = f"""
        SELECT {studyColumns},{dateColumn}
        FROM carbontwin.{tablename}
        WHERE {dateColumn} IS NOT NULL
        ORDER BY {dateColumn} ASC
        """

        # ì¿¼ë¦¬ ì‹¤í–‰ ë° DataFrameìœ¼ë¡œ ë³€í™˜
        lstmData = pd.read_sql_query(query, engine)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(lstmData)}í–‰")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        return {"status": "error", "message": str(e)}
        
    # ================= ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì‘ì—… ì‹¤í–‰ =================
    def background_task():
        """
        ì‹¤ì œ LSTM ëª¨ë¸ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
        Returns:
            dict: í•™ìŠµ ê²°ê³¼
        """
        try:
            # ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ í˜¸ì¶œ
            result = lstmFinance(lstmData, dateColumn, studyColumns, targetColumn)
            print("âœ… ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì„±ê³µ:", result)
            return result
        except Exception as e:
            print("âŒ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì˜¤ë¥˜:", str(e))
            return {"status": "error", "message": str(e)}

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹¤í–‰
    result = background_task()
    
    return {"status": "success", "message": "í•™ìŠµì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "result": result}

# ================= LSTM ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ í•¨ìˆ˜ =================
def lstmFinance(lstmData, dateColumn, studyColumns, targetColumn):
    """
    LSTM ëª¨ë¸ì˜ í•™ìŠµ, ì˜ˆì¸¡, í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    Args:
        lstmData (DataFrame): í•™ìŠµìš© ë°ì´í„°
        dateColumn (str): ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ëª…
        studyColumns (str): í•™ìŠµ í”¼ì²˜ ì»¬ëŸ¼ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)
        targetColumn (str): ì˜ˆì¸¡ íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
        
    Returns:
        dict: í•™ìŠµ ë° ì˜ˆì¸¡ ê²°ê³¼
    """
    # TensorFlow eager execution í™œì„±í™” (ë””ë²„ê¹… ìš©ì´)
    if not tf.executing_eagerly():
        tf.config.run_functions_eagerly(True)

    # ================= í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…ë ¥ ë°›ê¸° =================
    try:
        modelName = input("ëª¨ë¸ëª…: ").strip() or "training_1"
        r_epochs = int(input("ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 20): ").strip() or "20")
        r_batchSize = int(input("ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 16): ").strip() or "16")
        r_validationSplit = float(input("ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1): ").strip() or "0.1")
        r_seqLen = int(input("ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 14): ").strip() or "14")
        r_predDays = int(input("ì˜ˆì¸¡ ì¼ìˆ˜ (ê¸°ë³¸ê°’: 1): ").strip() or "1")
    except ValueError:
        print("âŒ ì˜ëª»ëœ ì…ë ¥ê°’ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        modelName = "training_1"
        r_epochs = 20           # í•™ìŠµ ì—í¬í¬ ìˆ˜
        r_batchSize = 16        # ë°°ì¹˜ í¬ê¸°
        r_validationSplit = 0.1 # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (10%)
        r_seqLen = 14          # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (14ì¼ê°„ì˜ ë°ì´í„°)
        r_predDays = 1         # ì˜ˆì¸¡í•  ë¯¸ë˜ ì¼ìˆ˜

    # ëª¨ë¸ íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    model_file_path = os.path.join(model_path, modelName + ".h5")

    # ================= ë°ì´í„° ê²€ì¦ =================
    if lstmData.empty:
        print(f"âŒ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {"status": "error", "message": "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
    
    # í•™ìŠµ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    study_columns_list = [col.strip() for col in studyColumns.split(',')]
    
    # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ í•™ìŠµ ì»¬ëŸ¼ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if targetColumn not in study_columns_list:
        print(f"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ '{targetColumn}'ì´ í•™ìŠµ ì»¬ëŸ¼ì— ì—†ìŠµë‹ˆë‹¤.")
        return {"status": "error", "message": f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{targetColumn}'ì´ í•™ìŠµ ì»¬ëŸ¼ì— ì—†ìŠµë‹ˆë‹¤."}
    
    # ================= ë‚ ì§œ ë°ì´í„° ì²˜ë¦¬ =================
    if dateColumn in lstmData.columns:
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        dates = pd.to_datetime(lstmData[dateColumn], errors='coerce')
    else:
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê°€ìƒì˜ ë‚ ì§œ ë²”ìœ„ ìƒì„± (5ë¶„ ê°„ê²©)
        dates = pd.date_range(start='2023-01-01', periods=len(lstmData), freq='5T')
        print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ '{dateColumn}'ì´ ì—†ì–´ì„œ ê°€ìƒ ë‚ ì§œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    
    # ================= ë°ì´í„° ì „ì²˜ë¦¬ =================
    # í•™ìŠµìš© í”¼ì²˜ ë°ì´í„° ì„ íƒ ë° float íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    stock_data_for_training = lstmData[study_columns_list].astype(float)

    # ë°ì´í„° ì •ê·œí™” (StandardScaler ì‚¬ìš©)
    scaler = StandardScaler()
    stock_data_scaled = scaler.fit_transform(stock_data_for_training)

    # ================= í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  =================
    n_train = int(0.9 * stock_data_scaled.shape[0])  # 90%ë¥¼ í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš©
    train_data_scaled = stock_data_scaled[:n_train]   # í›ˆë ¨ ë°ì´í„°
    test_data_scaled = stock_data_scaled[n_train:]    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_dates = dates[n_train:]                      # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë‚ ì§œ

    # ================= LSTM ì…ë ¥ íŒŒë¼ë¯¸í„° ì„¤ì • =================
    pred_days = int(r_predDays)                              # ì˜ˆì¸¡í•  ë¯¸ë˜ ì¼ìˆ˜
    seq_len = int(r_seqLen)                                 # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
    input_dim = stock_data_for_training.shape[1]            # ì…ë ¥ í”¼ì²˜ ìˆ˜
    target_idx = study_columns_list.index(targetColumn)     # íƒ€ê²Ÿ ì»¬ëŸ¼ì˜ ì¸ë±ìŠ¤

    # ================= ì‹œê³„ì—´ ë°ì´í„°ì…‹ ìƒì„± =================
    trainX, trainY, testX, testY = [], [], [], []
    
    # í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
    # seq_lenë§Œí¼ì˜ ê³¼ê±° ë°ì´í„°ë¡œ pred_days í›„ì˜ ê°’ì„ ì˜ˆì¸¡
    for i in range(seq_len, n_train - pred_days + 1):
        trainX.append(train_data_scaled[i - seq_len:i, 0:input_dim])  # ì…ë ¥: seq_lenê°œì˜ ì‹œì  ë°ì´í„°
        trainY.append(train_data_scaled[i + pred_days - 1:i + pred_days, target_idx])  # ì¶œë ¥: pred_days í›„ íƒ€ê²Ÿ ê°’

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
    for i in range(seq_len, len(test_data_scaled) - pred_days + 1):
        testX.append(test_data_scaled[i - seq_len:i, 0:input_dim])
        testY.append(test_data_scaled[i + pred_days - 1:i + pred_days, target_idx])

    # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    trainX, trainY = np.array(trainX), np.array(trainY)
    testX, testY = np.array(testX), np.array(testY)

    # ================= í•™ìŠµ ì‹œì‘ ì•Œë¦¼ =================
    print(f"================ì ì‹œí›„ {modelName} ëª¨ë¸ í•™ìŠµ ì‹œì‘================")
    print(f"================ì„¤ì • ê°’: epochs={r_epochs}, batchSize={r_batchSize}, validationSplit={r_validationSplit}, seqLen={r_seqLen}, predDays={r_predDays}================")
    print(f"================ë‚ ì§œ ë°ì´í„° ì»¬ëŸ¼: {dateColumn}================")
    print(f"================í•™ìŠµ ë°ì´í„° ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸: {studyColumns}================")
    print(f"================í•™ìŠµ ë°ì´í„° ë°ì´í„° ìˆ˜ : {len(lstmData)}================")
    print(f"================ì˜ˆì¸¡ ë°ì´í„° ì»¬ëŸ¼: {targetColumn}================")

    # ================= ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ìƒì„± =================
    try:
        # ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë“œ
        model = load_model(model_file_path, compile=False)
        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
        print("âœ… Loaded full model from disk")
    except (OSError, IOError):
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        print("ğŸ”„ Training model from scratch...")

        # LSTM ëª¨ë¸ êµ¬ì¡° ì •ì˜
        model = Sequential([
            Input(shape=(trainX.shape[1], trainX.shape[2])),  # ì…ë ¥ ë ˆì´ì–´
            LSTM(64, return_sequences=True),                   # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´ (64 ìœ ë‹›)
            LSTM(32, return_sequences=False),                  # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´ (32 ìœ ë‹›)
            Dense(trainY.shape[1])                            # ì¶œë ¥ ë ˆì´ì–´ (Dense)
        ])

        # ëª¨ë¸ ì»´íŒŒì¼ (ì˜µí‹°ë§ˆì´ì €: Adam, ì†ì‹¤í•¨ìˆ˜: MSE)
        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')

        # ================= ì»¤ìŠ¤í…€ ì½œë°± í´ë˜ìŠ¤ ì •ì˜ =================
        class TrainingCallback(Callback):
            """
            ì—í¬í¬ë§ˆë‹¤ í•™ìŠµ ì§„í–‰ìƒí™©ì„ ì¶œë ¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±
            """
            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                print(f" Epoch {epoch + 1}: loss={logs.get('loss', 0):.4f}, val_loss={logs.get('val_loss', 0):.4f}")

        # ================= ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ =================
        history = model.fit(
            trainX, trainY,                           # í›ˆë ¨ ë°ì´í„°
            epochs=int(r_epochs),                     # ì—í¬í¬ ìˆ˜
            batch_size=int(r_batchSize),             # ë°°ì¹˜ í¬ê¸°
            validation_split=float(r_validationSplit), # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            verbose=1,                                # í•™ìŠµ ì§„í–‰ìƒí™© ì¶œë ¥
            callbacks=[TrainingCallback()]            # ì»¤ìŠ¤í…€ ì½œë°± ì ìš©
        )

        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        model.save(model_file_path)
        print("âœ… Full model saved successfully.")

    # ================= ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ =================
    prediction = model.predict(testX)

    # ================= ì˜ˆì¸¡ ê²°ê³¼ ì—­ì •ê·œí™” =================
    # ì˜ˆì¸¡ê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
    mean_values_pred = np.repeat(scaler.mean_[np.newaxis, :], prediction.shape[0], axis=0)
    mean_values_pred[:, target_idx] = np.squeeze(prediction)
    y_pred = scaler.inverse_transform(mean_values_pred)[:, target_idx]

    # ì‹¤ì œê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
    mean_values_testY = np.repeat(scaler.mean_[np.newaxis, :], testY.shape[0], axis=0)
    mean_values_testY[:, target_idx] = np.squeeze(testY)
    testY_original = scaler.inverse_transform(mean_values_testY)[:, target_idx]
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” ë‚ ì§œ ì¶”ì¶œ
    valid_test_dates = test_dates[seq_len : seq_len + len(testY_original)]

    # ================= ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ =================
    def mean_absolute_percentage_error(y_true, y_pred):
        """
        í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨(MAPE) ê³„ì‚°
        Args:
            y_true: ì‹¤ì œê°’
            y_pred: ì˜ˆì¸¡ê°’
        Returns:
            float: MAPE ê°’ (%)
        """
        mask = y_true != 0  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€
        if np.sum(mask) == 0:
            return 999.0
        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

    # MAPEì™€ ì •í™•ë„ ê³„ì‚°
    mape = mean_absolute_percentage_error(testY_original, y_pred)
    accuracy = 100 - mape if not np.isnan(mape) else np.nan

    # ================= ê²°ê³¼ ì¶œë ¥ =================
    print(f"âœ… MAPE: {mape:.2f}%")
    print(f"âœ… ì˜ˆì¸¡ ì •í™•ë„: {accuracy:.2f}%")

# ================= ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ =================
if __name__ == "__main__":
    """
    ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ë™ì‘
    """
    result = lstmLearning()
    print("ìµœì¢… ê²°ê³¼:", result)